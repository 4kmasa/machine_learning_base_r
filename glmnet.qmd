---
title: "lasso回归"
---

>本文是对`glmnet`包的说明，主要参考官方文档：https://glmnet.stanford.edu/

>`glmnet`包可以实现lasso回归、岭（ridge）回归、弹性网络（elastic-net），它非常强大，可以用于**线性回归、逻辑回归、多项式回归、泊松回归、Cox模型、多响应高斯模型和分组多项式回归**的Lasso或弹性网络正则化路径拟合，并且效率极高。

我们主要介绍它的lasso回归功能，主要是因为lasso可以把变量的系数变为0，达到筛选变量的目的。并且我们会以逻辑回归和COX回归的lasso为例进行演示。

在进行演示前，有一些基础知识需要大家提前了解。

对于一些回归模型来说，变量的系数可以说明变量的重要程度，所以如果某个变量的系数是0，那么说明这个变量不太重要。lasso回归就可以通过算法让其中一些不重要变量的系数变成0，达到筛选变量的目的。让系数变小，就是大家常说的对系数进行**惩罚（penalty）**，也被称为**正则化（regularization）**。具体实现方式大家可以自己去学习复杂的公式~

正则化一般有2种，也就是**L1正则化**和**L2正则化**，又称为**L1范数**和**L2范数**。如果使用L1正则化，就是lasso回归，使用L2正则化就是岭回归。

在`glmnet`包中，`lambda`是总的正则化程度，该值越大惩罚力度越大，最终保留的变量越少，模型复杂度越低；alpha是L1正则化的比例，当alpha=1时，就是lasso，当alpha=0时，就是岭回归，当0<alpha<1时，就是弹性网络。

## 安装

```{r,eval=FALSE}
install.packages("glmnet")
```

## 建模

```{r}
library(glmnet)
```

用一个二分类数据进行演示，因为大家最常用的就是二分类数据和生存数据了。

```{r}
data(BinomialExample)
x <- BinomialExample$x
y <- BinomialExample$y

dim(x)
class(x)
x[1:4,1:4]

class(y)
head(y)
```

注意`glmnet`需要的自变量格式，需要是`matrix`或者`稀疏矩阵`格式！

`family`用来指定不同的模型类型，对于二分类数据，应该选择`binomial`。

`family`的其他选项如下："gaussian"（默认）, "poisson", "multinomial", "cox", "mgaussian"。

建立模型就是1句代码，非常简单：

```{r}
fit <- glmnet(x, y, family = "binomial")
```

官方不建议直接提取`fit`中的元素，因为提供了`plot`，`print`，`coef`，`predict`方法帮助大家探索结果。

## 可视化

可视化各个变量系数的变化，这个图是大家最常见的图形之一：

```{r,fig.asp=0.8}
plot(fit,label = T)
```

这个图形中的每一条线都代表1个变量，并且展示了在不同的L1范数（L1 Norm）下该变量的系数变化。这个图下面的横坐标是L1范数，上面的横坐标是L1范数下对应的非零系数的个数，比如当L1范数是20时，对应的非零系数有27个，也就是此时可以有27个变量保留下来。左侧纵坐标是变量的系数值。

这里的`plot()`函数还有一个`xvar`参数，可以用于指定不同的横坐标：

- `norm`：横坐标是L1 norm，这个是默认值；
- `lambda`：横坐标是log-lambda；
- `dev`：横坐标是模型解释的%deviance

```{r,fig.asp=0.8}
plot(fit, xvar = "lambda")
```

这里的横坐标是log-lambda，可以看做是正则化程度。

上面这幅图展示了随着lambda值的变化，每个变量系数的变化，可以看到随着lambda值变大，系数值逐渐减小，直至为0，上面的横坐标也显示随着lambda值变大，保留的变量数量也越来越少。

```{r,fig.asp=0.8}
plot(fit, xvar = "dev", label = TRUE)
```

这幅图和上面图的解释是一样的，只有下面的横坐标不一样。

最后一幅图下面的横坐标是模型解释的偏差百分比，也可以用来衡量模型复杂度。可以看出在图形的右侧部分，模型能够解释的偏差百分比基本变化不大，但是模型系数基本都是往上或往下“飘”的很厉害。

虽然官方不建议提取数据，但是很明显大家都喜欢提取数据再自己美化图片，我之前也介绍过一种简便方法，可以实现自定义美化图形：[lasso回归结果美化](https://mp.weixin.qq.com/s/JKFis5gyFeBjxPZIJr50fg)

## 打印结果

使用`print(fit)`可以查看不同lambda值对应的自由度和模型能够解释的偏差百分比：

```{r}
print(fit) # 直接fit也可
```

左侧的`df`是非零系数的个数，中间的`%Dev`是模型解释的偏差百分比，右侧的`Lambda`是总惩罚值大小。

默认情况下，`glmnet()`函数中的`nlambda`参数的取值是100，也就是会取100个不同的Lambda值，但是如果`%Dev`变化不大或者不再变化，它可能会提前停止，取不到100个值，比如我们这个例子就是这样。

## 查看变量系数

我们可以通过`coef()`查看某个Lambda值下的变量系数：

```{r}
coef(fit, s = 0.065380)
```

可以看到此时一共有12个变量的系数不是0，和上面`print(fit)`的结果是一样的。

这里使用了`s`表示lambda，为什么不直接用`lambda`呢？这是作者为了以后的某些功能做准备（截止到20240428也没见到这些功能），但是这一点在`tidymodels`中大受诟病...

也可以同时指定多个lambda值：

```{r}
coef(fit, s = c(0.065380,0.078750))
```

除此之外，`coef()`还有一个`exact`参数，如果`exact = TRUE`，那么当一个lambda不在默认的lambda值中时，函数会重新使用这个lambda值拟合模型然后给出结果，如果`exact = FALSE`（默认值），那么会使用线性插值给出结果。

举个例子，0.08并不在lambda值向量中：

```{r}
# 可以看前面的print(fit)的结果，看看lambda的取值有哪些
any(fit$lambda == 0.08)
```

此时两种情况下的系数是不太一样的：

```{r}
coef.apprx <- coef(fit, s = 0.08, exact = FALSE)
coef.exact <- coef(fit, s = 0.08, exact = TRUE, x=x, y=y)
cbind2(coef.exact[which(coef.exact != 0)], 
       coef.apprx[which(coef.apprx != 0)])
```

注意在使用`exact = TRUE`时，需要提供`x`和`y`，因为需要重新拟合模型。

## 预测新数据

对于新数据，可直接使用`predict()`进行预测，此时也是可以指定lambda值的：

```{r}
nx <- head(x) #随便准备的新的测试数据

predict(fit, newx = nx, s = c(0.065380,0.078750))
```

由于`glmnet`包可以用于线性回归、逻辑回归、cox回归、泊松回归、多项式回归等（通过参数`family`指定即可，默认值是`gaussian`，可通过`?glmnet`查看帮助文档），所以在`predict()`时，`type`参数略有不同，对于逻辑回归，`type`可以是以下3种：

- `link`：线性预测值，默认是这个
- `response`：预测概率
- `class`：预测类别

如果要获得预测概率：

```{r}
predict(fit, newx = nx, s = c(0.065380,0.078750), type = "response")
```

可以通过`?predict.glmnet`查看帮助文档。

## 交叉验证

`glmnet()`函数会返回多个模型（因为会使用多个lambda值，这个过程其实就是超参数调优的过程），**但是很多情况下，用户并不知道到底选择哪一个lambda值，即不知道到底保留哪些变量，或者希望函数能自动给出结果**。

所以`glmnet`包提供了交叉验证法，帮助用户做出选择，使用方法也非常简单：

```{r}
cvfit <- cv.glmnet(x, y)
```

除了`glmnet()`中的参数之外，`cv.glmnet()`还有一些独有的参数：

- `nfolds`：交叉验证的折数，默认是10折交叉验证；
- `foldid`：指定哪个观测在哪一折中，一般用不到；
- `type.measure`：模型性能指标，对于不同的`family`，也是略有不同，可查看帮助文档

对于逻辑回归，`type.measure`可以是以下取值：

- `mse`：均方误差；
- `deviance`：偏差；
- `mae`：平均绝对误差，mean absolute error；
- `class`：错分率；
- `auc`：只能用于**二分类**逻辑回归

### plot方法

对于`cv.glmnet()`的结果，也提供了`plot`，`print`，`coef`，`predict`方法。

```{r}
plot(cvfit)
```

该图形下面的横坐标是log10(lambda)，上面的横坐标是非零系数的数量，左侧的纵坐标是MSE（均方误差），改图展示了不同lambda取值下MSE的变化以及MSE±1倍标准差的置信区间。

图中的两条竖线就是函数帮你挑选的两个结果，一个是`lambda.min`，此时的lambda值可以使得MSE最小，另外一个是`lambda.1se`，此时的lambda值可以使得MSE在最小MSE的1倍标准误区间内，但是同时可以使模型的复杂度降低。（在模型误差之间的差距不是很大的时候，我们肯定是喜欢更简单的模型啦，这个不难理解吧？）

查看这两个lambda值：

```{r}
cvfit$lambda.min
cvfit$lambda.1se
```

换一个`type.measure`试试看：

```{r}
cvfit1 <- cv.glmnet(x, y, family = "binomial", type.measure = "auc")
plot(cvfit1)
```

这个图的解读和上面那个图的解读也是一样的，只不过左侧纵坐标不一样而已。

交叉验证的图形也是可以自己美化的，参考推文：[lasso回归结果美化](https://mp.weixin.qq.com/s/JKFis5gyFeBjxPZIJr50fg)

### coef方法

查看这两个取值下保留的非零系数情况：

```{r}
# 此时s不能同时使用多个值
coef(cvfit, s = "lambda.min")
coef(cvfit, s = "lambda.1se") # 这个是默认值
```

可以看到`coef()`的结果都是稀疏矩阵格式，这种格式计算效率更高，但是不方便后续使用，可以使用`as.matrix()`转换为矩阵格式：

```{r}
as.matrix(coef(cvfit))
```

### predict方法

对新数据进行预测也是一样的用法：

```{r}
predict(cvfit, newx = x[1:5,], s = "lambda.min")
```

## 一些参数解释

- `alpha`：可以看做是L1正则化的比例，当alpha=1时，就是lasso，当alpha=0时，就是岭回归，当0<alpha<1时，就是弹性网络。
- `weights`：不同观测的权重，默认都是1。（`glmnet`会自动对权重进行重新标准化，使得所有观测的权重相加等于样本数量）。
- `nlambda`：lambda的取值个数，默认是100。
- `lambda`：用户可以通过这个参数自己指定lambda的取值。
- `standardize`：逻辑值，是否在拟合模型前对自变量进行标准化，默认是`TRUE`。

下面是一个对不同观测自定义权重的示例。

我们这个示例中，样本量是100，所以我们为100个观测自定义以下权重：

```{r}
# 简单定义一下，前50个是1，后50个是2
wts <-  c(rep(1,50), rep(2,50))
fit1 <- glmnet(x, y, alpha = 0.2, weights = wts, nlambda = 20)

print(fit1)
```

可以看到结果中只有17个lambda值，少于我们指定的20个，原因已经在前面解释过了。

## 在测试集评估模型

模型建立后，我们可能会使用测试集检测模型性能，`glmnet`包为我们提供了`assess.glmnet`，`roc.glmnet`，`confusion.glmnet`，帮助我们快速在衡量模型性能。

### assess.glmnet()

还是使用这个二分类数据，我们把前70个观测作为训练集，用来建模，后30个观测作为测试集。

```{r}
data(BinomialExample)
x <- BinomialExample$x
y <- BinomialExample$y
itrain <- 1:70 # 前70个作为训练集
fit <- glmnet(x[itrain, ], y[itrain], family = "binomial", nlambda = 6)

# 在测试集评估模型
assess.glmnet(fit, newx = x[-itrain, ], newy = y[-itrain])
```

结果是一个列表，里面是`family = "binomial"`时，5个性能指标在不同lambda值下的结果，由于我们这里指定了只使用6个lambda值，所以结果就是6个，你指定几个，结果就会有几个。

不同的`family`对应着不同的性能指标，可以通过`glmnet.measures()`查看每个`family`对应的性能指标：

```{r}
glmnet.measures()
```

交叉验证同样也是适用的：

```{r}
cfit <- cv.glmnet(x[itrain, ], y[itrain], family = "binomial", nlambda = 30)
assess.glmnet(cfit, newx = x[-itrain, ], newy = y[-itrain])
```

不过此时默认使用的lambda值是`lambda.1se`，也可以使用`lambda.min`：

```{r}
assess.glmnet(cfit, newx = x[-itrain, ],newy = y[-itrain], s = "lambda.min")
```

当然也可以获取**训练集**的各种指标，只要在建模时使用`keep=TRUE`参数即可：

```{r}
cfit <- cv.glmnet(x, y, family = "binomial", keep = TRUE, nlambda = 3)
assess.glmnet(cfit$fit.preval, newy = y, family = "binomial")
```

### roc.glmnet()

对于二分类数据，ROC曲线是非常重要的模型衡量工具。

`roc.glmnet()`可以快速计算出画ROC曲线需要的数据，然后使用`plot()`画图即可。

```{r}
fit <- glmnet(x[itrain,], y[itrain], family = "binomial")

rocs <- roc.glmnet(fit, newx = x[-itrain,], newy=y[-itrain])
```

这个`rocs`是一个列表，其长度就是lambda值的数量，对于每一个lambda取值，它都计算了可以用来画ROC曲线的数据。

我们随便取其中一个画出来：

```{r,fig.asp=1}
plot(rocs[[3]],type = "l",xlim=c(0,1),ylim=c(0,1))
invisible(sapply(rocs, lines)) # 把所有的ROC都画出来
abline(0,1,col="grey")
```

交叉验证的结果当然也是可以的：

```{r}
# 建立模型
cfit <- cv.glmnet(x, y, family = "binomial", type.measure = "auc", 
                  keep = TRUE)

# 计算画ROC曲线需要的数据
rocs <- roc.glmnet(cfit$fit.preval, newy = y)

class(rocs)
length(rocs)
dim(rocs[[1]])
head(rocs[[1]])
```

这个`rocs`也是一个列表，其长度就是lambda值的数量，对于每一个lambda取值，它都计算了可以用来画ROC曲线的数据。

下面我们把AUC最大的ROC曲线画出来，用红色标记，并把其他ROC曲线也画在一起：

```{r,fig.asp=1}
best <- cvfit$index["min",] # 提取AUC最大的lambda值
plot(rocs[[best]], type = "l") # 画出AUC最大的ROC曲线
invisible(sapply(rocs, lines, col="grey")) # 把所有的ROC都画出来
lines(rocs[[best]], lwd = 2,col = "red") # 把AUC最大的标红
```

### confusion.glmnet()

混淆矩阵作为分类数据必不可少的工具，可以通过`confusion.glmnet()`实现。

用一个多分类数据进行演示。

```{r}
data(MultinomialExample)
x <- MultinomialExample$x
y <- MultinomialExample$y
set.seed(101)
itrain <- sample(1:500, 400, replace = FALSE)
cfit <- cv.glmnet(x[itrain, ], y[itrain], family = "multinomial")

# 默认lambda值是lambda.1se
cnf <- confusion.glmnet(cfit, newx = x[-itrain, ], newy = y[-itrain]) 

print(cnf)
```

如果使用`keep=TRUE`，那么结果也是多个混淆矩阵，此时也可以选择任意一个进行展示：

```{r}
cfit <- cv.glmnet(x, y, family = "multinomial", type = "class", keep = TRUE)
cnf <- confusion.glmnet(cfit$fit.preval, newy = y, family = "multinomial")
best <- cfit$index["min",]
print(cnf[[best]])
```

虽然`glmnet`包提供了这3个函数帮助我们查看模型性能，但是很明显不能满足大家的需求，所以一般情况下我们都用其他的R包来代替这几个函数了，比如`caret`，`yardstick`，`pROC`等。

## 其他功能

### 拟合非正则化的广义线性模型

`glmnet`包提供了`bigGlm()`函数，可以对大型数据拟合非正则化的广义线性模型，类似于常规的`glm()`，但是支持`glmnet`中的所有参数。其实此时的`lambda=0`，也就是不进行正则化。如果你的数据巨大，使用`glm`很慢，或者你需要其他参数，可以尝试一下`bigGlm()`。

以下是一个使用示例：

```{r}
data(BinomialExample)
x <- BinomialExample$x
y <- BinomialExample$y
fit <- bigGlm(x, y, family = "binomial", lower.limits = -1)
print(fit)
```

### 修改自变量矩阵格式

`glmnet`包提供了一个`makeX()`函数，可以对自变量的格式进行修改，比如，如果你提供了1个数据框`data.frame`，这个格式是不行的，它可以帮你转换为`matrix`格式，除此之外，还可以进行如下操作：

- 对因子型或字符型变量进行独热编码；
- 使用均值填补缺失值；
- 可以直接变为稀疏矩阵，适合大数据；
- 可以直接提供训练集和测试集两个数据集，这样可以保证两个数据集的因子水平对应，以及使用训练集中的均值对测试集进行插补

先展示下独热编码转换功能，我们建立一个数据框，其中两列是字符型，`makeX`可以帮我们进行独热编码，并把数据变为稀疏矩阵格式：

```{r}
set.seed(101)
X <- matrix(rnorm(5), nrow = 5)
X2 <- sample(letters[1:3], 5, replace = TRUE)
X3 <- sample(LETTERS[1:3], 5, replace = TRUE)
df <- data.frame(X, X2, X3)
df
makeX(df) # 转换
```

添加`sparse=T`可以返回稀疏矩阵格式：

```{r}
makeX(df, sparse = TRUE)
```

下面我们对原数据框添加一些缺失值，用来演示`makeX`的缺失值插补功能：

```{r}
Xn  <- X ; Xn[3,1] <- NA
X2n <- X2; X2n[1]  <- NA
X3n <- X3; X3n[5]  <- NA
dfn <- data.frame(Xn, X2n, X3n)
dfn
```

通过添加`na.impute=T`可以进行插补：

```{r}
makeX(dfn,na.impute = T)
```

这个函数总体来说还是挺方便的。

### 添加进度条

`glmnet()`和`cv.glmnet()`都可以通过添加`trace.it=TRUE`实现进度条功能：

```{r}
fit <- glmnet(x, y, trace.it = TRUE)
```

```{r}
fit <- cv.glmnet(x, y, trace.it = TRUE)
```

也可以通过以下方式实现：

```{r,eval=FALSE}
glmnet.control(itrace = 1) # 变成0就不显示了~
```

## 扩展：正则化Cox回归

正则化的COX回归，也就是`glmnet`在生存分析中的应用，这里我们还是以lasso为例进行演示。

`glmnet`包的详细使用介绍已经在前面都介绍过了，正则化的COX回归并没有太大的不同，所以这里简单介绍一下。

下面是一些理论解释，大家随便看看就好。

在`glmnet`中，我们使用弹性网络（elastic net）方法对部分似然的负对数进行惩罚。

部分似然（partial-likelihood）是一种用于处理生存分析（survival-analysis）中右侧截尾（right-censored）观测的方法。而负对数部分似然（negative-log-partial-likelihood）则是对部分似然取反并求对数，目的是将最大化似然函数的问题转化为最小化负对数似然函数的问题。

为了进一步约束模型的复杂度和提高模型的泛化能力，我们在负对数部分似然的基础上引入了弹性网络惩罚（elastic-net-penalty）。弹性网惩罚结合了L1正则化（L1-regularization）和L2正则化（L2-regularization）的特性，从而既能产生稀疏解，又能保留一些高度相关的特征。这样我们可以在建立模型时在部分似然的基础上，使用弹性网惩罚来进行模型的优化和参数选择，以提高模型的性能和泛化能力。

### 基础使用

`glmnet`对数据格式是有要求的，之前也说过，`x`必须是由自变量组成的`matrix`，`y`可以是一个两列的`matrix`，两列的列名必须是`time`和`status`，分别表示生存时间和生存状态，其中`status`必须使用0和1组成，0表示删失，1表示发生终点事件（又叫失效事件，比如死亡）。除此之外，`y`还可以是由`Surv()`函数生成的对象。

下面是一个示例数据：

```{r}
library(glmnet)
library(survival)

data(CoxExample)
x <- CoxExample$x
y <- CoxExample$y

# 查看y的数据格式
y[1:5, ]
```

建立模型，只需要使用`family = "cox"`即可：

```{r}
fit <- glmnet(x, y, family = "cox")
```

其中的一些参数比如`alpha`，`weights`，`nlambda`等，在前面已经介绍过了，这里就不再多介绍了。

可视化、提取系数、预测新数据和之前介绍的用法也是一模一样，这里也不再多说了。

### 交叉验证

对于正则化的cox来说，`cv.glmnet()`中的`type.measure`只能是`"deviance"`（默认值，给出部分似然），或者`"C"`，给出 Harrell-C-index。

```{r}
set.seed(1)
cvfit <- cv.glmnet(x, y, family = "cox", type.measure = "C")

print(cvfit)
```

画图也是一样的，下面这幅图的解释在前面也已经详细介绍过了，这里就不再多做解释了：

```{r,fig.asp=0.8}
plot(cvfit)
```

>在glmnet中，对于生存时间的排列相同（ties）情况，使用的是Breslow近似（Breslow approximation）。这与survival软件包中的coxph函数的默认排列处理方法（tie-handling method）——Efron近似（Efron approximation）不同。
>
>当存在相同的生存时间观测时，例如多个个体在同一时间发生事件，排列的处理方法对估计结果和推断的准确性至关重要。Breslow近似与Efron近似是最常见的两种处理方法。
>
>在glmnet中，使用Breslow近似处理排列，该方法假设所有的排列发生在后一事件之前的所有时间上。这种近似方法在计算效率上比较高，但可能会导致估计的偏差。
>
>而在survival软件包中的coxph函数，默认使用的是Efron近似处理排列。Efron近似方法基于考虑排列发生的时间顺序进行调整，更接近真实的结果，但在计算过程中稍微耗时一些。
>
>因此，当在glmnet和survival软件包中处理生存分析时，需要注意到在处理排列的方法上的差异，以确保得到准确和一致的结果。

### 分层COX

`coxph()`支持`strata()`函数，因为它是使用公式形式的，但是`glmnet`不支持公式形式，只能使用`x/y`形式的输入，所以如果要实现分层，需要使用`stratifySurv()`。

继续使用上面的示例数据，我们把1000个观测分成5层：

```{r}
# 把1000个观测分5层
strata <- rep(1:5, length.out = 1000)
y2 <- stratifySurv(y, strata) # 对y进行分层
str(y2[1:6])
```

接下来把`y2`提供给`glmnet()`或者`cv.glmnet()`就可以实现正则化的分层COX了。

```{r,fig.asp=0.8}
fit <- glmnet(x, y2, family = "cox")

cv.fit <- cv.glmnet(x, y2, family = "cox", nfolds = 5)
plot(cv.fit)
```

### 生存曲线

`glmnet`的结果可以直接提供给`survfit()`使用，可以用来画生存曲线。这里简单介绍一下，大家知道即可，因为大家在平时写文章时根本不会这么用......

以下是一个示例。

```{r}
data(CoxExample)
x <- CoxExample$x
y <- CoxExample$y

y <- Surv(y[,1],y[,2]) # 需要用Surv转换格式

fit <- glmnet(x, y, family = "cox")
survival::survfit(fit, s = 0.05, x = x, y = y)
```

直接画图即可：

```{r,fig.asp=0.8}
plot(survival::survfit(fit, s = 0.05, x = x, y = y))
```

这个生存曲线有些奇怪，因为数据原因，大家可以自己尝试下。

基于新的数据画生存曲线也是可以的：

```{r}
plot(survival::survfit(fit, s = 0.05, x = x, y = y, newx = x[1:3, ]))
```

## 其他

- [tidymodels实现lasso回归及超参数调优](https://mp.weixin.qq.com/s/UdGPhQ3tvkvpUHuPmLgOVA)
- [lasso回归的列线图、内部验证、外部验证](https://mp.weixin.qq.com/s/q04OeFQsd346tvHDlCMAJQ)

