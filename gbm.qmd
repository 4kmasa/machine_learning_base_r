---
title: "梯度提升机GBM"
---

提升法（boosting）是三大集成方法之一（另外两种是袋装法bagging和堆叠法stacking），提升法发展出了非常多的分支，梯度提升（gradient boosting）就是其中一种。GBM就是基于梯度提升实现的一种算法。

GBM（Gradient boosting machines）的参数主要是两类，

- 和提升法有关的参数：
  - 树的数量（number of trees）
  - 学习率（learning rate）
- 和树模型有关的参数：
  - 树的深度（tree depth）
  - 叶子节点的最小样本量（Minimum number of observations in terminal nodes）

在R中有非常多的R包可以实现GBM以及由GBM发展而来的其他模型，其中最著名的是`gbm`包。

但是目前`gbm`包已经停止更新了,新的项目是`gbm3`,但是今天还是主要讲解下`gbm`的用法.

## 安装

```{r,eval=FALSE}
install.packages("gbm")
```

## 准备数据

使用皮玛印第安人糖尿病数据集。这个一个分类数据,其中`diabetes`是结果变量,`pos`表示有糖尿病,`neg`表示没有糖尿病,`gbm`要求结果变量必须用数字1和数字0表示,不能是字符型或者因子型,所以我们改一下,并按照7:3的比例划分训练集和测试集:

```{r,collapse=TRUE}
rm(list = ls())

load(file = "datasets/pimadiabetes.rdata")
pimadiabetes$diabetes <- ifelse(pimadiabetes$diabetes=="pos",1,0)

# 划分是随机的，设置种子数可以让结果复现
set.seed(123)
ind <- sample(1:nrow(pimadiabetes), size = 0.7*nrow(pimadiabetes))

# 去掉真实结果列
train <- pimadiabetes[ind,]
test <- pimadiabetes[-ind,]

dim(train)
dim(test)

head(train)
```

## 拟合模型

首先加载R包:

```{r}
library(gbm)
```

该包的默认学习率（`shrinkage`）是0.001，学习率越小，需要的树的数量就越多，但是默认的树的数量（`n.trees`）是100，这个数量是偏小的。默认的树的深度（interaction.depth）是1。

下面我们建立一个`gbm`模型，注意因变量需要是0/1这种数值型：

```{r}
set.seed(123)  
gbm1 <- gbm(
  formula = diabetes ~ .,
  data = train,
  distribution = "bernoulli", # 回归数据选"gaussian"
  n.trees = 100,
  shrinkage = 0.1,
  interaction.depth = 3,
  n.minobsinnode = 10,
  cv.folds = 10,
  train.fraction = 0.7
)

gbm1
```

结果表明,交叉验证法的最佳的树的数量是29,测试集法的最佳的树的数量是42.

## 结果探索

`gbm1$fit`是训练集的结果,但是并不是直接的类别或者预测概率,对于不同的`distribution`有不同的含义,可查看帮助文档`?gbm.object`

```{r}
head(gbm1$fit)
length(gbm1$fit)
```

`gbm.perf`可用于展示模型性能随树的数量的变化趋势,首先看看使用测试集的结果:

```{r}
# 使用0.3的测试集,train.fraction = 0.7
best.iter <- gbm.perf(gbm1, method = "test")

print(best.iter)
```

此时最佳的树的数量是42.

再看看使用交叉验证法的情况:

```{r}
# plot error curve
best.iter <- gbm.perf(gbm1, method = "cv")

print(best.iter)
```

此时最佳的树的数量是29.

再看看使用袋外数据的情况,结果中还贴心的给出了提示:

```{r}
best.iter <- gbm.perf(gbm1, method = "OOB")

print(best.iter)
```

`summary.gbm`可用于计算每个变量的相对影响,还可以画图:

```{r}
summary(gbm1, n.trees = 1)  # 使用1棵树
```

结果显示`insulin`/`mass`/`glucose`这3个变量影响最大.

再看看使用最佳树的数量:

```{r}
summary(gbm1, n.trees = 29)  
```

可以看到变量的影响发生了一些变化,但是最重要的前3个还是没变.

`pretty.gbm.tree`函数可以提取每棵树的具体信息,但其实作用不大,帮助文档也说这个函数主要是为了debug和满足某些用户的好奇心!

```{r}
print(pretty.gbm.tree(gbm1, i.tree = 1)) # 选择第一棵树
```

直接对`gbm`使用`plot`方法竟然是画出某个变量的部份依赖图(partial dependence plots,PDP)!

PDP是一种模型解释方法，我专门写文章介绍过，可参考合集：[R语言模型解释](https://mp.weixin.qq.com/mp/appmsgalbum?__biz=MzUzOTQzNzU0NA==&action=getalbum&album_id=3302133732023369734&scene=173&subscene=&sessionid=svr_0d47bdc8565&enterid=1714905817&from_msgid=2247501825&from_itemidx=1&count=3&nolastread=1#wechat_redirect)

比如我们画一下第一个变量的部份依赖图,展示了`pregnant`这个变量对结果变量的贡献.

```{r}
plot(gbm1, i.var = 1, n.trees = 29)
```

也可以直接使用变量的名字:

```{r}
plot(gbm1, i.var = "insulin", n.trees = 29)
```

还可以同时展示两个变量的部分依赖图,比如选择前两个变量:

```{r}
plot(gbm1, i.var = 1:2, n.trees = best.iter)
```

或者直接使用变量名称:

```{r}
plot(gbm1, i.var = c("insulin", "pregnant"), n.trees = 29)
```

还可以同时展示3个变量:

```{r}
plot(gbm1, i.var = c(1, 2, 6), n.trees = 29,
     continuous.resolution = 20)
```

## 测试集预测

`predict.gbm`用于对新数据进行预测,返回预测向量。默认情况下，返回的结果为f(x)。 例如，如果distribution选择伯努利，返回值是log-odds，而coxph则采用log-hazard,如果是泊松分布也是返回log尺度的值.

`type="response"`只对伯努利分布和泊松分布有效,如果是伯努利则返回类别概率,如果是泊松分布则返回预期计数。

我们这个数据是二分类，所以distribution选择伯努利，此时我们可以计算预测概率：

```{r}
# 返回概率
pred <- predict.gbm(gbm1, newdata = test, 
                    n.trees = 29,
                    type = "response")

head(pred)
```

随手画个ROC曲线，就是需要真实结果和预测概率而已！

```{r,message=FALSE,fig.asp=1}
library(pROC)

# 提供真实结果，预测概率
rocc <- roc(test$diabetes, pred)
rocc

plot(rocc, 
     print.auc=TRUE, 
     auc.polygon=TRUE, 
     max.auc.polygon=TRUE, 
     auc.polygon.col="skyblue", 
     grid=c(0.1, 0.2), 
     grid.col=c("green", "red"), 
     print.thres=TRUE)
```

我们可以自己把概率转换为类别,比如规定概率大于0.5就是类别1,否则就是类别2:

```{r}
pred_type <- ifelse(pred > 0.5,1,0)
head(pred_type)
```

这样就可以得到混淆矩阵以及其他指标了:

```{r}
caret::confusionMatrix(factor(test$diabetes), factor(pred_type))
```

## 超参数调优

GBM对各种超参数很敏感，比随机森林的调参更加复杂，

通常的调参策略：

- 先选择一个相对较高的学习率。一般来说，默认值0.1就可以了，但是建议在0.05和0.2之间尝试。
- 确定此学习率下树的最佳数量。
- 在固定树的数量的情况下，再尝试微调学习率，便于在速度与性能之间取得平衡。
- 调整树相关的参数以确定学习率。
- 确定树相关的参数后，适当降低学习率以评估准确性有无改进。
- 使用最终的超参数设置和增加交叉验证的折数来获得更稳健的估计。

我们可以自己写for循环实现,也可以借助`caret`/`mlr3`实现,`tidymodels`暂不支持`gbm`引擎.

由于梯度提升模型有更多更好的选择,比如xgboost/lightgbm等,所以我们就不详细演示`gbm`的调优了,感兴趣自己试一下即可.

