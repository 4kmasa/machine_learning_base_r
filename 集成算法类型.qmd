---
title: "集成算法简介"
---

集成模型简单来说就是集合多个模型的结果来给出最终预测的方法。一个模型的预测结果可能出错的几率很大，我集合1000个甚至10000个模型的结果，那么出错的几率就会明显下降。所以集成模型是一种“群体的智慧”。

>人多力量大！

一个集成模型可能是由几十个、几百个、几千个单一模型构成的，这些单一的模型被称为基模型（base model，base learner），或者子模型（single model），或者弱分类器（weak model）、弱学习器（weak learner）、弱评估器（weak estimator）。我们之前介绍过的[随机森林](https://mp.weixin.qq.com/s/T2SSdL5OTkuVcx7bTRwloQ)、[xgboost](https://mp.weixin.qq.com/s/f7J1To_Cj3u5D3dM-F6YTg)、[lightGBM](https://mp.weixin.qq.com/s/TaIxZg8f_F-XMoAoBfR_xQ)，[catboost]()都是集成模型的一种。

集成模型在具体实现时方法又有很多种,根据这些方法的不同又可以有多种分类方法。比如可以分成：

- 袋装法（Bagging）
- 提升法（Boosting）
- 堆叠法（Stacking）

[随机森林](https://mp.weixin.qq.com/s/T2SSdL5OTkuVcx7bTRwloQ)可以看成是一种特殊的袋装法，但有些时候会把**随机森林(random-forest)**单独作为一种集成方法，有时还会再增加一种融合法（Blending）。

各种集成算法都和树模型（比如决策树）有非常紧密的联系。

集成模型最大的优点自然就是准确率高，因为毕竟是集合了超多个模型的结果。

## 袋装法

袋装法又称为自助聚合法(boosting-aggregating)。袋装的过程和[自助法重抽样](https://mp.weixin.qq.com/s/sUioO8DJS8b0ahs8p55AhA)的过程非常像，比如，一个数据集有100个样本，每次随机抽取1个，然后放回，再随机抽取1个，再放回，再随机抽取1个，再放回，这样的过程重复100次，就得到了一个和原数据集样本量相等的抽样数据集，这个抽样数据集就叫做自助集。

由于每次都是有放回然后再随机抽取，所以一个自助集中可能有多个同一样本！所以就有可能在100次随机抽取中，有一些没被抽中过的样本，这些样本就被称为袋外样本(out-of-bag)，其中被抽中的样本(也就是自助集)用于训练模型，袋外样本用来评估模型表现。

袋装法的典型代表就是随机森林算法。

随机森林只有一点和上面的过程不同，那就是在建立模型时使用的预测变量（特征）也是随机选择的，并不是每次都使用全部的预测变量（特征）。

## 提升法

提升法也是通过集合多个基模型的结果来给出更准确的预测，但是提升法可以对每次学习出错的样本给予更多的权重，也就是对于这些没预测对的样本进行加强学习，从而获得更好的结果。所以袋装法是并行训练的，但是提升法是顺序学习的，后一个学习器需要以前一个学习器的结果为基础。

提升法的典型代表是`Gradient boosting`（梯度提升法）和`Adaboost`（自适应提升法）。不过目前来看还是梯度提升法发展的更好。

AdaBoost 是通过提升错分数据点的权重来定位模型的不足，而Gradient Boosting 是通过算梯度（gradient）来定位模型的不足。因此相比 AdaBoost，Gradient Boosting可以使用更多种类的目标函数。

如果Gradient Boosting使用的基模型是CART（classification and regression tree），就被称为Gradient Boosting Decision Tree（GBDT）。

你可能还见过GBRT，可以简单理解为当GBDT用于回归模型时，是梯度提升回归（regression）树GBRT，二者的区别主要是损失函数不同。

梯度提升法目前有以下几个不同的算法：

- [GBM](https://mp.weixin.qq.com/s/JlKPcsMexDV13YpNfHcRYg)
- xgboost
- lightgbm
- catboost

[CatBoost]()和[xgboost](https://mp.weixin.qq.com/s/f7J1To_Cj3u5D3dM-F6YTg)、[LightGBM](https://mp.weixin.qq.com/s/TaIxZg8f_F-XMoAoBfR_xQ)并称为GBDT的三大主流神器，都是在GBDT算法框架下的一种改进实现。

XGBoost是陈天奇等人开源的一个机器学习项目，LightGBM是由微软开发的算法，CatBoost是俄罗斯的搜索巨头Yandex开源的机器学习库。它们在R语言中的实现我们都介绍过，可以在公众号后台回复**提升算法**获取相关推文。

## 堆叠法

堆叠法现在经常和融合法（模型融合）混在一起。它的实现方法简单来说是这样的：先在训练数据中拟合多个模型（可以是多个同一种模型，比如多个支持向量机模型（同质）；也可以是多个不同种模型，比如SVM和决策树（异质）），并拿到这些模型的预测结果，然后使用这些预测结果作为训练数据再拟合一个新的模型，这个新的模型被叫做超模型或者元模型（meta model）。

当然具体实现起来还是挺复杂的，没有我说的这么简单。目前在R中堆叠法可以通过`mlr3`或者`tidymodels`实现，后续我会写一些教程。

## 参考资料：

- tidymodeling with R
- 典型Stacking方法图解：https://zhuanlan.zhihu.com/p/48262572

