---
title: "决策树"
---

>本文部分内容节选自[随机生存森林合集](https://mp.weixin.qq.com/mp/appmsgalbum?__biz=MzUzOTQzNzU0NA==&action=getalbum&album_id=2699591889800560640&from_itemidx=1&from_msgid=2247498838&scene=173&count=3&nolastread=1#wechat_redirect)

## 算法原理

决策树（decision tree）是一种有监督的学习方法，它通过不断的根据条件进行分支来解决分类和回归问题。

下面是一个决策树工作原理的简单示意图，通过多个条件，一层一层的判断某个西瓜是否是“好瓜”还是“坏瓜”：

![](figs/PixPin_2024-04-26_20-06-48.png)

决策树的结构就像一棵树，通过不断的询问**是或者否**来对样本进行预测。

最开始进行分支的问题被称为**根节点**（root node），上图中的**脐部=？**就是根节点。最终的结论，也就是“好瓜”、“坏瓜”是**叶子节点**（leaf node）或者叫**终末节点**（terminal node），中间的每个问题都是**中间节点**。

在上下两个相邻的节点中，更靠近根节点的是**父节点**（parent node），另一个是**子节点**（child node）。

对于决策树模型来说，数据中的任何一个预测变量都可以当做是其中一个中间节点，可以不断的进行分裂，这样的好处是最终每一个样本都可以得到完美的结果，也就是误差会非常小，但是这样做一个很明显的问题就是容易使模型过拟合。

根据具体的实现方式，决策树包括多种方法，比如：ID3、C4.5、CART。

本文会介绍决策树的R语言实现和超参数调优。我会介绍决策树常见的超参数以及它们的意义，超参数调优的常见方法和调优思路，比如学习曲线、网格搜索等。

## 准备数据和R包

演示数据为印第安人糖尿病数据集，这个数据一共有768行，9列，其中`diabetes`是结果变量，为二分类，其余列是预测变量。

```{r}
rm(list = ls())
library(rpart)
library(rpart.plot)

load(file = "datasets/pimadiabetes.rdata")

dim(pimadiabetes)
str(pimadiabetes)
```

划分训练集、测试集。训练集用于建立模型，测试集用于测试模型表现，划分比例为7:3。

```{r}
# 划分是随机的，设置种子数可以让结果复现
set.seed(123)
ind <- sample(1:nrow(pimadiabetes), size = 0.7*nrow(pimadiabetes))

# 去掉真实结果列
train <- pimadiabetes[ind,]
test <- pimadiabetes[-ind,]

dim(train)
dim(test)
```

## 建立模型

首先在训练集上建立模型，先不进行任何设置，就用默认的，看看效果如何：

```{r}
# 决策树计算过程中有随机性，需设置种子数保证结果能复现
set.seed(123)
treefit <- rpart(diabetes ~ ., data = train)
treefit
```

上面就是一颗建立好的决策树，它给出了每次分支的标准。

我们可以把结果通过图形的方式画出来，默认的画图：

```{r}
par(xpd = TRUE)
plot(treefit,compress = TRUE)
text(treefit, use.n = TRUE)
```

并不是很美观，建立好的决策树的结果还可以通过`rpart.plot`函数画出来：

```{r}
rpart.plot(treefit)
```

这样的结果一目了然，它告诉我们，第一次分支使用了`insulin`这个变量，分支的标准是124.5,分支后每个子节点的样本比例也很清楚的显示了。

这棵树一共分支了8次，树的深度为9层。

也可以使用`partykit`进行可视化：

```{r,message=FALSE,fig.width=16,fig.height=12}
library(partykit)

plot(as.party(treefit))
```

如果你还想要更加花里胡哨的可视化效果，可以试一下我们之前介绍过的`treeheatR`，效果绝对炫酷：[超级炫酷的决策树可视化R包](https://mp.weixin.qq.com/s/yohVxTWi1PkATG9yOdgvOA)

![](figs/PixPin_2024-05-05_19-41-50.png)

## 超参数介绍

这是默认情况下建立好的决策树，没有进行任何的超参数调优，这样的一棵树，会自由地“生长”，直到不能再生长为止。

决策树的超参数有很多，大部分都是和剪枝有关的，通过剪枝参数，可以有效限制树的生长（防止过拟合），这几个参数也是所有和树有关的模型的通用参数，比如随机森林，常见的有：

- **树的深度**：设置树分支的最大深度；
- **使用的特征个数**：设置建立模型时最多使用几个特征；
- **最小分支节点**：如果当前节点的样本数少于某个值就不再往下分支了。

常见的是这几个，其他还有很多，需要深入了解可以自行学习。

这些超参数都可以通过`rpart.control`函数设置。

通过预先设定好这些参数的值，达到防止树过度生长的目的，这种叫做**预剪枝**。

除此之外，`rpart.control`函数中提供了`cp`参数，可以用来实现**后剪枝**。

当决策树复杂度超过一定程度后，随着复杂度的提高，模型的分类准确度反而会降低。因此，建立的决策树不宜太复杂，需进行剪枝。该剪枝算法依赖于复杂度参数`cp`,`cp`随树复杂度的增加而减小，当增加一个节点引起的分类准确度变化量小于树复杂度变化的`cp`倍时，则须剪去该节点。故建立一棵既能精确分类，又不过度拟合的决策树的关键是求解一个合适的`cp`值。一般选择错判率最小值(Xerror)对应的`cp`值来修剪。

`rpart`函数默认是使用十折交叉验证进行计算，也可以通过`rpart.control`函数进行设置

```{r}
# 这里都选择了默认值

set.seed(123)
treefit <- rpart(diabetes ~ ., data = train,
                 control = rpart.control(minsplit = 20, # 最小分支节点
                                         #minbucket = 2, # 分支后最小节点
                                         maxdepth = 30, # 树的深度
                                         cp = 0.01, # 复杂度
                                         xval = 10 # 交叉验证，默认10折
                                         )
                 )
treefit
```

可以看到这棵树和我们最开始建立的模型一模一样，因为我们用了默认的设置，并没有更改。

接下来我们在测试集上看看模型表现：

```{r}
pred <- predict(treefit, newdata = test,type = "class")

# 计算混淆矩阵
caret::confusionMatrix(test$diabetes, pred)
```

混淆矩阵的结果显示准确度Accuracy : 0.6987，效果不是很好！

## 后剪枝

下面我们通过`cp`参数进行后剪枝。

首先可以通过`printcp`函数查看复杂度和误差之间的数值关系：

```{r}
printcp(treefit)
```

`CP`是复杂度参数，`nsplit`是分裂次数，`rel error`是相对误差，即某次分裂的RSS(残差平方和)除以不分裂的RSS，`xerror`是平均误差，`xstd`是交叉验证的标准差

从上面的结果可以看出，CP=0.016043时，分裂次数为6，平均误差和标准差最小，分别是0.66310和0.052222。

下面用图形化的方式展示结果：

```{r}
plotcp(treefit)
```

纵坐标是相对误差，底下的横坐标是复杂度，上面的横坐标是树的规模，上面可以看出树的规模为7(也就是分裂6次)时，相对误差最小，和上面`printcp()`结果相同。

接下来就可以就可以根据`CP`进行剪枝了，通常选择`xerror`最小时的CP值(还有别的选择方法，比如1倍标准差法等)：

```{r}
cp <- treefit$cptable[which.min(treefit$cptable[,"xerror"]), "CP"]
cp
treepruned <- prune(treefit, cp = cp)
treepruned
```

剪枝后的树简单多了，画图展示剪枝后的树：

```{r}
rpart.plot(treepruned)
```

接下来使用修剪后的树再次作用于测试集：

```{r}
pred2 <- predict(treepruned, newdata = test, type = "class")
caret::confusionMatrix(test$diabetes, pred2)
```

通过后剪枝的准确度和不剪枝的结果差不多，但是由于分裂次数明显减少，模型复杂度明显减小了。

这是非常常见的情况，有时候你调优后的模型可能还不如默认的好，有时候却效果超群，原因是多方面的，首先是数据问题，其次是选择的方法。

## 学习曲线

接下来我们看看通过预剪枝能不能让模型的表现更好一点。

如果要**对单个超参数进行调优，我们可以用学习曲线的方式**，学习曲线的横坐标是超参数的各个取值，纵坐标是不同超参数下的模型表现，通过学习曲线，可以找到模型表现较好的超参数值。

我们就以调整`minsplit`为例。

`minsplit`是每个节点的最少样本量，我们训练集一共有537个样本，就设置为2:300先看看情况。

模型的评价指标选择准确率。

```{r}
res <- list()
for(i in 2:300){
  
  f <- rpart(diabetes ~ ., data = train,
                 control = rpart.control(minsplit = i )
             )
  pred <- predict(f, newdata = test,type = "class")
  acc <- caret::confusionMatrix(test$diabetes, pred)[["overall"]][["Accuracy"]]
  df <- data.frame(minsplit = i, accuracy = acc)
  res[[i-1]] <- df
}

acc.res <- do.call(rbind, res)
```

查看准确率的范围：

```{r}
range(acc.res$accuracy)
```

准确率最高是0.7878788，接下来我们把结果画出来：

```{r}
library(ggplot2)

ggplot(acc.res, aes(minsplit, accuracy))+
  geom_point()+
  geom_line()+
  geom_vline(xintercept = 13,linetype = 2, color = "red")+
  theme_bw()
```

这个图就是`minsplit`的学习曲线，从这个学习曲线可以看出，`minsplit`=13的时候，准确度accuracy达到了最大。

我们可以轻松找出准确度最高的`minsplit`值（可能有多个`minsplit`都对应着最高的准确率，但是我们肯定是优先选择最简单的一个）：

```{r}
acc.res[which.max(acc.res$accuracy),]
```

`minsplit`为13时，准确度就已经到最高了。这个结果比后剪枝的结果好一点。

所以我们就选择`minsplit`为13，重新建立决策树模型。

```{r}
# 建立模型
set.seed(123)
treef <- rpart(diabetes ~ ., data = train,
               control = rpart.control(minsplit = 13) # 这里选择13
               )

# 测试集查看效果
pred <- predict(treef, newdata = test,type = "class")

# 测试集的混淆矩阵
caret::confusionMatrix(test$diabetes, pred)
```

决策树的可视化：

```{r}
rpart.plot(treef)
```

## 网格搜索

学习曲线适合于某个单独的超参数调优，如果是多个超参数一起调优还是要使用其他方法，最常见的一种就是网格搜索，网格搜索现在我们有非常成熟的工具包，比如`tidymodels`、`mlr3`、`caret`。

但是今天我想介绍下使用`e1071`包实现网格搜索，这个包功能强大，不仅可以实现支持向量机，而且包含多种调优功能，详情请参考：[R语言支持向量机e1071](https://mp.weixin.qq.com/s/lTff5UT4jQ3oNT0VDSrxzg)

```{r}
library(e1071)

# 默认10折交叉验证
set.seed(456)
tune_obj <- tune.rpart(diabetes ~ ., data = train,
                       minsplit = seq(10,120,5),
                       cp = c(0.0001,0.001,0.01,0.1,1)
                       ) 

tune_obj
```

速度很快，使用也很简单。直接给出最优结果是`minsplit=100`，`cp=0.0001`。

重新拟合模型：

```{r}
rpart_fit <- rpart(diabetes ~ ., data = train,
                   control = rpart.control(minsplit = 100, 
                                         cp = 0.0001, 
                                         xval = 10 
                                         )
                   )
rpart_fit
```

这棵树就简单多了，效果也不错。

接下来我们在测试集上看看模型表现：

```{r}
pred <- predict(rpart_fit, newdata = test,type = "class")

# 计算混淆矩阵
caret::confusionMatrix(test$diabetes, pred)
```

混淆矩阵的结果显示准确度Accuracy : 0.7662 ，效果还可以哦！

## 其他

- [以决策树为例演示超参数调优的基本方法(下)](https://mp.weixin.qq.com/s/ow0Stq8AsW-icpqTrrC89g)
- [R语言机器学习caret-09：决策树的小例子](https://mp.weixin.qq.com/s/O8ddO9N30LwsGpTU4LTYdg)
