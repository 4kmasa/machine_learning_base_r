[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "R语言实战机器学习",
    "section": "",
    "text": "前言",
    "crumbs": [
      "前言"
    ]
  },
  {
    "objectID": "index.html#为什么写这个合集",
    "href": "index.html#为什么写这个合集",
    "title": "R语言实战机器学习",
    "section": "为什么写这个合集？",
    "text": "为什么写这个合集？\n写这个合集的目的是为了帮助，从未接触过或不太了解机器学习的医学生/医生，快速了解机器学习，以及快速上手机器学习实战。\n各种机器学习方法在医学领域中的使用越来越频繁，在各种医学相关的文献期刊中也是越来越常见，但是机器学习对于没有任何计算机背景的医学生/医生来说并不是那么友好。\n\n首先，复杂的概念以及冗长的公式推导让人望而却步；\n其次，目前市面上关于机器学习的书籍或视频资料等大多都是面向计算机相关人员的，对于没有任何计算机背景的人来说学习门槛偏高；\n第三，网络中的资料知识点太过杂乱，没有系统性的整理，质量也良莠不齐，很多代码难以复现，或者使用的方法太过老旧等，东拼西凑的学习体验较差。\n\n所以我基于自己的学习经验以及看过的一些资料，整理了这本R语言机器学习相关的合集。相比于其他资料，个人认为本合集的优势很明显：\n\n本人是外科医生，也是从零开始学的这些东西，我对于各位医学生/医生学习过程中的痛点难点更加熟悉；\n系统整理、有头有尾、从易到难、循序渐进，重点展示大家迫切需要的内容；\n对于一些原理和概念，使用通俗易懂的语言进行解释，没有任何公式推导的内容。",
    "crumbs": [
      "前言"
    ]
  },
  {
    "objectID": "index.html#合集介绍",
    "href": "index.html#合集介绍",
    "title": "R语言实战机器学习",
    "section": "合集介绍",
    "text": "合集介绍\n本合集主要介绍如何使用R语言实现常见的机器学习方法，具体来说，本合集介绍了以下13种方法（后续可能会增加）：\n\n聚类分析\n主成分分析\n线性回归\n逻辑回归\nlasso回归\nKNN\n支持向量机\n决策树\n随机森林\nGBM\nXGBoost\nlightGBM\nCatBoost\n\n对于每种方法，都会首先介绍它的原理，这部分内容使用简洁易懂的语言向大家介绍一些算法的基本概念，不包含任何数学公式，方便大家理解。\n然后是代码实战部分，这部分内容主要包括模型建立、参数解释、结果解读、模型评价、超参数调优等内容，力求详细、方便理解。由于临床医学中常见的任务都是分类任务，所以示例代码基本都是分类任务的演示。\n对于生存分析，本合集并未涉及，因为生存任务和传统的分类/回归任务差别很大，大到可能需要单独开一个合集介绍，相关内容可参考：R语言生存分析。\n模型评价部分重点展示了混淆矩阵、敏感度/特异度、ROC曲线、PR曲线、校准曲线、决策曲线等。\n代码实战部分我并没有使用目前R语言机器学习领域很火爆的两个R包：tidymodels和mlr3，而是对于每种算法都使用了其对应的经典R包，比如：lasso回归部分使用了glmnet包、支持向量机部分使用了e1071包、随机森林部分使用了randomForest包。因为这两个R包本身还是基于这些经典的R包的，tidymodels和mlr3本身没有实现任何算法，它们只是对这些经典的R包进行了深度整合。所以，从这些R包开始学习方便为以后的进一步学习打下基础，如果你没接触过这些经典R包的话，有一些小问题你可能很难发现。\ntidymodels和mlr3的使用教程我在公众号中一直在更新，不久后也会整理成合集，还有一个模型解释的合集也会整理出来。有了这些经典R包的基础之后，你会发现再学习tidymodels和mlr3会更加容易上手，毕竟就只需要学习下这两个R包的使用逻辑和语法就好了。\n除此之外，本合集还介绍了一些机器学习中的其他重要问题，主要包括：\n\n数据预处理\n数据划分\n模型评价\n超参数调优\n变量选择\n\n本合集介绍的内容非常基础，只是机器学习相关内容中的冰山一角，比如，对于模型的超参数调优，我只是演示了如何使用R包实现，虽然方法是对的，但是调参的方向并不一定对，或者说，有可能你一通操作下来，模型的表现还不如默认参数好。因为每种模型的调参方向都是不一样的，使用的方法也会有差别，这个需要专业知识，也需要相关的经验。更多专业的知识，需要大家自己阅读专业的书籍。\n由于合集内容大部分都是代码实操，所以需要你对R语言有一些基础的理解，不然你可能会一直被一些基础的报错所困扰。每篇内容可能需要不同的R包，这里并没有展示安装这些R包的过程，大家需要自己安装。\n限于个人专业、水平、时间等问题，难免有很多不足之处，欢迎大家以各种方式（微信、QQ、公众号留言等）交流、建议。但是请不要抬杠，也请不要成为喷子、伸手党。\n\n\n\n\n\n\n注释\n\n\n\n本合集是基于我公众号发表的机器学习相关推文修改而来，为了更加适合机器学习初学者阅读以及方便理解，进行了大量修改，甚至可以说是从头重写！和我的其他合集一样，本合集也会保持长期更新。",
    "crumbs": [
      "前言"
    ]
  },
  {
    "objectID": "index.html#作者介绍",
    "href": "index.html#作者介绍",
    "title": "R语言实战机器学习",
    "section": "作者介绍",
    "text": "作者介绍\n\n阿越，外科医生，R语言爱好者，长期分享R语言和医学统计学、临床预测模型、生信数据挖掘、R语言机器学习等知识。\n哔哩哔哩：阿越就是我\nGithub：ayueme\n公众号：医学和生信笔记，欢迎扫码关注：",
    "crumbs": [
      "前言"
    ]
  },
  {
    "objectID": "机器学习简介.html",
    "href": "机器学习简介.html",
    "title": "1  机器学习简介",
    "section": "",
    "text": "1.1 概念简介\n机器学习是一个很宽泛的概念，但是从机器学习在医学中的使用（尤其是在发文章这块）情况来看，它只是作为一个能够根据多个指标给出预测结果的数学方法而已，并不是什么高大上的概念。\n举个简单的例子说明。\n学过《医学统计学》的肯定都知道多元线性回归和逻辑回归，这两种方法在医学统计中都是用来探索危险因素的，比如：探索身高、体重、血脂、饮食情况等对血糖有没有影响？这时我们会根据因变量的类型选择合适的方法，然后建立回归方程，看看各个变量的P值等。\n这就是典型的机器学习方法在医学中的使用，多元线性回归和逻辑回归也属于机器学习方法，只不过当我们在医学统计的范围中使用时，我们更加关注的是各个自变量到底有没有意义（也就是P值），当在机器学习的范围中使用这些方法时，我们会更偏向于使用多个自变量预测因变量的值。比如根据患者的身高、体重、血脂、饮食情况等，预测患者是否会得糖尿病。此时我们依然是建立回归方程，只不过目的变了。\n线性回归、逻辑回顾、cox回归是我们见到的最多的方法，因为临床中用的非常多，这些不仅是统计方法，也是机器学习方法，除此之外，大家常见到的其他机器学习方法还有：决策树、随机森林、K最近邻、支持向量机等。使用它们的目的都是为了预测结果，比如：预测患者的生死、预测是否会得某种疾病、预测肿瘤是良性还是恶性、预测患者是出血还是血栓、预测患者的出血风险、流产风险等。\n具体使用时的步骤也是差不多的，都是要根据数据选择合适的方法，然后建立模型，然后进行预测。机器学习并不神秘，我们只是把它当做是一种预测结果的工具而已，不需要理解背后复杂的数学原理，只需要会用即可。\n那么机器学习到底怎么学习的呢？难道是像人类一样能用脑子思考吗？并不是。比如你要用患者的身高、体重、血脂、饮食习惯，预测患者的餐后2h血糖值，那么这4个自变量和因变量之间肯定是存在某种关系，才能让我们去探索，去预测。可能是“血脂越高则血糖越高”这种正相关或者负相关的关系，也可能是在某个区间内增加然后在某个区间内下降这种不单调的关系、甚至是其他类型更加复杂的关系。\n机器学习方法，就可以识别这种数据内部的关系，或者叫规律，或者叫模式（pattern）。不同的机器学习方法能够识别不同的关系模式。比如对于正相关或者负相关这种关系，线性回归就很擅长，因为它拟合出来就是一条线，完美的线性关系。而且当它根据当前的数据识别出这种关系后，你再给它新的数据，它可以根据识别出的关系，对新的数据作出预测。\n所以“机器学习”这4个字并不是说机器能够像人类一样自思考，只是一个比喻，用来说明不同的算法能够识别出数据间的模式，还能够根据识别出来的模式，对新的数据作出预测。",
    "crumbs": [
      "基础知识",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>机器学习简介</span>"
    ]
  },
  {
    "objectID": "机器学习简介.html#回归和分类",
    "href": "机器学习简介.html#回归和分类",
    "title": "1  机器学习简介",
    "section": "1.2 回归和分类",
    "text": "1.2 回归和分类\n根据因变量的数据类型，我们使用机器学习的目的主要可以分为两类：\n\n回归：因变量是数值型，比如血压、血脂、血糖，我们要预测的是具体的数值\n分类：因变量是分类型，比如是/否，患病/不患病，良性/恶性，生存/死亡，我们要把结果归到某一个类别中，而不是预测数值\n\n机器学习最主要目的就是回归和分类，当然除了回归和分类，还有聚类分析和主成分分析这种，也属于机器学习方法。大多数模型都是既支持分类又支持回归的，比如：随机森林、支持向量机等，还有些也支持聚类、异常值检测等，但是有些算法可能只支持一种类型，比如：线性回归只支持回归任务。\n除了这些，医学中还有一块非常重要的内容：生存分析，比如cox生存分析。在传统的机器学习领域，这块内容很少涉及，但是近几年支持生存分析的机器学习算法越来越多了，比如随机生存森林、生存支持向量机、生存提升树模型、deepsurv等，这方面的发展也越来越快。",
    "crumbs": [
      "基础知识",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>机器学习简介</span>"
    ]
  },
  {
    "objectID": "机器学习简介.html#有监督和无监督",
    "href": "机器学习简介.html#有监督和无监督",
    "title": "1  机器学习简介",
    "section": "1.3 有监督和无监督",
    "text": "1.3 有监督和无监督\n线性回归、随机森林等这种方法，在使用时是需要数据中有因变量的，使用的数据中有因变量的就是有监督学习方法（supervised learning）；常见的方法都是有监督学习方法，比如：\n\n线性回归\n逻辑回归\nlasso回归\n决策树\n随机森林\nKNN\n支持向量机\n各种提升模型\n…\n\n像聚类分析、主成分分析这种，使用的数据中没有因变量的，被称为无监督学习方法（unsupervised learning）。",
    "crumbs": [
      "基础知识",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>机器学习简介</span>"
    ]
  },
  {
    "objectID": "机器学习简介.html#一些术语",
    "href": "机器学习简介.html#一些术语",
    "title": "1  机器学习简介",
    "section": "1.4 一些术语",
    "text": "1.4 一些术语\n因变量：应变量、结果变量（outcome）、标签（label） 自变量：预测变量（predictor）、特征（feature）、协变量（注意上下文）",
    "crumbs": [
      "基础知识",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>机器学习简介</span>"
    ]
  },
  {
    "objectID": "机器学习简介.html#一般流程",
    "href": "机器学习简介.html#一般流程",
    "title": "1  机器学习简介",
    "section": "1.5 一般流程",
    "text": "1.5 一般流程\n机器学习在医学中使用的一般流程是什么样的呢？\n首先第一步是准备数据，你得有数据才能进行接下来的一切。\n医学中遇到的数据一般都是很乱的，不能直接用，不管你是从临床中收集的还是白嫖的各种数据库资源，都是不能直接用的。所以我们要对这些数据进行整理，把这些数据变成算法能够使用的样子，这个过程叫做数据清洗（data cleaning）。\n数据清洗其实是最重要的一步！只有把数据整理成合适的格式，才能让模型充分识别数据内部的关系。你自己要对自己的数据有一个基本的理解，比如：\n\n你的数据是什么类型的？\n有哪些行哪些列？每行每列都是什么意思？\n是数值型还是类别型？\n有没有缺失值？\n是不是偏态分布？\n有没有极端值？\n需不需要进行转换？\n你的建模目的是什么？\n你希望本次建模能达到什么样的目标？\n…\n\n这些问题都是最基础的问题，如果你自己都不清楚，别指望结果会很好！\n在进行数据清洗的过程中，除了对数据进行整理外，还不断的借助图形来展示结果，通常这一过程会持续重复多次。在建模之前的这种数据清洗和可视化的过程，被称为探索性数据分析（exploratory data analysis，EDA）\n数据整理好之后，第二步才是建立模型。这一步并不是简单地拟合模型，而是包括了模型拟合、模型评价、模型调优、变量选择等多个步骤。这一步是机器学习的核心。也是我们要详细介绍的部分。\n最后一步就是整理结果，把报告呈现出来。",
    "crumbs": [
      "基础知识",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>机器学习简介</span>"
    ]
  },
  {
    "objectID": "数据预处理.html",
    "href": "数据预处理.html",
    "title": "2  数据预处理",
    "section": "",
    "text": "2.1 加载R包和数据\n这里使用细胞分离数据进行演示。这是一个分类数据，结果变量是二分类的，预测变量有连续型，也有分类型。\nlibrary(AppliedPredictiveModeling)\nlibrary(caret)\n## Loading required package: ggplot2\n## Loading required package: lattice\n\ndata(\"segmentationOriginal\")\n\nsegData &lt;- subset(segmentationOriginal, Case == \"Train\")\ncellID &lt;- segData$Cell\ncalss &lt;- segData$Class\ncase &lt;- segData$Case\nsegData &lt;- segData[ ,  -(1:3)]\nstatusColNum &lt;- grep(\"Status\", names(segData))\nstatusColNum\n##  [1]   2   4   9  10  11  12  14  16  20  21  22  26  27  28  30  32  34  36  38\n## [20]  40  43  44  46  48  51  52  55  56  59  60  63  64  68  69  70  72  73  74\n## [39]  76  78  80  82  84  86  88  92  93  94  97  98 103 104 105 106 110 111 112\n## [58] 114\n\nsegData &lt;- segData[ , -statusColNum]\n\n# 查看数据结构\nstr(segData)\n## 'data.frame':    1009 obs. of  58 variables:\n##  $ AngleCh1               : num  133.8 106.6 69.2 109.4 104.3 ...\n##  $ AreaCh1                : int  819 431 298 256 258 358 158 315 246 223 ...\n##  $ AvgIntenCh1            : num  31.9 28 19.5 18.8 17.6 ...\n##  $ AvgIntenCh2            : num  206 115 101 126 124 ...\n##  $ AvgIntenCh3            : num  69.9 63.9 28.2 13.6 22.5 ...\n##  $ AvgIntenCh4            : num  164.2 106.7 31 46.8 71.2 ...\n##  $ ConvexHullAreaRatioCh1 : num  1.26 1.05 1.2 1.08 1.08 ...\n##  $ ConvexHullPerimRatioCh1: num  0.797 0.935 0.866 0.92 0.931 ...\n##  $ DiffIntenDensityCh1    : num  31.9 32.5 26.7 28 27.9 ...\n##  $ DiffIntenDensityCh3    : num  43.1 36 22.9 14.9 16.1 ...\n##  $ DiffIntenDensityCh4    : num  79.3 51.4 26.4 32.7 36.2 ...\n##  $ EntropyIntenCh1        : num  6.09 5.88 5.42 5.38 5.18 ...\n##  $ EntropyIntenCh3        : num  6.64 6.68 5.44 4.15 5.49 ...\n##  $ EntropyIntenCh4        : num  7.88 7.14 5.78 6.19 6.62 ...\n##  $ EqCircDiamCh1          : num  32.3 23.4 19.5 18.1 18.2 ...\n##  $ EqEllipseLWRCh1        : num  1.56 1.38 3.39 1.38 1.62 ...\n##  $ EqEllipseOblateVolCh1  : num  2233 802 725 368 404 ...\n##  $ EqEllipseProlateVolCh1 : num  1433 583 214 267 250 ...\n##  $ EqSphereAreaCh1        : num  3279 1727 1195 1027 1036 ...\n##  $ EqSphereVolCh1         : num  17654 6751 3884 3096 3134 ...\n##  $ FiberAlign2Ch3         : num  0.488 0.301 0.22 0.364 0.359 ...\n##  $ FiberAlign2Ch4         : num  0.352 0.522 0.733 0.481 0.244 ...\n##  $ FiberLengthCh1         : num  64.3 21.1 43.1 22.3 26.5 ...\n##  $ FiberWidthCh1          : num  13.2 21.1 7.4 12.1 10.2 ...\n##  $ IntenCoocASMCh3        : num  0.02805 0.00686 0.03096 0.10816 0.01303 ...\n##  $ IntenCoocASMCh4        : num  0.01259 0.00614 0.01103 0.00995 0.00896 ...\n##  $ IntenCoocContrastCh3   : num  8.23 14.45 7.3 6.16 9.4 ...\n##  $ IntenCoocContrastCh4   : num  6.98 16.7 13.39 10.59 10.3 ...\n##  $ IntenCoocEntropyCh3    : num  6.82 7.58 6.31 5.04 6.96 ...\n##  $ IntenCoocEntropyCh4    : num  7.1 7.67 7.2 7.13 7.14 ...\n##  $ IntenCoocMaxCh3        : num  0.1532 0.0284 0.1628 0.3153 0.0739 ...\n##  $ IntenCoocMaxCh4        : num  0.0739 0.0232 0.0775 0.0586 0.0348 ...\n##  $ KurtIntenCh1           : num  -0.249 -0.293 0.626 -0.365 -0.556 ...\n##  $ KurtIntenCh3           : num  -0.331 1.051 0.128 1.083 -0.512 ...\n##  $ KurtIntenCh4           : num  -0.265 0.151 -0.347 -0.626 -0.647 ...\n##  $ LengthCh1              : num  47.2 28.1 37.9 23.1 26.3 ...\n##  $ NeighborAvgDistCh1     : num  174 158 206 264 231 ...\n##  $ NeighborMinDistCh1     : num  30.1 34.9 33.1 38.4 29.8 ...\n##  $ NeighborVarDistCh1     : num  81.4 90.4 116.9 88.5 103.5 ...\n##  $ PerimCh1               : num  154.9 84.6 101.1 68.7 73.4 ...\n##  $ ShapeBFRCh1            : num  0.54 0.724 0.589 0.635 0.557 ...\n##  $ ShapeLWRCh1            : num  1.47 1.33 2.83 1.31 1.49 ...\n##  $ ShapeP2ACh1            : num  2.26 1.27 2.55 1.4 1.59 ...\n##  $ SkewIntenCh1           : num  0.399 0.472 0.882 0.547 0.443 ...\n##  $ SkewIntenCh3           : num  0.62 0.971 1 1.432 0.556 ...\n##  $ SkewIntenCh4           : num  0.527 0.325 0.604 0.704 0.137 ...\n##  $ SpotFiberCountCh3      : int  4 2 4 0 1 1 4 2 2 2 ...\n##  $ SpotFiberCountCh4      : int  11 6 7 5 4 5 4 2 5 1 ...\n##  $ TotalIntenCh1          : int  24964 11552 5545 4613 4340 14461 4743 88725 136957 79885 ...\n##  $ TotalIntenCh2          : int  160997 47510 28869 30855 30719 74259 15434 148012 57421 62235 ...\n##  $ TotalIntenCh3          : int  54675 26344 8042 3332 5548 14474 6265 58224 20304 23878 ...\n##  $ TotalIntenCh4          : int  128368 43959 8843 11466 17588 23099 17534 120536 15482 98948 ...\n##  $ VarIntenCh1            : num  18.8 17.3 13.8 13.9 12.3 ...\n##  $ VarIntenCh3            : num  56.7 37.7 30 18.6 17.7 ...\n##  $ VarIntenCh4            : num  118.4 49.5 24.7 40.3 41.9 ...\n##  $ WidthCh1               : num  32.2 21.2 13.4 17.5 17.7 ...\n##  $ XCentroid              : int  215 371 487 211 172 276 239 95 438 386 ...\n##  $ YCentroid              : int  347 252 295 495 207 385 404 95 16 14 ...",
    "crumbs": [
      "基础知识",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>数据预处理</span>"
    ]
  },
  {
    "objectID": "数据预处理.html#中心化和标准化",
    "href": "数据预处理.html#中心化和标准化",
    "title": "2  数据预处理",
    "section": "2.2 中心化和标准化",
    "text": "2.2 中心化和标准化\n某些算法对预测变量是有要求的，比如需要预测变量具有相同的尺度，如果有的预测变量范围是0.10.2，但是有的却是1000020000，这种变量间的巨大差距会影像某些模型的稳定性，所以需要想办法把它们变成差不多的范围。\n中心化和标准化可以解决这样的问题。\n中心化是将所有变量减去其均值，其结果是变换后的变量均值为0；标准化是将每个变量除以其自身的标准差，标准化迫使变量的标准差为1。\nR语言中scale()函数可实现中心化和标准化，比如对演示数据的第1列和第4列进行中心化和标准化：\n\n# 进行中心化，不进行标准化\nscaled &lt;- scale(segData[,c(1,4)], center = T, scale = F)\nhead(scaled)\n##     AngleCh1 AvgIntenCh2\n## 2   42.62563    20.68784\n## 3   15.51998   -69.87514\n## 4  -21.97608   -83.89594\n## 12  18.29002   -59.25190\n## 15  13.15225   -60.82225\n## 16 -13.13447    31.94090\n\n# 同时进行中心化和标准化\nscaled &lt;- scale(segData[,c(1,4)], center = T, scale = T)\nhead(scaled)\n##      AngleCh1 AvgIntenCh2\n## 2   0.8714777   0.1343483\n## 3   0.3173048  -0.4537741\n## 4  -0.4492993  -0.5448262\n## 12  0.3739380  -0.3847860\n## 15  0.2688966  -0.3949840\n## 16 -0.2685331   0.2074265",
    "crumbs": [
      "基础知识",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>数据预处理</span>"
    ]
  },
  {
    "objectID": "数据预处理.html#偏度问题",
    "href": "数据预处理.html#偏度问题",
    "title": "2  数据预处理",
    "section": "2.3 偏度问题",
    "text": "2.3 偏度问题\n无偏分布类似我们常说的正态分布，有偏分布又分为右偏和左偏，分别类似正偏态分布和负偏态分布。\n一个判断数据有偏的黄金标准：如果最大值与最小值的比例超过20，那么我们认为数据有偏。\n可以通过计算偏度统计量来衡量偏度。如果预测变量分布是大致对称的，那么偏度将接近于0，右偏分布偏度大于0，越大说明偏的越厉害；左偏分布偏度小于0，越小说明偏的越厉害。\n计算偏度的包很多。\n使用e1071包查看变量的偏度\n\n# 查看偏度\ne1071::skewness(segData$AngleCh1)\n## [1] -0.02426252\n## [1] -0.02426252\n\n# 查看每一列的偏度\nskewValues &lt;- apply(segData, 2, e1071::skewness)\nhead(skewValues)\n##    AngleCh1     AreaCh1 AvgIntenCh1 AvgIntenCh2 AvgIntenCh3 AvgIntenCh4 \n## -0.02426252  3.52510745  2.95918524  0.84816033  2.20234214  1.90047128\n\n也可以通过psych包查看：\n\npsych::skew(segData$AngleCh1) # 偏度\n## [1] -0.02426252\n\npsych::kurtosi(segData$AngleCh1) # 峰度\n## [1] -0.8594789\n\n通过对数据进行变换可以一定程度解决偏度的问题，常用的方法有：取对数(log)，平方根，倒数，Box&Cox法等。\nlog、平方根、倒数这些很简单，就不演示了，下面演示下BoxCox变换。\n\n# 准备对数据进行BoxCox变换\nCh1AreaTrans &lt;- BoxCoxTrans(segData$AreaCh1)\nCh1AreaTrans\n## Box-Cox Transformation\n## \n## 1009 data points used to estimate Lambda\n## \n## Input data summary:\n##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n##   150.0   194.0   256.0   325.1   376.0  2186.0 \n## \n## Largest/Smallest: 14.6 \n## Sample Skewness: 3.53 \n## \n## Estimated Lambda: -0.9\n\n# 进行变换\nAreaCh1_transed &lt;- predict(Ch1AreaTrans, segData$AreaCh1)\n\n# 查看变换前、后的数据\nhead(segData$AreaCh1)\n## [1] 819 431 298 256 258 358\nhead(AreaCh1_transed)\n## [1] 1.108458 1.106383 1.104520 1.103554 1.103607 1.105523\n\n这里可以看到caret对数据预处理的方式，首先是选择方法，然后使用predict()函数把变换应用到具体的变量上。这是caret的基本操作，大家一定要记住！\n对于变换前后的数据变化，只看数字没有直观的感受，下面给大家画图演示。\n\n# 画图看变换前后\nopar &lt;- par(mfrow=c(1,2))\nhist(segData$AreaCh1)\nhist(AreaCh1_transed)\n\n\n\n\n\n\n\npar(opar)\n\n可以明显看到变换前是右偏分布，变换后基本接近无偏，可以再次计算偏度看看：\n\npsych::skew(AreaCh1_transed)\n## [1] 0.0976087\n\n下面是BoxCox变换的一点点扩展，不看也影响不大。\nBoxCox变换需要一个参数lambda，这个参数需要我们计算并指定，如上使用caret进行变换时，它会自动帮我们处理好，其中一句代码显示Estimated Lambda: -0.9，也就是lambda=0.9。\n还有很多R包可以实现BoxCox变换，其中比较简单的是forecast，简单演示如下：\n\nlibrary(forecast)\n## Registered S3 method overwritten by 'quantmod':\n##   method            from\n##   as.zoo.data.frame zoo\n\nbest.lambda &lt;- BoxCox.lambda(segData$AreaCh1) # 计算lambda\nbest.lambda\n## [1] -0.9999264\n\nAreaCh1.transformed &lt;- BoxCox(segData$AreaCh1, lambda = best.lambda) # 变换\nhead(AreaCh1.transformed)\n## [1] 0.9988519 0.9977522 0.9967163 0.9961655 0.9961958 0.9972789\n\ny0 &lt;- InvBoxCox(AreaCh1.transformed,lambda=best.lambda) # 还原",
    "crumbs": [
      "基础知识",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>数据预处理</span>"
    ]
  },
  {
    "objectID": "数据预处理.html#解决离群值",
    "href": "数据预处理.html#解决离群值",
    "title": "2  数据预处理",
    "section": "2.4 解决离群值",
    "text": "2.4 解决离群值\n离群值其实是有明确定义的，通常我们会选择直接删除离群值，但是还是要根据实际情况来看，有的离群值是非常有意义的，这样的离群值不能直接删除。\n\n有的离群值可能是数据录入时不小心输错了，比如错把收缩压132mmHg录成了-132mmHg，只需要改正即可；\n在样本量较小时，不宜直接删除离群值，有的离群值可能是因为数据来自一个明显有偏的分布，只是因为我们的样本量太小无法观测到这个偏度；\n有些离群值可能来自一个特殊的子集，只是这个子集才刚开始被收集到。\n\n有些模型对离群值很敏感，比如线性模型，这样是需要处理的，一个常见的方法是空间表示变换，该变换将预测变量取值映射到高纬的球上，它会把所有样本变换到离球心相等的球面上。在caret中可以实现。关于它的具体数学运算过程，感兴趣的自己了解即可。\n在进行空间表示变换前，最好先进行中心化和标准化，这也和它的数学计算有关。\n\n# 变换前的图形\ndata(mdrr)\ntransparentTheme(trans = .4)\n\nplotSubset &lt;- data.frame(scale(mdrrDescr[, c(\"nC\", \"X4v\")])) \nxyplot(nC ~ X4v,\n       data = plotSubset,\n       groups = mdrrClass, \n       auto.key = list(columns = 2))\n\n\n\n\n\n\n\n\n\n# 变换后的图形\ntransformed &lt;- spatialSign(plotSubset)\ntransformed &lt;- as.data.frame(transformed)\nxyplot(nC ~ X4v, \n       data = transformed, \n       groups = mdrrClass, \n       auto.key = list(columns = 2)) \n\n\n\n\n\n\n\n\n是不是很神奇？",
    "crumbs": [
      "基础知识",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>数据预处理</span>"
    ]
  },
  {
    "objectID": "数据预处理.html#降维和特征提取",
    "href": "数据预处理.html#降维和特征提取",
    "title": "2  数据预处理",
    "section": "2.5 降维和特征提取",
    "text": "2.5 降维和特征提取\n有很多方法，比如PCA，ICA，PLS，UMAP等，最流行的还是PCA，主要是它给出的主成分是彼此不相关的，这恰好符合一些模型的需求。\n对数据进行PCA变换之前，最好先解决偏度问题，然后进行中心化和标准化，和它的数学计算过程有关，感兴趣的自己了解。\n可视化前后不同：\n\n# 主成分分析，可参考我之前的推文\npr &lt;- prcomp(~ AvgIntenCh1 + EntropyIntenCh1, \n             data = segData, \n             scale. = TRUE)\n\n# 可视化前后图形\nlibrary(ggplot2)\n\np1 &lt;- ggplot(segData, aes(AvgIntenCh1,EntropyIntenCh1))+\n  geom_point()+\n  labs(x=\"Channel 1 Fiber Width\",y=\"Intensity Entropy Channel 1\")+\n  theme_bw()\np2 &lt;- ggplot(as.data.frame(pr$x), aes(PC1,PC2))+\n  geom_point()+\n  theme_bw()\ncowplot::plot_grid(p1,p2)\n\n\n\n\n\n\n\n\n如果你要对结果进行解释，那么不推荐使用降维，因为降维后得到的模型很难解释！尤其是对于医学来说，我们不喜欢不确定的东西。",
    "crumbs": [
      "基础知识",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>数据预处理</span>"
    ]
  },
  {
    "objectID": "数据预处理.html#处理缺失值",
    "href": "数据预处理.html#处理缺失值",
    "title": "2  数据预处理",
    "section": "2.6 处理缺失值",
    "text": "2.6 处理缺失值\n处理缺失值主要有两种方法，直接删除或者进行插补，使用哪种方法应取决于对数据的理解！\n一些常见的缺失值处理方法可以参考我之前的推文：我常用的缺失值插补方法",
    "crumbs": [
      "基础知识",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>数据预处理</span>"
    ]
  },
  {
    "objectID": "数据预处理.html#过滤",
    "href": "数据预处理.html#过滤",
    "title": "2  数据预处理",
    "section": "2.7 过滤",
    "text": "2.7 过滤\n这里的过滤和解决共线性，其实部分属于特征选择的范围，就是大家常见的自变量选择问题，这个问题在以后的推文中还会详细介绍。\n\n冗余的变量通常增加了模型的复杂度而非信息量\n\n主要是过滤两种变量：(近)零方差变量和高度相关变量。\n如果一个变量只有1个值，那么这个变量的方差为0；如果一个变量只有少量不重复的取值，这种变量称为近零方差变量；这2种变量包含的信息太少了，应当过滤；\n检测近零方差变量的准则是：\n\n不重复取值的数目与样本量的比值低（比如10%）；\n最高频数和次高频数的比值高（比如20%）\n\n如果两个变量相关性太高，那么它们携带的信息可能很多是重叠的，会对某些模型产生较大的影响，应当解决。\n移除共线变量的方法如下：\n\n计算预测变量的相关系数矩阵\n找出相关系数绝对值最大的那对预测变量（记为变量A和B）\n分别计算A和B和其他预测变量的相关系数\n如果A的平均相关系数更大，移除A，否则移除B\n重复步骤2-4，直至所有相关系数的绝对值都低于设定的阈值\n\ncaret可以轻松实现以上过程。\n使用mdrr数据集演示。其中一列nR11大部分都是501，这种变量方差是很小的！\n\ndata(mdrr)\ntable(mdrrDescr$nR11) # 大部分值都是0\n## \n##   0   1   2 \n## 501   4  23\n\nsd(mdrrDescr$nR11)^2 # 方差很小！\n## [1] 0.1731787\n\n使用nearZeroVar()找出零方差和近零方差变量，结果中会给出zeroVar和nzv两列，用逻辑值表示是不是近零方差变量或者零方差变量。\n\nnzv &lt;- nearZeroVar(mdrrDescr, saveMetrics= TRUE)\nnzv[nzv$nzv,][1:10,]\n##        freqRatio percentUnique zeroVar  nzv\n## nTB     23.00000     0.3787879   FALSE TRUE\n## nBR    131.00000     0.3787879   FALSE TRUE\n## nI     527.00000     0.3787879   FALSE TRUE\n## nR03   527.00000     0.3787879   FALSE TRUE\n## nR08   527.00000     0.3787879   FALSE TRUE\n## nR11    21.78261     0.5681818   FALSE TRUE\n## nR12    57.66667     0.3787879   FALSE TRUE\n## D.Dr03 527.00000     0.3787879   FALSE TRUE\n## D.Dr07 123.50000     5.8712121   FALSE TRUE\n## D.Dr08 527.00000     0.3787879   FALSE TRUE\n\n去掉近零方差变量：\n\ndim(mdrrDescr)\n## [1] 528 342\n\nnzv &lt;- nearZeroVar(mdrrDescr)\nfilteredDescr &lt;- mdrrDescr[, -nzv]\ndim(filteredDescr)\n## [1] 528 297\n\n下面是处理高度相关的变量。\n\n# 相关系数矩阵\ncorrelations &lt;- cor(segData)\ndim(correlations)\n## [1] 58 58\n\n# 可视化相关系数矩阵，中间几个颜色深的就是高度相关的变量\nlibrary(corrplot)\n## corrplot 0.92 loaded\ncorrplot(correlations, order = \"hclust\",tl.col = \"black\")\n\n\n\n\n\n\n\n\n去掉高度相关的变量：\n\n# 阈值设为0.75\nhighCorr &lt;- findCorrelation(correlations, cutoff = 0.75)\nlength(highCorr)\n## [1] 32\nhead(highCorr)\n## [1] 23 40 43 36  7 15\n\n# 去掉高度相关的变量\nfilteredSegData &lt;- segData[, -highCorr]",
    "crumbs": [
      "基础知识",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>数据预处理</span>"
    ]
  },
  {
    "objectID": "数据预处理.html#共线性",
    "href": "数据预处理.html#共线性",
    "title": "2  数据预处理",
    "section": "2.8 共线性",
    "text": "2.8 共线性\n假设一个下面这种的数据，其中第2列和第3列的值加起来和第1列一样，第4,5,6列的值起来也和第1列一样。这种数据的某些变量间是有高度共线性的。\n\nltfrDesign &lt;- matrix(0, nrow=6, ncol=6)\nltfrDesign[,1] &lt;- c(1, 1, 1, 1, 1, 1)\nltfrDesign[,2] &lt;- c(1, 1, 1, 0, 0, 0)\nltfrDesign[,3] &lt;- c(0, 0, 0, 1, 1, 1)\nltfrDesign[,4] &lt;- c(1, 0, 0, 1, 0, 0)\nltfrDesign[,5] &lt;- c(0, 1, 0, 0, 1, 0)\nltfrDesign[,6] &lt;- c(0, 0, 1, 0, 0, 1)\n\nltfrDesign\n##      [,1] [,2] [,3] [,4] [,5] [,6]\n## [1,]    1    1    0    1    0    0\n## [2,]    1    1    0    0    1    0\n## [3,]    1    1    0    0    0    1\n## [4,]    1    0    1    1    0    0\n## [5,]    1    0    1    0    1    0\n## [6,]    1    0    1    0    0    1\n\nfindLinearCombos()可以通过算法给出需要去除的变量，关于具体的方法可以官网查看。\n\ncomboInfo &lt;- findLinearCombos(ltfrDesign)\ncomboInfo\n## $linearCombos\n## $linearCombos[[1]]\n## [1] 3 1 2\n## \n## $linearCombos[[2]]\n## [1] 6 1 4 5\n## \n## \n## $remove\n## [1] 3 6\n\n结果给出了需要去除的变量是第3列和第6列。\n\n# 去除第3列和第6列\nltfrDesign[, -comboInfo$remove]\n##      [,1] [,2] [,3] [,4]\n## [1,]    1    1    1    0\n## [2,]    1    1    0    1\n## [3,]    1    1    0    0\n## [4,]    1    0    1    0\n## [5,]    1    0    0    1\n## [6,]    1    0    0    0",
    "crumbs": [
      "基础知识",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>数据预处理</span>"
    ]
  },
  {
    "objectID": "数据预处理.html#构建虚拟变量",
    "href": "数据预处理.html#构建虚拟变量",
    "title": "2  数据预处理",
    "section": "2.9 构建虚拟变量",
    "text": "2.9 构建虚拟变量\n最常见的回归分析中的哑变量设置，可以参考之前的推文，详细介绍了常见的分类变量的编码方式：分类变量进行回归分析时的编码方案\n这里介绍下独热编码（one-hot encoding），和哑变量编码稍有不同，哑变量是变成k-1个变量，独热编码是变成k个变量。\n使用以下数据进行演示\n\ndata(\"cars\", package = \"caret\")\nhead(cars)\n##      Price Mileage Cylinder Doors Cruise Sound Leather Buick Cadillac Chevy\n## 1 22661.05   20105        6     4      1     0       0     1        0     0\n## 2 21725.01   13457        6     2      1     1       0     0        0     1\n## 3 29142.71   31655        4     2      1     1       1     0        0     0\n## 4 30731.94   22479        4     2      1     0       0     0        0     0\n## 5 33358.77   17590        4     2      1     1       1     0        0     0\n## 6 30315.17   23635        4     2      1     0       0     0        0     0\n##   Pontiac Saab Saturn convertible coupe hatchback sedan wagon\n## 1       0    0      0           0     0         0     1     0\n## 2       0    0      0           0     1         0     0     0\n## 3       0    1      0           1     0         0     0     0\n## 4       0    1      0           1     0         0     0     0\n## 5       0    1      0           1     0         0     0     0\n## 6       0    1      0           1     0         0     0     0\n\ntype &lt;- c(\"convertible\", \"coupe\", \"hatchback\", \"sedan\", \"wagon\")\ncars$Type &lt;- factor(apply(cars[, 14:18], 1, function(x) type[which(x == 1)]))\n\ncarSubset &lt;- cars[sample(1:nrow(cars), 20), c(1, 2, 19)]\n\n# 上面是数据生成过程，不重要，记住下面这个数据的样子即可！！\nhead(carSubset)\n##        Price Mileage        Type\n## 104 12464.07   21891       coupe\n## 379 14771.00   22255       coupe\n## 582 38167.17   13162 convertible\n## 20  30800.66    8017       sedan\n## 147 37510.25   21593       sedan\n## 701 20537.14   16950       coupe\nlevels(carSubset$Type) # Type是一个因子型变量\n## [1] \"convertible\" \"coupe\"       \"hatchback\"   \"sedan\"       \"wagon\"\n\n现在把Type这个变量进行独热编码。\n使用dummyVars构建虚拟变量：\n\nsimpleMod &lt;- dummyVars(~Mileage + Type, # 用mileage和Type对价格进行预测\n                       data = carSubset,\n                       levelsOnly = TRUE) # 从列名中移除因子变量的名称\nsimpleMod\n## Dummy Variable Object\n## \n## Formula: ~Mileage + Type\n## 2 variables, 1 factors\n## Factor variable names will be removed\n## A less than full rank encoding is used\n\n接下来就可以使用predict和simpleMod对训练集进行生成虚拟变量的操作了：\n\npredict(simpleMod, head(carSubset))\n##     Mileage convertible coupe hatchback sedan wagon\n## 104   21891           0     1         0     0     0\n## 379   22255           0     1         0     0     0\n## 582   13162           1     0         0     0     0\n## 20     8017           0     0         0     1     0\n## 147   21593           0     0         0     1     0\n## 701   16950           0     1         0     0     0\n\n可以看到Type变量没有了，完成了虚拟变量的转换。\n假如你认为车型和里程有交互影响，则可以使用:表示：\n\nwithInteraction &lt;- dummyVars(~Mileage + Type + Mileage:Type,\n                             data = carSubset,\n                             levelsOnly = TRUE)\nwithInteraction\n## Dummy Variable Object\n## \n## Formula: ~Mileage + Type + Mileage:Type\n## 2 variables, 1 factors\n## Factor variable names will be removed\n## A less than full rank encoding is used\n\n应用于新的数据集：\n\npredict(withInteraction, head(carSubset))\n##     Mileage convertible coupe hatchback sedan wagon Mileage:Typeconvertible\n## 104   21891           0     1         0     0     0                       0\n## 379   22255           0     1         0     0     0                       0\n## 582   13162           1     0         0     0     0                   13162\n## 20     8017           0     0         0     1     0                       0\n## 147   21593           0     0         0     1     0                       0\n## 701   16950           0     1         0     0     0                       0\n##     Mileage:Typecoupe Mileage:Typehatchback Mileage:Typesedan Mileage:Typewagon\n## 104             21891                     0                 0                 0\n## 379             22255                     0                 0                 0\n## 582                 0                     0                 0                 0\n## 20                  0                     0              8017                 0\n## 147                 0                     0             21593                 0\n## 701             16950                     0                 0                 0",
    "crumbs": [
      "基础知识",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>数据预处理</span>"
    ]
  },
  {
    "objectID": "数据预处理.html#区间化预测变量",
    "href": "数据预处理.html#区间化预测变量",
    "title": "2  数据预处理",
    "section": "2.10 区间化预测变量",
    "text": "2.10 区间化预测变量\n主要是为了好解释结果，比如把血压分为高血压1级、2级、3级，把贫血分为轻中重极重等，这样比如你做logistic回归，可以说血压每增高一个等级，因变量的风险增加多少，但是你如果说血压值每增加1mmHg，因变量增加多少倍，这就有点扯了。",
    "crumbs": [
      "基础知识",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>数据预处理</span>"
    ]
  },
  {
    "objectID": "数据预处理.html#多个预处理步骤放一起",
    "href": "数据预处理.html#多个预处理步骤放一起",
    "title": "2  数据预处理",
    "section": "2.11 多个预处理步骤放一起",
    "text": "2.11 多个预处理步骤放一起\n在caret中是通过preProcess()函数里面的method参数实现的，把不同的预处理步骤按照顺序写好即可。\n\nlibrary(AppliedPredictiveModeling)\ndata(schedulingData)\nstr(schedulingData)\n## 'data.frame':    4331 obs. of  8 variables:\n##  $ Protocol   : Factor w/ 14 levels \"A\",\"C\",\"D\",\"E\",..: 4 4 4 4 4 4 4 4 4 4 ...\n##  $ Compounds  : num  997 97 101 93 100 100 105 98 101 95 ...\n##  $ InputFields: num  137 103 75 76 82 82 88 95 91 92 ...\n##  $ Iterations : num  20 20 10 20 20 20 20 20 20 20 ...\n##  $ NumPending : num  0 0 0 0 0 0 0 0 0 0 ...\n##  $ Hour       : num  14 13.8 13.8 10.1 10.4 ...\n##  $ Day        : Factor w/ 7 levels \"Mon\",\"Tue\",\"Wed\",..: 2 2 4 5 5 3 5 5 5 3 ...\n##  $ Class      : Factor w/ 4 levels \"VF\",\"F\",\"M\",\"L\": 2 1 1 1 1 1 1 1 1 1 ...\n\n# 中心化、标准化、YeoJohnson变换\npp_hpc &lt;- preProcess(schedulingData[, -8], \n                     method = c(\"center\", \"scale\", \"YeoJohnson\"))\npp_hpc\n## Created from 4331 samples and 7 variables\n## \n## Pre-processing:\n##   - centered (5)\n##   - ignored (2)\n##   - scaled (5)\n##   - Yeo-Johnson transformation (5)\n## \n## Lambda estimates for Yeo-Johnson transformation:\n## -0.08, -0.03, -1.05, -1.1, 1.44\n\n# 应用于数据\ntransformed &lt;- predict(pp_hpc, newdata = schedulingData[, -8])\nhead(transformed)\n##   Protocol  Compounds InputFields Iterations NumPending         Hour Day\n## 1        E  1.2289592  -0.6324580 -0.0615593  -0.554123  0.004586516 Tue\n## 2        E -0.6065826  -0.8120473 -0.0615593  -0.554123 -0.043733201 Tue\n## 3        E -0.5719534  -1.0131504 -2.7894869  -0.554123 -0.034967177 Thu\n## 4        E -0.6427737  -1.0047277 -0.0615593  -0.554123 -0.964170752 Fri\n## 5        E -0.5804713  -0.9564504 -0.0615593  -0.554123 -0.902085020 Fri\n## 6        E -0.5804713  -0.9564504 -0.0615593  -0.554123  0.698108782 Wed\n\nmean(schedulingData$NumPending == 0)\n## [1] 0.7561764\n\n# 进行中心化、标准化、YeoJohnson、nzv\npp_no_nzv &lt;- preProcess(schedulingData[, -8], \n                        method = c(\"center\", \"scale\", \"YeoJohnson\", \"nzv\"))\npp_no_nzv\n## Created from 4331 samples and 7 variables\n## \n## Pre-processing:\n##   - centered (4)\n##   - ignored (2)\n##   - removed (1)\n##   - scaled (4)\n##   - Yeo-Johnson transformation (4)\n## \n## Lambda estimates for Yeo-Johnson transformation:\n## -0.08, -0.03, -1.05, 1.44\n\npredict(pp_no_nzv, newdata = schedulingData[1:6, -8])\n##   Protocol  Compounds InputFields Iterations         Hour Day\n## 1        E  1.2289592  -0.6324580 -0.0615593  0.004586516 Tue\n## 2        E -0.6065826  -0.8120473 -0.0615593 -0.043733201 Tue\n## 3        E -0.5719534  -1.0131504 -2.7894869 -0.034967177 Thu\n## 4        E -0.6427737  -1.0047277 -0.0615593 -0.964170752 Fri\n## 5        E -0.5804713  -0.9564504 -0.0615593 -0.902085020 Fri\n## 6        E -0.5804713  -0.9564504 -0.0615593  0.698108782 Wed\n\n如果你用过tidymodels，那你应该知道里面的数据预处理步骤是通过recipes包完成的，每一步都是step_xx，说实话我觉得caret的这种方式更加简洁易懂！\n以上就是数据预处理的一般过程，一个caret包可以解决上面所有的问题，有兴趣的小伙伴可以自行学习。\n数据预处理是一个非常系统且专业的过程，如同开头说的那样：最有效的编码数据的方法来自于建模者对数据的理解，而不是通过任何数学方法，在对数据进行预处理之前，一定要仔细理解自己的数据哦，结果导向的思维是不对的哦！\n本文简单介绍了常见的数据预处理方法和简单的实现方法，目前在R中实现数据预处理是非常方便的，这部分内容可参考：\n\nmlr3数据预处理\ntidymodels菜谱：数据预处理\nR语言机器学习caret-02：数据预处理",
    "crumbs": [
      "基础知识",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>数据预处理</span>"
    ]
  },
  {
    "objectID": "数据预处理.html#推荐的预处理步骤",
    "href": "数据预处理.html#推荐的预处理步骤",
    "title": "2  数据预处理",
    "section": "2.12 推荐的预处理步骤",
    "text": "2.12 推荐的预处理步骤\n以下是不同算法推荐的预处理步骤，参考自：tidymodeling with R",
    "crumbs": [
      "基础知识",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>数据预处理</span>"
    ]
  },
  {
    "objectID": "数据划分.html",
    "href": "数据划分.html",
    "title": "3  数据划分",
    "section": "",
    "text": "3.1 留出法(holdout)\n大家最常使用的，把数据集随机划分为训练集(train)/测试集(test)的做法就是holdout，其中训练集用于建模，测试集用于评估模型表现。测试集有时也被称为验证集(validation)。\n如果你的数据样本量足够大，大到几乎不可能产生错误的结果，那你可以大胆放心用这种方法，否则一般都不推荐只使用这种方法。\n通常的做法是把数据集划分为训练集/测试集后，在训练集中建立模型，当确定最终的模型后，我们会让模型对测试集进行预测，以评估模型最终的表现。",
    "crumbs": [
      "基础知识",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>数据划分</span>"
    ]
  },
  {
    "objectID": "数据划分.html#交叉验证cross-validation",
    "href": "数据划分.html#交叉验证cross-validation",
    "title": "3  数据划分",
    "section": "3.2 交叉验证(cross validation)",
    "text": "3.2 交叉验证(cross validation)\n交叉验证，意思就是一份数据既用作训练，也用作验证，互相交叉，主要有以下几种：\nK折交叉验证(K-fold cross validation)，就是把数据集随机分为K个样本量基本相同的子数据集。比如5折交叉验证，就是把数据集分为5个子集（比如分成A,B,C,D,E,5份），在建模时，首先会使用其中A,B,C,D,4份数据进行建模，然后用剩下的E数据评估模型表现，接下来使用A,B,C,E，4份数据建模，用剩下的D评估模型表现。这样依次进行5个循环，每份数据都会用来评估模型表现。最后将得到的5个模型表现结果进行汇总。\n下面是一个10折交叉验证的示意图：\n\n留一交叉验证(LOOCV, leave one out cross validation)，是K折交叉验证的特例。每次都只留1个样本用于评估模型表现，所以这里的K其实就等于样本量，每一个样本都会被用来评估模型表现。\n重复交叉验证(repeated cross validation)，也是K折交叉验证的扩展版本，比如，重复10次的5折交叉验证，就是把5折交叉验证这个过程重复10遍。\n蒙特卡洛交叉验证(Monte Carlo cross validation)，也是交叉验证的一个变种。留出法是将数据集划分1次，而蒙特卡洛交叉验证就是将留出法进行多次。",
    "crumbs": [
      "基础知识",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>数据划分</span>"
    ]
  },
  {
    "objectID": "数据划分.html#bootstrap",
    "href": "数据划分.html#bootstrap",
    "title": "3  数据划分",
    "section": "3.3 bootstrap",
    "text": "3.3 bootstrap\n自助法，即有放回的随机抽样法。具体做法如下：\n比如，一个数据集有100个样本，每次随机抽取1个，然后放回去，再随机抽取1个，再放回去，这样的过程重复100次，就得到了一个和原数据集样本量相等的抽样数据集，这个抽样数据集就叫做自助集。\n由于每次都是有放回然后再随机抽取，所以一个自助集中可能有多个同一样本！所以就有可能在100次随机抽取中，有一些没被抽中过的样本，这些样本就被称为袋外样本(out-of-bag，OOB)，其中被抽中的样本(也就是自助集)用于训练模型，袋外样本用来评估模型表现。\n如果设置bootstrap的此时是10，就是抽取10个自助集，10个袋外样本，在每个自助集中训练一个模型，并在相应的袋外样本中评估模型，这样就会得到10个模型表现，对它们取平均值，就是最终的模型表现。",
    "crumbs": [
      "基础知识",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>数据划分</span>"
    ]
  },
  {
    "objectID": "数据划分.html#其他方法",
    "href": "数据划分.html#其他方法",
    "title": "3  数据划分",
    "section": "3.4 其他方法",
    "text": "3.4 其他方法\n除了以上方法，其实还有非常多没有介绍，比如在mlr3中经常使用的嵌套重抽样，这些见到了再给大家介绍。",
    "crumbs": [
      "基础知识",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>数据划分</span>"
    ]
  },
  {
    "objectID": "数据划分.html#内部验证外部验证",
    "href": "数据划分.html#内部验证外部验证",
    "title": "3  数据划分",
    "section": "3.5 内部验证/外部验证",
    "text": "3.5 内部验证/外部验证\n临床预测模型中会有内部验证/外部验证的说法，内部验证又叫内部重抽样，其实没有任何特殊的地方，只是画个说法而已。\n一个数据集被随机划分为训练集和测试集，在训练集建立模型时，我们可能会对训练集使用K折交叉验证或者bootstrap等方法，这样可以使我们的模型更加稳健，避免出现极坏或者极好的结果。在训练集中使用K折交叉验证或者bootstrap等方法，就被叫做内部验证或者内部重抽样。在测试集（指建模时未使用过的数据）测试模型的表现，就叫做外部验证。\n下图是《tidymodeling with R》中的典型数据划分方法示意图，非常贴合临床预测模型的数据划分方法。\n\n\n\n典型的数据划分方法\n\n\n首先我们会把所有数据划分为训练集和测试集，然后在训练集中建立模型，这个过程会对训练集使用另一种重抽样方法（比如交叉验证或者bootstrap等），每一次重抽样我们都会使用其中一部分数据用于拟合模型，另一部分数据用于评估模型，用于拟合模型的这部分数据被称为分析集，用于评估模型的数据被称为评估集。",
    "crumbs": [
      "基础知识",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>数据划分</span>"
    ]
  },
  {
    "objectID": "数据划分.html#重抽样的目的",
    "href": "数据划分.html#重抽样的目的",
    "title": "3  数据划分",
    "section": "3.6 重抽样的目的",
    "text": "3.6 重抽样的目的\n经常有粉丝问我：为什么我用了各种方法，10折交叉验证、10折重复交叉验证、自助法，都用过了，为什么最后模型的表现还是很差？\n看到类似的问题，我想这部分朋友可能把重抽样的目的搞错了，重抽样的目的不是为了提高模型表现，重抽样也确实不能提高模型表现！开头我已说过，重抽样技术是为了让模型更好的认识数据而已，这样能够得到更加稳健、无偏的结果，但是对于提高模型表现没有直接的影响哦~\n你可以这么理解，如果你不重抽样，可能某一次结果的AUC是0.9，再做一次可能就变成0.5了，而你重抽样10次，得到的结果是10次的平均，这样的结果很明显是更加稳健的。\n模型表现好不好首先是数据原因，一个牛逼的数据不需要复杂的模型也能有很好的结果，数据预处理对数据影响很大，大家可以参考上一章内容。另外还和模型本身的性质有关，比如模型的超参数、模型本身的上限等，这些都会影响模型的表现。",
    "crumbs": [
      "基础知识",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>数据划分</span>"
    ]
  },
  {
    "objectID": "数据划分.html#为什么要单独划分",
    "href": "数据划分.html#为什么要单独划分",
    "title": "3  数据划分",
    "section": "3.7 为什么要单独划分",
    "text": "3.7 为什么要单独划分\n通常我们建立模型时，会把数据集A划分为A1和A2两份，A1用来训练模型，A2用来测试模型，在训练模型的过程中，完全不用使用到A2这部分数据。有些人不理解，把这种方法和嵌套重抽样混为一谈。其实这两个有着本质的区别。\n嵌套重抽样是在训练模型时使用的，把两份数据集全都用到了，而且两份数据集都会再叠加其他重抽样方法。\n但我们划分数据的目的是什么呢？我们是为了测试最终的模型表现。临床问题数据很珍贵，通常都只有1份，这种情况下我把这份数据全都用于训练模型，那我用什么测试训练出来的模型好坏呢？有的人喜欢把训练好的模型作用于用来训练模型的数据上，发现结果竟然很好，这样是不严谨的，这叫数据泄露，因为你的数据模型已经学习过了。\n所以一开始把数据就划分为2份是一个很好的解决方法。如果你有很多个数据集，你完全可以在其中1个数据集中使用各种方法建模。比如，你的临床试验有多个分中心，你完全可以使用其中1个或几个中心的数据建立模型，然后在其余中心的数据中评估最终的模型。",
    "crumbs": [
      "基础知识",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>数据划分</span>"
    ]
  },
  {
    "objectID": "数据划分.html#方法选择建议",
    "href": "数据划分.html#方法选择建议",
    "title": "3  数据划分",
    "section": "3.8 方法选择建议",
    "text": "3.8 方法选择建议\n以上就是一些常见的重抽样方法，可以看到每种方法都强调一个问题，那就是随机！，只有随机，才能保证模型学习到这个数据集中的更多信息，才能获得稳健的模型表现！\n以下是一些方法选择建议：\n\n没有哪一种方法好，哪一种方法不好！！只有合不合适，没有好不好！\n如果样本量较小，建议选择重复10折交叉验证；\n如果样本量足够大，比如几万，几十万这种，随便选，都可以；\n如果目的不是得到最好的模型表现，而是为了在不同模型间进行选择，建议使用bootstrap；\n如果还不知道怎么选，建议都试一试，喜欢哪个选哪个",
    "crumbs": [
      "基础知识",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>数据划分</span>"
    ]
  },
  {
    "objectID": "模型评价.html",
    "href": "模型评价.html",
    "title": "4  模型评价",
    "section": "",
    "text": "4.1 回归任务评价\n如果你的数据是一个回归任务，也就是你的数据的结果变量是数值型的，常用的模型评价指标如下所示：\n凡是叫xxx误差的指标，一般都是越小说明模型越好。\n选择合适的评价指标取决于具体的应用场景、数据特性以及分析目的。例如，在某些领域，如金融或库存管理，可能更关心MAPE，因为它提供了关于预测误差相对于实际值比例的信息。而在其他情况下，R²或调整后的R²可能更适合用来评估模型的整体解释能力。",
    "crumbs": [
      "基础知识",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>模型评价</span>"
    ]
  },
  {
    "objectID": "模型评价.html#回归任务评价",
    "href": "模型评价.html#回归任务评价",
    "title": "4  模型评价",
    "section": "",
    "text": "均方误差（Mean Squared Error, MSE）: 定义为预测值与实际值之差的平方和的均值。\n均方根误差（Root Mean Squared Error, RMSE）: 是MSE的平方根，使得其单位与目标变量相同，便于解释。RMSE对异常值很敏感，极端异常值会让RMSE明显变大。如果只是为了衡量模型整体的误差，则使用RMSE更佳。\n平均绝对误差（Mean Absolute Error, MAE）: 计算预测值与实际值之间绝对差值的平均数。MAE对所有误差给予相等的权重，不被大误差过分影响，比RMSE更适合有极端值的情况。\n平均绝对百分比误差（Mean Absolute Percentage Error, MAPE）: 计算预测误差相对于实际值的百分比的平均值，适用于对预测误差的相对大小更感兴趣的场景。\n中位绝对误差（Median Absolute Error, MAE）：\nR²（决定系数，Coefficient of Determination）: 表示模型解释的变异量占总变异量的比例，值范围从0到1，越接近1表示模型拟合越好。R²衡量了模型解释数据变异的能力。\n解释方差分数（Explained Variance Score）: 类似于R²，但直接作为比例表示模型解释的方差部分，值越大表示模型拟合越好。",
    "crumbs": [
      "基础知识",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>模型评价</span>"
    ]
  },
  {
    "objectID": "模型评价.html#分类任务评价",
    "href": "模型评价.html#分类任务评价",
    "title": "4  模型评价",
    "section": "4.2 分类任务评价",
    "text": "4.2 分类任务评价\n\n4.2.1 常规指标\n如果你的数据是一个分类任务，也就是你的数据的结果变量是分类型的，常用的模型评价指标如下所示：\n\n混淆矩阵（Confusion Matrix）：混淆矩阵是分类任务中所有可能预测结果与真实结果的总结，包含了真阳性、假阳性、真阴性和假阴性的计数，是下述精确率、召回率等指标计算的基础。\n\n用一张图来总结混淆矩阵以及由此产生的各种指标（来自于维基百科）：\n\n\n\n混淆矩阵\n\n\n\n准确率（Accuracy）：准确率是最直观的评价指标，定义为分类正确的样本数占总样本数的比例。但是在类别分布不均匀（即类别不平衡）的情况下，准确率可能产生误导。假如有100个人，其中只有1个人有癌症，其余99个没有癌症，如果模型此时把100个人都预测为没有癌症，那么准确率是99%，非常高，但是此时对于那个漏诊的人来说是不能接受的。此时漏诊率为100%，漏诊率=1-灵敏度。\n精确率（Precision）：也叫查准率，精确率是指被模型预测为阳性的样本中，真正是阳性的比例。\n召回率（Recall）：也叫查全率，在医学中通常被称为灵敏度（sensitivity）,真阳性率，召回率是指所有实际为阳性的样本中，被模型正确识别为阳性的比例。还是上面那个100人的例子，此时它的灵敏度就是0，这样的结果是没有意义的，因为对于临床来说，漏诊是不能被接受的。以查全率为横坐标，以查准率为纵坐标，绘制曲线，可以得到如下图所示的P-R曲线。\n\n\n\nF1分数（F1-Score）：F1分数是精确率和召回率的加权平均值，旨在同时考虑精确率和召回率，特别适用于类别不平衡问题，范围也是0~1之间。精准率和召回率是相互矛盾的，一个增大另一个就会减小，所以我们需要找到一个平衡点，由此引出了F1分数，F1分数越高，说明模型越稳健。\n特异度（Specificity）：指所有实际为阴性的样本中，被模型正确识别为阴性的比例。上述的100人的例子，特异度就是100%，也就是说你没有误诊的。误诊率是0，误诊率=1-特异度。\nROC曲线下面积（Area Under the Receiver Operating Characteristic Curve）：AUC代表ROC曲线下的面积，衡量的是模型在不同阈值下的综合性能。AUC值越接近1，说明分类器的性能越好。AUC不受类别分布的影响，适用于不平衡数据集。临床上灵敏度和特异度是互相矛盾的，一个增加另一个必然减少，通常我们需要一个最佳的平衡点，让ROC曲线下面积最大。可参考：ROC曲线合集\n\n\n\n马修斯相关系数（Matthews Correlation Coefficient，MCC）：同时考虑混淆矩阵中4个类别的结果，并综合考量，范围是-1~1，1表示完美预测，0表示随机预测，-1表示完全相反。尤其适用于类不平衡的二分类问题。\n均衡准确率（Balanced Accuracy）：当出现类别不平衡时，使用准确率评价可能会出现较大偏差，此时可以使用均衡准确率，它是每个类别中预测正确的比例的算术平均值。\nLog Loss（交叉熵损失）：又叫对数损失、对数似然、逻辑损失，特别适用于概率预测模型，衡量预测概率分布与实际类别之间的差异。越接近0越好。\nBrier-Score：布里尔分数，衡量模型预测的类别的概率与真实值之间的误差，仅用于二分类数据，范围是0到1，越小越好。\n校准曲线（Calibration plot）：衡量模型预测的概率和真实概率的差异，是一种比单纯评价分类准确性更加精确的评价指标。\n预测概率直方图：评价模型预测概率的分布，并据此判断模型的置信度，临床预测模型中叫校准度。\n\n\n\n4.2.2 临床预测模型\n临床预测模型其实只是机器学习在临床医学中的一种应用形式而已，当然这一领域除了上面的常规指标外，还有一些其他评价方式。\n总的来说，临床预测模型的评价可以分为4个部分：\n\n区分度\n校准度\n临床适用性\n模型改善度\n\n区分度指的是一个模型能正确把人群分为患者/非患者，或者正确区分个体是处于低风险、还是处于高风险，或者正确预测患者是存活、还是死亡等的能力。混淆矩阵和由此计算出的各种指标都是区分度的评价。\n但是一个模型只是有良好的区分度是不够的，因为临床是很复杂的，并不是只要正确分类就行了。对于不同的患者，可能他们都处于高风险组，但是对于50%的风险和80%的风险，我们的处理是不一样的！\n这就引出了校准度的概念，校准度指的是结局实际发生的概率和模型预测出的概率之间的一致性，所以校准度又叫一致性、拟合优度（goodness-of-fit），校准度体现了一个模型对绝对风险预测的准确性。校准度的评价在文献中主要是通过校准曲线实现，但是Brier-Score、Log-Loss也是用来评价校准度的。\n临床适用性的评价主要是通过决策曲线实现的。模型改善度主要是评价新模型比旧模型改善了多少，需要有模型之间的比较才能得出。\n关于临床预测模型评价的更详细的内容，可参考：一文搞懂临床预测模型的评价",
    "crumbs": [
      "基础知识",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>模型评价</span>"
    ]
  },
  {
    "objectID": "模型评价.html#计算方法",
    "href": "模型评价.html#计算方法",
    "title": "4  模型评价",
    "section": "4.3 计算方法",
    "text": "4.3 计算方法\n在实际使用时我们不需要知道具体的计算方法，只需要它们的意义即可，因为计算只需要1行代码就实现了。\n虽然不同的R包的建模的语法是有区别的，但是在计算模型评价指标时，代码逻辑和使用语法却是高度一致的。\n不管是回归任务还是分类任务，计算各种指标时都是需要真实结果和预测结果的，使用形式一般都是如下的格式：\n# 计算RMSE\nrmse(data, 真实结果, 预测结果)\n# 或者\nrmse(真实结果, 预测结果)\n\n# 计算准确率\naccuracy(data, 真实结果, 预测结果)\n# 或者\naccuracy(真实结果, 预测结果)\n如果你是要计算其他指标，只要换个对应的函数即可（演示用，具体的函数名字可能会有差别）。\n对于分类任务的评价指标需要注意，因为分类任务的指标计算有的是需要真实类别和预测类别的，比如混淆矩阵、敏感性、特异性这种，有的是需要真实类别和预测概率的，比如ROC曲线，需要啥就提供啥，不能乱写。\n\n\n\n\n\n\n注释\n\n\n\n由于模型评价指标的计算（和绘图）语法基本上大同小异，你所有需要的只是预测结果和真实结果而已，为了避免无意义的重复，并没有在每个算法中都进行演示，比如校：准曲线和决策曲线只在KNN和CatBoost中演示。",
    "crumbs": [
      "基础知识",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>模型评价</span>"
    ]
  },
  {
    "objectID": "模型评价.html#总结",
    "href": "模型评价.html#总结",
    "title": "4  模型评价",
    "section": "4.4 总结",
    "text": "4.4 总结\n不同的任务类型需要的指标不同，不同的场景下需要的指标也是不同的，需要根据自己的实际情况灵活选择，有时你可能需要宁可错杀，不能放过，但是有时你可能需要宁可放过，不能错杀，所以指标的选择不是随便选的，一定要根据自己的情况来。",
    "crumbs": [
      "基础知识",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>模型评价</span>"
    ]
  },
  {
    "objectID": "超参数调优简介.html",
    "href": "超参数调优简介.html",
    "title": "5  超参数调优",
    "section": "",
    "text": "5.1 什么是超参数\n有些模型的参数对模型最终的结果影响是非常大的，比如K最近邻模型中的K值，也就是近邻的数量，而且这种模型无法从数据中直接算出来，这一类模型参数被称为超参数（hyper-parameter）或者调优参数（tuning-parameter）。\n下图是一个5-最近邻模型的展示。图中标出了需要预测的两个新样本点（实心点和实心三角），圆形的样本位于两个类的边界地带；5个近邻中有3个表明新样本点应属于第一类。另外一个三角形样本的所有5个近邻都表明应预测为第二类。我们应该选择K=3还是K=5呢？\n类似于这样的参数就是超参数，不断尝试这些超参数值并找到最佳超参数的过程就被称为超参数调优。",
    "crumbs": [
      "基础知识",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>超参数调优</span>"
    ]
  },
  {
    "objectID": "超参数调优简介.html#调参方法",
    "href": "超参数调优简介.html#调参方法",
    "title": "5  超参数调优",
    "section": "5.2 调参方法",
    "text": "5.2 调参方法\n主要是3种：\n\n学习曲线：适合只调整1个超参数\n网格搜索：适合同时调整多个超参数\n迭代搜索：复杂，强大，比如贝叶斯优化、模拟退火、遗传算法等",
    "crumbs": [
      "基础知识",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>超参数调优</span>"
    ]
  },
  {
    "objectID": "超参数调优简介.html#过拟合和欠拟合",
    "href": "超参数调优简介.html#过拟合和欠拟合",
    "title": "5  超参数调优",
    "section": "5.3 过拟合和欠拟合",
    "text": "5.3 过拟合和欠拟合\n不同的模型有不同的超参数需要调整，那么我们如何能快速找到最好的超参数？或者说初学者要怎么调整才能更好？\n首先我们要明确，调参的目的是什么？通常都是为了提高模型的准确性。这个准确性，有很多具体的指标可以衡量，比如：AUC、敏感性、特异性、RMSE、R2等。\n有了这个目标后我们就要考虑：哪些因素会影响模型的准确性？这就要了解一个叫泛化误差（genelization error）的概念。\n我们先了解下模型的过拟合（over-fitting）和欠拟合（under-fitting）。\n如下图所示，不同的点代表我们的样本，线条表示模型拟合线，左侧的图形是一个欠拟合的示例，模型并不能很好的拟合我们的样本；右侧的图形是过拟合的示例，模型拟合太过了；中间的图形表明模型拟合的刚好。\n\n模型在新数据上的预测误差被称为泛化误差。我们建立模型之后肯定是要用模型对新数据（新数据指建立模型时未使用过的数据）做出预测的，泛化误差就是用来衡量模型对新数据的预测准确性的。\n当模型在新数据（比如测试集数据或者袋外数据）上表现较差时，就是模型的泛化程度不够，泛化误差大，模型的效果不好。泛化误差受到模型的结构（复杂度）影响。看下面这张图，它展示了泛化误差与模型复杂度的关系，当模型太复杂，模型就会过拟合，泛化能力就不够，所以泛化误差大。当模型太简单，模型就会欠拟合，拟合能力就不够，所以误差也会大。\n\n那模型的复杂度与我们的参数有什么关系呢？以基于树的模型为例进行说明。对树模型来说，树越茂盛，深度越深，枝叶越多，模型就越复杂。所以基于树模型就是容易过拟合的模型，比如随机森林、梯度提升树。所以基于树的模型在调参时，一般都是要减小模型复杂度，防止过拟合。\n泛化误差、过拟合、欠拟合、模型复杂度其实是非常复杂的”偏差-方差”问题，我这里说的太简单了，大家入门之后可以自己阅读一些专业书籍加深理解。",
    "crumbs": [
      "基础知识",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>超参数调优</span>"
    ]
  },
  {
    "objectID": "超参数调优简介.html#参考资料",
    "href": "超参数调优简介.html#参考资料",
    "title": "5  超参数调优",
    "section": "5.4 参考资料",
    "text": "5.4 参考资料\n\n应用预测建模\n周志华机器学习",
    "crumbs": [
      "基础知识",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>超参数调优</span>"
    ]
  },
  {
    "objectID": "变量选择.html",
    "href": "变量选择.html",
    "title": "6  变量选择",
    "section": "",
    "text": "6.1 常见方法简介\n变量选择(特征选择,feature-selection)，是机器学习领域非常重要的问题，到底哪些变量是有用的，哪些是不重要的，可以删除的，怎么选才能提高模型表现，理论非常复杂，实在不是一个临床医生能完全掌握的，以下简单介绍下.\n在传统的临床预测模型中，比较常见的变量筛选方法有：\n本文介绍的机器学习中的变量筛选方法（并没有包含在本书中，可在公众号医学和生信笔记后台回复变量筛选获取相关合集）虽然可以用在临床预测模型中，但是和大家常见的“先单因素后多因素”这种完全不是一个概念，虽然它们的目的相同，都是为了提高模型表现。\n当数据的维度增加时，决定模型最终使用哪些预测变量是很关键的问题。数据的维度就是自变量(预测变量)\n特征选择是特征工程中非常重要的一部分内容，特征选择的方法非常多，主要可以分为以下3类，每个大类下又会细分为好多具体的方法，有机会慢慢介绍…\n大家经常使用的逐步选择法(step/stepAIC)，也属于包装法的一种，在之前的推文中已有介绍：R语言逻辑回归的细节解读，但是并不局限于逻辑回归。\n3种方法的简单解释如下，以后单独演示时会专门再解释：",
    "crumbs": [
      "基础知识",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>变量选择</span>"
    ]
  },
  {
    "objectID": "变量选择.html#常见方法简介",
    "href": "变量选择.html#常见方法简介",
    "title": "6  变量选择",
    "section": "",
    "text": "先单因素后多因素\n最优子集（全子集回归）\n逐步选择法\nlasso回归筛选变量\n随机森林筛选变量\n…\n\n\n\n\n\n过滤法(filter)\n\n缺失值比例、方差、相关系数、方差分析/t检验/卡方检验、ROC等\n信息增益 information gain\n最小冗余最大相关性mrmr，Minimum Redundancy Maximum Relevance\n…\n\n包装法(wrapper)\n\n向前、向后、逐步\n递归特征消除rfe(也属于向后)\n模拟退火\n遗传算法\n…\n\n嵌入法(embeded)\n\n随机森林\nMARS\nlasso\nGBDT\n…\n\n\n\n\n\n过滤法：进行变量选择时不考虑模型表现和变量重要性等，只是通过变量自身的情况、变量间的关系进行选择。\n包装法：变量选择考虑到了模型表现和变量重要性等信息，属于是对每一个模型进行“量身定制”的变量\n嵌入法：变量选择的过程就在模型训练的过程之中",
    "crumbs": [
      "基础知识",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>变量选择</span>"
    ]
  },
  {
    "objectID": "变量选择.html#r语言中的实现",
    "href": "变量选择.html#r语言中的实现",
    "title": "6  变量选择",
    "section": "6.2 R语言中的实现",
    "text": "6.2 R语言中的实现\n后续主要介绍3个包：caret、mlr3、tidymodels\n在caret包中主要可以实现包装法和过滤法。\ncaret包中的封装法有递归特征消除(recursive feature elimination，rfe)算法，遗传算法（genetic algorithms，ga）和模拟退火（Simulated annealing，sa）算法。\n过滤法通过sbf函数实现，但其实部分数据预处理方法属于过滤法的内容。\n目前mlr3已经实现了对这3种方法的支持，可以说是R语言中对变量筛选做的最好的综合性R包了。\n过滤法和嵌入法通过mlr3filters包实现，包装法通过mlr3fselect包实现，关于这两种方法的具体实现，早已在之前的推文介绍过，大家可以参考之前的推文mlr3特征选择\n不过随着mlr3的更新，部分细节稍有不同，以后再给大家慢慢演示。\ntidymodels中的特征选择很不完善，不如mlr3做得好，也不如caret做得好！\n部分过滤法包含在recipes中，部分包装法和嵌入法现在并不成熟，没有完整的实现，部分可通过colina包实现，但是这个包并不属于tidymodels，而是个人开发者贡献的R包。\n已经看到tidymodels的开发者有计划增加特征选择的这部分特性，但不知何时实现…",
    "crumbs": [
      "基础知识",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>变量选择</span>"
    ]
  },
  {
    "objectID": "数据准备.html",
    "href": "数据准备.html",
    "title": "7  数据准备",
    "section": "",
    "text": "本合集主要使用了三个示例数据，使用最多的是著名的皮马印第安人糖尿病数据集，除此之外还使用了R语言自带的鸢尾花数据集以及德国信用评分数据集。\n这里主要为大家简单介绍下印第安人糖尿病数据集的情况。\n\n皮马印第安人糖尿病数据集最初来自国家糖尿病、消化和肾脏疾病研究所，包含来自美国亚利桑那州凤凰城附近的768名女性的信息。检测结果为是否患有糖尿病，其中268人检测呈阳性，500人检测呈阴性。该数据集有8个预测变量：怀孕次数、OGTT（口服葡萄糖耐量试验）、血压、皮肤皱褶厚度、胰岛素、BMI、年龄、糖尿病谱系功能。自1965年以来，国家糖尿病、消化和肾脏疾病研究所每隔两年对皮马人群进行一次研究。由于流行病学证据表明T2DM是遗传和环境因素相互作用的结果，因此皮马印第安人糖尿病数据集包含的有关信息可能与糖尿病的发病及其未来的并发症有关。\n\n印第安人糖尿病数据集是一个二分类数据，我们需要使用多种指标预测患者是否有糖尿病，原始的数据集有很多缺失值，我们需要对它进行插补，在正式开始使用之前，先探索一下这个数据集。\n该数据集一共有768个样本，9个变量，其中diabetes是二分类的结果变量，因子型，其他变量都是预测变量，且都是数值型。\n\n# 加载原始的、有缺失值的数据集\ndata(\"PimaIndiansDiabetes2\", package = \"mlbench\")\ndim(PimaIndiansDiabetes2)\n## [1] 768   9\nstr(PimaIndiansDiabetes2)\n## 'data.frame':    768 obs. of  9 variables:\n##  $ pregnant: num  6 1 8 1 0 5 3 10 2 8 ...\n##  $ glucose : num  148 85 183 89 137 116 78 115 197 125 ...\n##  $ pressure: num  72 66 64 66 40 74 50 NA 70 96 ...\n##  $ triceps : num  35 29 NA 23 35 NA 32 NA 45 NA ...\n##  $ insulin : num  NA NA NA 94 168 NA 88 NA 543 NA ...\n##  $ mass    : num  33.6 26.6 23.3 28.1 43.1 25.6 31 35.3 30.5 NA ...\n##  $ pedigree: num  0.627 0.351 0.672 0.167 2.288 ...\n##  $ age     : num  50 31 32 21 33 30 26 29 53 54 ...\n##  $ diabetes: Factor w/ 2 levels \"neg\",\"pos\": 2 1 2 1 2 1 2 1 2 2 ...\n\n各变量意义解释：\n\npregnant：怀孕次数\nglucose：血浆葡萄糖浓度（葡萄糖耐量试验）\npressure：舒张压（毫米汞柱）\ntriceps：三头肌皮褶厚度（mm）\ninsulin：2小时血清胰岛素（mu U/ml）\nmass：BMI\npedigree：糖尿病谱系功能，是一种用于预测糖尿病发病风险的指标，该指标是基于家族史的糖尿病遗传风险因素的计算得出的。它计算了患者的家族成员是否患有糖尿病以及他们与患者的亲缘关系，从而得出一个综合评分，用于预测患糖尿病的概率。\nage：年龄\ndiabetes：是否有糖尿病，pos表示有糖尿病，neg表示没有糖尿病\n\n我们直接使用GGally包探索一下各个变量的分布情况以及和结果变量之间的关系：\n\nlibrary(GGally)\nggpairs(PimaIndiansDiabetes2,columns = c(1:8),\n        mapping=aes(color=diabetes,alpha=0.3),\n        upper = list(\n          continuous = wrap(\"cor\", size = 2.5) # 改变correlation的font size\n        )\n        )\n\n\n\n\n\n\n\n\n这张图其实就是一个相关系数矩阵的可视化，它展示了8个预测变量彼此之间的相关性。比如第一行第二列中的0.128是pregnant和glucose的相关系数。中间的密度曲线图展示了各个变量在不同组别（有糖尿病和没有糖尿病）的密度分布。\n这里面有两个变量的相关性是比较高的，mass和triceps的相关性是0.648（其实还好，不算非常高）。\n再简单查看下不同指标在糖尿病和非糖尿病组的分布有没有差异：\n\nggbivariate(PimaIndiansDiabetes2, outcome = \"diabetes\")+\n  scale_fill_brewer(type = \"qual\")\n\n\n\n\n\n\n\n\n可以看出各个指标在不同组别确实是有点差异的哦。\n下面简单探索下数据缺失情况：\n\nlibrary(visdat)\n#vis_dat(PimaIndiansDiabetes2)\nvis_miss(PimaIndiansDiabetes2)\n\n\n\n\n\n\n\n\n可以看到triceps和insulin这两列的缺失值比较多。\n下面我们使用随机森林算法插补缺失值：\n\nlibrary(missForest)\n\nset.seed(1234)\npimadiabetes &lt;- missForest(PimaIndiansDiabetes2)$ximp\n\n# 没有缺失值了\nvis_miss(pimadiabetes)\n\n\n\n\n\n\n\n\n默认的结果变量diabetes是因子型的，但是它的因子顺序是先neg再pos，通常我们需要把阳性结果的顺序放在前面，所以这里我们修改一下：\n\nlevels(pimadiabetes$diabetes) &lt;- c(\"pos\",\"neg\")\nstr(pimadiabetes)\n## 'data.frame':    768 obs. of  9 variables:\n##  $ pregnant: num  6 1 8 1 0 5 3 10 2 8 ...\n##  $ glucose : num  148 85 183 89 137 116 78 115 197 125 ...\n##  $ pressure: num  72 66 64 66 40 ...\n##  $ triceps : num  35 29 22.9 23 35 ...\n##  $ insulin : num  202.2 64.6 217.1 94 168 ...\n##  $ mass    : num  33.6 26.6 23.3 28.1 43.1 ...\n##  $ pedigree: num  0.627 0.351 0.672 0.167 2.288 ...\n##  $ age     : num  50 31 32 21 33 30 26 29 53 54 ...\n##  $ diabetes: Factor w/ 2 levels \"pos\",\"neg\": 2 1 2 1 2 1 2 1 2 2 ...\n\n保存下来方便后续使用：\n\nsave(pimadiabetes, file = \"datasets/pimadiabetes.rdata\")\n\n后续的分类任务演示都是使用的pimadiabetes这个数据集哦。",
    "crumbs": [
      "代码实战",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>数据准备</span>"
    ]
  },
  {
    "objectID": "聚类分析.html",
    "href": "聚类分析.html",
    "title": "8  聚类分析",
    "section": "",
    "text": "8.1 系统聚类\n系统聚类又被称为层次聚类，英文：Hierarchical clustering\n使用nutrient数据集进行演示。\n该数据集包括27种食物的不同营养成分含量，我们需要借助聚类分析把这27种食物归为不同的类别。\n# 没安装flexclust包的需要先安装\ndata(nutrient, package = \"flexclust\")\nrow.names(nutrient) &lt;- tolower(row.names(nutrient))\n\ndim(nutrient) # 27行5列\n## [1] 27  5\n\npsych::headTail(nutrient)\n##                 energy protein fat calcium iron\n## beef braised       340      20  28       9  2.6\n## hamburger          245      21  17       9  2.7\n## beef roast         420      15  39       7    2\n## beef steak         375      19  32       9  2.6\n## ...                ...     ... ...     ...  ...\n## salmon canned      120      17   5     159  0.7\n## sardines canned    180      22   9     367  2.5\n## tuna canned        170      25   7       7  1.2\n## shrimp canned      110      23   1      98  2.6\n层次聚类在R语言中非常简单，通过hclust实现。\n不管是哪种类型的聚类分析，都需要找到一个评价相似性的指标，通常使用相似系数（similarity coefficient）衡量，相似系数类似于相关系数，但是并不是完全一样。\n由于计算相似系数需要计算不同样本间的距离，因此就需要对数据提前进行标准化处理，以消除不同单位对计算的影响。一般来说，样本间的距离越大，样本间的相异质性越大，相似性越小。\n计算距离时，有以下几种方法（其实还有非常多，可参考距离定义）：\n计算相似系数也有多种不同的方法，比如：\n由于聚类分析需要计算距离，所以要把所有变量的单位放在同一尺度上，不能有的是几万，有的是零点几。\n# 聚类前先进行标准化\nnutrient.scaled &lt;- scale(nutrient)\n\n# 然后计算距离，方法就选择欧氏距离。这也是默认方法\nd &lt;- dist(nutrient.scaled,method = \"euclidean\")\n\n# 进行层次聚类\nh.clust &lt;- hclust(d, \n                  method = \"average\" # 计算相似系数的方法\n                  )\n这样我们就完成了聚类分析。\nplot(h.clust,main = \"\",xlab = \"\")\n树状图应该从下往上读，它展示了这些食物如何被聚到一起。\n每个观测值起初自成一个聚类簇，然后相距最近的两个聚类簇（beef braised和smoked ham)合并。比如，pork roast和pork simmered合并，chicken canned和 tuna canned合并。\n然后，beef braised/smoked ham这一簇和pork roast/pork simmered这一簇合并（这个聚类簇目前包含4种食品)。合并继续进行下去，直到所有的食物合并成一个聚类簇。\n纵坐标的刻度代表了该高度的聚类簇之间合并的判定值。\n如果我们的目的是理解不同食物营养物质之间的相似性和异质性，那么做到这里就可以满足需求了。但是如果我们想要知道到底聚成几个类才是最合适的，就需要继续探索。\n聚类是把性质相似的样本聚在一起，通常我们也不知道应该分为几类比较合适，那么到底如何选择最佳的聚类个数呢？\n可以通过R包NbClust实现。\nlibrary(NbClust)\n\nnc &lt;- NbClust(nutrient.scaled, distance = \"euclidean\",\n              min.nc = 2, # 最小聚类数\n              max.nc = 10, # 最大聚类数\n              method = \"average\"\n              )\n\n\n\n\n\n\n\n## *** : The Hubert index is a graphical method of determining the number of clusters.\n##                 In the plot of Hubert index, we seek a significant knee that corresponds to a \n##                 significant increase of the value of the measure i.e the significant peak in Hubert\n##                 index second differences plot. \n## \n\n\n\n\n\n\n\n## *** : The D index is a graphical method of determining the number of clusters. \n##                 In the plot of D index, we seek a significant knee (the significant peak in Dindex\n##                 second differences plot) that corresponds to a significant increase of the value of\n##                 the measure. \n##  \n## ******************************************************************* \n## * Among all indices:                                                \n## * 5 proposed 2 as the best number of clusters \n## * 5 proposed 3 as the best number of clusters \n## * 2 proposed 4 as the best number of clusters \n## * 4 proposed 5 as the best number of clusters \n## * 1 proposed 8 as the best number of clusters \n## * 1 proposed 9 as the best number of clusters \n## * 5 proposed 10 as the best number of clusters \n## \n##                    ***** Conclusion *****                            \n##  \n## * According to the majority rule, the best number of clusters is  2 \n##  \n##  \n## *******************************************************************\n输出日志里给出了评判准则以及最终结果。\nHubert index和D index使用图形的方式判断最佳聚类个数，拐点明显的可视作最佳聚类个数。\n它给出的结论是最佳聚类数是2。我们也可以通过条形图查看这些评判准则的具体数量。\nbarplot(table(nc$Best.nc[1,]),\n        xlab = \"聚类数目\",\n        ylab = \"评判准则个数\"\n        )\n从条形图中可以看出，聚类数目为2,3,5,10时，评判准则个数最多，为5个，这里我们可以选择5个（也可以选择2，3，10，根据需要选择即可）。\n# 把聚类树划分为5类\ncluster &lt;- cutree(h.clust, k=5)\n\n# 查看每一类有多少例\ntable(cluster)\n## cluster\n##  1  2  3  4  5 \n##  7 16  1  2  1\n把最终结果画出来：\nplot(h.clust, hang = -1,main = \"\",xlab = \"\")\nrect.hclust(h.clust, k=5) # 添加矩形，方便观察",
    "crumbs": [
      "代码实战",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>聚类分析</span>"
    ]
  },
  {
    "objectID": "聚类分析.html#系统聚类",
    "href": "聚类分析.html#系统聚类",
    "title": "8  聚类分析",
    "section": "",
    "text": "欧氏距离（Euclidean）\n绝对距离（曼哈顿距离，Manhattan）\n马氏距离（Mahalanobis）：欧式距离的平方\n闵可夫斯基距离（Minkowski）\n兰氏距离（Lance and Williams），又称为堪培拉距离（Canberra）\n杰卡德距离（Jaccard），又称为二元距离，用来计算二元变量的距离\n\n\n\nsingle：单联动法，一个簇中的点到另一个簇中的点的最小距离\ncomplete：全联动法，一个簇中的点到另一个簇中的点的最大距离\naverage：平均联动法，一个簇中的点到另一个簇中的点的平均距离\nward.D：Ward法，两个簇之间所有变量的方差分析的平方和\ncentroid：质心法，两个簇的质心之间的距离\n\n\n\n\n\n\n\n提示\n\n\n\n单联动聚类方法倾向于发现细长的、雪茄型的聚类簇。它也通常展示一种链式的现象，即不相似的观测值分到一个聚类簇中，因为它们和它们的中间值很相像。 全联动聚类倾向于发现具有大致相等直径的紧凑型聚类簇。它对异常值很敏感。 平均联动提供了以上两种方法的折中。相对来说，它不像链式，而且对异常值没有那么敏感。它倾向于把方差小的聚类簇聚合。 Ward 法倾向于把有少量观测值的聚类簇聚合到一起，并且倾向于产生与观测值数目大致相等的聚类簇。它对异常值也是敏感的。 质心法是一种很受欢迎的方法，因为其中聚类簇距离的定义比较简单、易于理解。相比其他方法，它对异常值不是很敏感。但是，它可能不如平均联动或Ward法表现得好。 –《R语言实战》",
    "crumbs": [
      "代码实战",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>聚类分析</span>"
    ]
  },
  {
    "objectID": "聚类分析.html#系统聚类可视化",
    "href": "聚类分析.html#系统聚类可视化",
    "title": "8  聚类分析",
    "section": "8.2 系统聚类可视化",
    "text": "8.2 系统聚类可视化\n对于聚类分析，可视化结果是非常重要的一步。\n简单点可以直接用plot()函数进行聚类树的可视化。\n\nplot(h.clust,hang = -1,main = \"层次聚类\", sub=\"\", \n     xlab=\"\", cex.lab = 1.0, cex.axis = 1.0, cex.main = 2)\n\n\n\n\n\n\n\n\n默认的聚类树可视化函数已经非常好用，有非常多的自定义设置，可以轻松实现好看的聚类树可视化。\n\n# 修改画布背景色\nop &lt;- par(bg = \"grey90\")\n\n# 细节修改\nplot(h.clust, \n     main = \"层次聚类\", # 主标题\n     sub=\"\",            # 次标题\n     xlab = \"\",         # x轴标题\n     col = \"#487AA1\",   # 主体颜色\n     col.main = \"#45ADA8\", # 主标题颜色\n     col.lab = \"#7C8071\",  # 坐标轴标签颜色\n     col.axis = \"#F38630\", # 坐标轴颜色\n     lwd = 2,           # 线条宽度\n     lty = 1,           # 线条类型\n     hang = -1,         # 对齐\n     axes = FALSE)      # 不画坐标轴\n# 自己增加坐标轴\naxis(side = 2, at = 0:5, col = \"#F38630\",\n     labels = FALSE, lwd = 2)\n# 增加坐标轴文字\nmtext(0:5, side = 2, at = 0:5,\n      line = 1, col = \"#A38630\", las = 2)\n\n\n\n\n\n\n\n# 修改结束\npar(op)\n\n如果对默认的可视化效果不满意，可以先用as.dendrogram()转化一下，再画图可以指定更多细节。\n\ndhc &lt;- as.dendrogram(h.clust)\nplot(dhc,type = \"triangle\") # 比如换个类型\n\n\n\n\n\n\n\n\n可以提取部分树进行查看，使用cut指定某个高度以上或以下的树进行查看。\n\nop &lt;- par(mfrow = c(2, 1))\n\n# 高度在3以上的树\nplot(cut(dhc, h = 3)$upper, main = \"Upper tree of cut at h=3\")\n\n# 高度在3以下的树\nplot(cut(dhc, h = 3)$lower[[2]],\n     main = \"Second branch of lower tree with cut at h=3\")\n\n\n\n\n\n\n\n\npar(op)\n\n每一个节点都有不同的属性，比如颜色、形状等，我们可以用函数修改每个节点的属性。\n比如修改标签的颜色。\n\n# 按照上面画出来的结果，我们可以分为5类，所以准备好5个颜色\nlabelColors = c(\"#CDB380\", \"#036564\", \"#EB6841\", \"#EDC951\", \"#487AA1\")\n\n# 把聚类树分为5个类\nclusMember &lt;- cutree(h.clust,k=5)\n\n# 给标签增加不同的颜色\ncolLab &lt;- function(n) {\n  if (is.leaf(n)) {\n    a &lt;- attributes(n)\n    labCol &lt;- labelColors[clusMember[which(names(clusMember) == a$label)]]\n    attr(n, \"nodePar\") &lt;- c(a$nodePar,\n                            list(cex=1.5, # 节点形状大小\n                                 pch=20, # 节点形状\n                                 col=labCol, # 节点颜色\n                                 lab.col=labCol, # 标签颜色\n                                 lab.font=2, # 标签字体，粗体斜体粗斜体\n                                 lab.cex=1 # 标签大小\n                                 )\n                            )\n  }\n  n\n}\n\n# 把自定义标签颜色应用到聚类树中\ndiyDendro = dendrapply(dhc, colLab)    \n\n# 画图\nplot(diyDendro, main = \"DIY Dendrogram\")  \n\n# 加图例\nlegend(\"topright\", \n     legend = c(\"Cluster 1\",\"Cluster 2\",\"Cluster 3\",\"Cluster 4\",\"Cluster 5\"), \n     col = c(\"#CDB380\", \"#036564\", \"#EB6841\", \"#EDC951\", \"#487AA1\"), \n     pch = c(20,20,20,20,20), bty = \"n\", pt.cex = 2, cex = 1 , \n     text.col = \"black\", horiz = FALSE, inset = c(0, 0.1))\n\n\n\n\n\n\n\n\n也可以使用colorhcplot包：\n\n# 把聚类树分为5个类\nclusMember &lt;- cutree(h.clust,k=5)\n\ncl &lt;- factor(clusMember, levels = c(1:5),labels = paste(\"cluster\",1:5))\n\nlibrary(colorhcplot)\n## Warning: package 'colorhcplot' was built under R version 4.3.3\ncolorhcplot(h.clust, cl, hang = -1)\n\n\n\n\n\n\n\n\n如果想要更加精美的聚类分析可视化，可以参考之前的几篇推文：\n\n又是聚类分析可视化\nR语言可视化聚类树\nR语言画好看的聚类树",
    "crumbs": [
      "代码实战",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>聚类分析</span>"
    ]
  },
  {
    "objectID": "聚类分析.html#快速聚类",
    "href": "聚类分析.html#快速聚类",
    "title": "8  聚类分析",
    "section": "8.3 快速聚类",
    "text": "8.3 快速聚类\n又被称为划分聚类，partitioning clustering\n\n8.3.1 K-means聚类\nK-means聚类，K均值聚类，是快速聚类的一种。比层次聚类更适合大样本的数据。在R语言中可以通过kmeans()实现K均值聚类。\n使用K均值聚类处理178种葡萄酒中13种化学成分的数据集，目标是根据化学成分的相似性对这178种葡萄酒进行分类。\n\ndata(wine, package = \"rattle\")\n# 标准化数据\ndf &lt;- scale(wine[,-1])\n\npsych::headTail(df)\n##     Alcohol Malic   Ash Alcalinity Magnesium Phenols Flavanoids Nonflavanoids\n## 1      1.51 -0.56  0.23      -1.17      1.91    0.81       1.03         -0.66\n## 2      0.25  -0.5 -0.83      -2.48      0.02    0.57       0.73         -0.82\n## 3       0.2  0.02  1.11      -0.27      0.09    0.81       1.21          -0.5\n## 4      1.69 -0.35  0.49      -0.81      0.93    2.48       1.46         -0.98\n## ...     ...   ...   ...        ...       ...     ...        ...           ...\n## 175    0.49  1.41  0.41       1.05      0.16   -0.79      -1.28          0.55\n## 176    0.33  1.74 -0.39       0.15      1.42   -1.13      -1.34          0.55\n## 177    0.21  0.23  0.01       0.15      1.42   -1.03      -1.35          1.35\n## 178    1.39  1.58  1.36        1.5     -0.26   -0.39      -1.27          1.59\n##     Proanthocyanins Color   Hue Dilution Proline\n## 1              1.22  0.25  0.36     1.84    1.01\n## 2             -0.54 -0.29   0.4     1.11    0.96\n## 3              2.13  0.27  0.32     0.79    1.39\n## 4              1.03  1.18 -0.43     1.18    2.33\n## ...             ...   ...   ...      ...     ...\n## 175           -0.32  0.97 -1.13    -1.48    0.01\n## 176           -0.42  2.22 -1.61    -1.48    0.28\n## 177           -0.23  1.83 -1.56     -1.4     0.3\n## 178           -0.42  1.79 -1.52    -1.42   -0.59\n\n进行K均值聚类时，需要在一开始就指定聚类的个数，我们也可以通过NbClust包实现这个过程。\n\nlibrary(NbClust)\n\nset.seed(123)\nnc &lt;- NbClust(df, min.nc = 2, max.nc = 15, method = \"kmeans\")# 方法选择kmeans\n\n\n\n\n\n\n\n## *** : The Hubert index is a graphical method of determining the number of clusters.\n##                 In the plot of Hubert index, we seek a significant knee that corresponds to a \n##                 significant increase of the value of the measure i.e the significant peak in Hubert\n##                 index second differences plot. \n## \n\n\n\n\n\n\n\n## *** : The D index is a graphical method of determining the number of clusters. \n##                 In the plot of D index, we seek a significant knee (the significant peak in Dindex\n##                 second differences plot) that corresponds to a significant increase of the value of\n##                 the measure. \n##  \n## ******************************************************************* \n## * Among all indices:                                                \n## * 2 proposed 2 as the best number of clusters \n## * 19 proposed 3 as the best number of clusters \n## * 1 proposed 14 as the best number of clusters \n## * 1 proposed 15 as the best number of clusters \n## \n##                    ***** Conclusion *****                            \n##  \n## * According to the majority rule, the best number of clusters is  3 \n##  \n##  \n## *******************************************************************\n\n结果中给出了划分依据以及最佳的聚类数目为3个，可以画图查看结果：\n\ntable(nc$Best.nc[1,])\n## \n##  0  1  2  3 14 15 \n##  2  1  2 19  1  1\n\nbarplot(table(nc$Best.nc[1,]),\n        xlab = \"聚类数目\",\n        ylab = \"评判准则个数\"\n        )\n\n\n\n\n\n\n\n\n可以看到聚类数目为3是最佳的选择。\n确定最佳聚类个数过程也可以通过非常好用的R包factoextra实现。\n\nlibrary(factoextra)\n## Loading required package: ggplot2\n## Welcome! Want to learn more? See two factoextra-related books at https://goo.gl/ve3WBa\n\nset.seed(123)\nfviz_nbclust(df, kmeans, k.max = 15)\n\n\n\n\n\n\n\n\n这个结果给出的最佳聚类个数也是3个。\n下面进行K均值聚类，聚类数目设为3。\nK均值聚类在每次开始前需要随机选择K个质心。\n\n# 设置种子数保证结果可复现\nset.seed(123)\nfit.km &lt;- kmeans(df, centers = 3, nstart = 25) # 尝试多种配置选择最好的一个\nfit.km\n## K-means clustering with 3 clusters of sizes 51, 62, 65\n## \n## Cluster means:\n##      Alcohol      Malic        Ash Alcalinity   Magnesium     Phenols\n## 1  0.1644436  0.8690954  0.1863726  0.5228924 -0.07526047 -0.97657548\n## 2  0.8328826 -0.3029551  0.3636801 -0.6084749  0.57596208  0.88274724\n## 3 -0.9234669 -0.3929331 -0.4931257  0.1701220 -0.49032869 -0.07576891\n##    Flavanoids Nonflavanoids Proanthocyanins      Color        Hue   Dilution\n## 1 -1.21182921    0.72402116     -0.77751312  0.9388902 -1.1615122 -1.2887761\n## 2  0.97506900   -0.56050853      0.57865427  0.1705823  0.4726504  0.7770551\n## 3  0.02075402   -0.03343924      0.05810161 -0.8993770  0.4605046  0.2700025\n##      Proline\n## 1 -0.4059428\n## 2  1.1220202\n## 3 -0.7517257\n## \n## Clustering vector:\n##   [1] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n##  [38] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 1 3 3 3 3 3 3 3 3 3 3 3 2\n##  [75] 3 3 3 3 3 3 3 3 3 1 3 3 3 3 3 3 3 3 3 3 3 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n## [112] 3 3 3 3 3 3 3 1 3 3 2 3 3 3 3 3 3 3 3 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n## [149] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n## \n## Within cluster sum of squares by cluster:\n## [1] 326.3537 385.6983 558.6971\n##  (between_SS / total_SS =  44.8 %)\n## \n## Available components:\n## \n## [1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"     \"tot.withinss\"\n## [6] \"betweenss\"    \"size\"         \"iter\"         \"ifault\"\n\n结果很详细，K均值聚类聚为3类，每一类数量分别是51,62,65。然后还给出了聚类中心（Cluster means），每一个观测分别属于哪一个类（Clustering vector）。\n不管是哪一种聚类方法，factoextra配合factomineR都可以给出非常好看的可视化结果。\n\nfviz_cluster(fit.km, data = df)\n\n\n\n\n\n\n\n\n有非常多的细节可以调整，大家在使用的时候可以自己尝试，和之前推文中介绍的PCA美化一样，也是支持ggplot2语法的。\n\nfviz_cluster(fit.km, data = df, \n             ellipse = T, # 增加椭圆\n             ellipse.type = \"t\", # 椭圆类型\n             geom = \"point\", # 只显示点不要文字\n             palette = \"lancet\", # 支持超多配色方案\n             ggtheme = theme_bw() # 支持更换主题\n             )\n\n\n\n\n\n\n\n\n聚类分析是不需要使用结果变量的，但是我们用的这个葡萄酒数据其实提供了真实的分类结果，此时我们可以评价下这个方法和真实结果的一致性如何，通关计算兰德指数(Rand Index)实现：\n\nlibrary(flexclust)\n## Loading required package: grid\n## Loading required package: lattice\n## Loading required package: modeltools\n## Loading required package: stats4\n\ntb &lt;- table(wine$Type, fit.km$cluster)\nrandIndex(tb)\n##      ARI \n## 0.897495\n\n兰德指数的范围是[-1,1]，-1表示完全不一致，1表示完全一致，所以我们这个结果还是非常好的。\n\n\n8.3.2 围绕中心点的划分PAM\nK均值聚类是基于均值的，所以对异常值很敏感。一个更稳健的方法是围绕中心点的划分（PAM）。用一个最有代表性的观测值代表这一类(有点类似于主成分)。K均值聚类一般使用欧氏距离，而PAM可以使用任意的距离来计算。因此，PAM可以容纳混合数据类型，并且不仅限于连续变量。\n我们还是用葡萄酒数据进行演示。PAM聚类可以通过cluster包中的pam()实现。\n\nlibrary(cluster)\n\nset.seed(123)\nfit.pam &lt;- pam(wine[,-1], k=3 # 聚为3类\n               , stand = T # 聚类前进行标准化\n               )\nfit.pam\n## Medoids:\n##       ID Alcohol Malic  Ash Alcalinity Magnesium Phenols Flavanoids\n## [1,]  36   13.48  1.81 2.41       20.5       100    2.70       2.98\n## [2,] 107   12.25  1.73 2.12       19.0        80    1.65       2.03\n## [3,] 175   13.40  3.91 2.48       23.0       102    1.80       0.75\n##      Nonflavanoids Proanthocyanins Color  Hue Dilution Proline\n## [1,]          0.26            1.86   5.1 1.04     3.47     920\n## [2,]          0.37            1.63   3.4 1.00     3.17     510\n## [3,]          0.43            1.41   7.3 0.70     1.56     750\n## Clustering vector:\n##   [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n##  [38] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 1 2 1 2 2 2 1 2 1 2 1\n##  [75] 1 2 2 2 1 1 2 2 2 3 2 2 2 2 2 2 2 2 2 2 2 1 1 2 1 2 2 2 2 2 2 2 2 2 2 1 1\n## [112] 2 2 2 2 2 2 2 2 2 1 1 3 2 1 2 2 2 2 2 3 3 3 3 2 3 3 3 3 3 3 3 3 3 3 3 3 3\n## [149] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n## Objective function:\n##    build     swap \n## 3.593378 3.476783 \n## \n## Available components:\n##  [1] \"medoids\"    \"id.med\"     \"clustering\" \"objective\"  \"isolation\" \n##  [6] \"clusinfo\"   \"silinfo\"    \"diss\"       \"call\"       \"data\"\n\nMedoids给出了中心点，用第36个观测代表第1类，第107个观测代表第2类，第175个观测代表第3类。Clustering vector给出了每一个观测分别属于哪一个类。结果可以画出来：\n\nclusplot(fit.pam, main = \"PAM cluster\")\n\n\n\n\n\n\n\n\n同样也可以用factoextra包实现可视化。\n\nfviz_cluster(fit.pam, \n             ellipse = T, # 增加椭圆\n             ellipse.type = \"t\", # 椭圆类型\n             geom = \"point\", # 只显示点不要文字\n             palette = \"aaas\", # 支持超多配色方案\n             ggtheme = theme_bw() # 支持更换主题\n             )\n\n\n\n\n\n\n\n\n也是用兰德指数看下一致性：\n\ntb &lt;- table(wine$Type, fit.pam$clustering)\nrandIndex(tb)\n##       ARI \n## 0.6994957\n\n效果不如K均值聚类好。",
    "crumbs": [
      "代码实战",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>聚类分析</span>"
    ]
  },
  {
    "objectID": "聚类分析.html#参考资料",
    "href": "聚类分析.html#参考资料",
    "title": "8  聚类分析",
    "section": "8.4 参考资料",
    "text": "8.4 参考资料\n\nR帮助文档\nhttps://r-graph-gallery.com/31-custom-colors-in-dendrogram.html\nhttps://www.gastonsanchez.com/visually-enforced/how-to/2012/10/0/Dendrograms/\n孙振球版医学统计学第四版\nR语言实战第三版",
    "crumbs": [
      "代码实战",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>聚类分析</span>"
    ]
  },
  {
    "objectID": "主成分分析.html",
    "href": "主成分分析.html",
    "title": "9  主成分分析",
    "section": "",
    "text": "9.1 加载数据\n使用R语言自带的iris鸢尾花数据进行演示。\nstr(iris)\n## 'data.frame':    150 obs. of  5 variables:\n##  $ Sepal.Length: num  5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...\n##  $ Sepal.Width : num  3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...\n##  $ Petal.Length: num  1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...\n##  $ Petal.Width : num  0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...\n##  $ Species     : Factor w/ 3 levels \"setosa\",\"versicolor\",..: 1 1 1 1 1 1 1 1 1 1 ...\npsych::headTail(iris)\n##     Sepal.Length Sepal.Width Petal.Length Petal.Width   Species\n## 1            5.1         3.5          1.4         0.2    setosa\n## 2            4.9           3          1.4         0.2    setosa\n## 3            4.7         3.2          1.3         0.2    setosa\n## 4            4.6         3.1          1.5         0.2    setosa\n## ...          ...         ...          ...         ...      &lt;NA&gt;\n## 147          6.3         2.5            5         1.9 virginica\n## 148          6.5           3          5.2           2 virginica\n## 149          6.2         3.4          5.4         2.3 virginica\n## 150          5.9           3          5.1         1.8 virginica\n首先给大家介绍下R自带的主成分分析函数。",
    "crumbs": [
      "代码实战",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>主成分分析</span>"
    ]
  },
  {
    "objectID": "主成分分析.html#相关性检验",
    "href": "主成分分析.html#相关性检验",
    "title": "9  主成分分析",
    "section": "9.2 相关性检验",
    "text": "9.2 相关性检验\n在进行PCA之前可以先进行相关性分析，看看相关系数：\n\ncor(iris[,-5])\n##              Sepal.Length Sepal.Width Petal.Length Petal.Width\n## Sepal.Length    1.0000000  -0.1175698    0.8717538   0.8179411\n## Sepal.Width    -0.1175698   1.0000000   -0.4284401  -0.3661259\n## Petal.Length    0.8717538  -0.4284401    1.0000000   0.9628654\n## Petal.Width     0.8179411  -0.3661259    0.9628654   1.0000000",
    "crumbs": [
      "代码实战",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>主成分分析</span>"
    ]
  },
  {
    "objectID": "主成分分析.html#kmo和bartlett球形检验",
    "href": "主成分分析.html#kmo和bartlett球形检验",
    "title": "9  主成分分析",
    "section": "9.3 KMO和Bartlett球形检验",
    "text": "9.3 KMO和Bartlett球形检验\n使用psych实现，关于这两个检验的解读大家自行学习~\n\npsych::KMO(iris[,-5])\n## Kaiser-Meyer-Olkin factor adequacy\n## Call: psych::KMO(r = iris[, -5])\n## Overall MSA =  0.54\n## MSA for each item = \n## Sepal.Length  Sepal.Width Petal.Length  Petal.Width \n##         0.58         0.27         0.53         0.63\n\n这个检验主要反应样本量够不够，Overall MSA是总体的检验统计量，然后是每个变量的检验统计量。 MSA越大越好。一般要求大于0.5才可以（没有绝对标准，根据实际情况来）。\n\npsych::cortest.bartlett(iris[,-5])\n## R was not square, finding R from data\n## $chisq\n## [1] 706.9592\n## \n## $p.value\n## [1] 1.92268e-149\n## \n## $df\n## [1] 6\n\np.value小于0.05，表明数据可以进行主成分分析。",
    "crumbs": [
      "代码实战",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>主成分分析</span>"
    ]
  },
  {
    "objectID": "主成分分析.html#r自带的pca",
    "href": "主成分分析.html#r自带的pca",
    "title": "9  主成分分析",
    "section": "9.4 R自带的PCA",
    "text": "9.4 R自带的PCA\n主成分的实现可以通过分步计算，主要就是标准化-求相关矩阵-计算特征值和特征向量。\nR中自带了prcomp()进行主成分分析，这就是工具的魅力，一次完成多步需求。\n使用prcomp()进行主成分分析：\n\n# R自带函数\npca.res &lt;- prcomp(iris[,-5], scale. = T, # 标准化\n                  center = T # 中心化\n                  )\n\n# 查看标准差、特征向量（回归系数）\npca.res\n## Standard deviations (1, .., p=4):\n## [1] 1.7083611 0.9560494 0.3830886 0.1439265\n## \n## Rotation (n x k) = (4 x 4):\n##                     PC1         PC2        PC3        PC4\n## Sepal.Length  0.5210659 -0.37741762  0.7195664  0.2612863\n## Sepal.Width  -0.2693474 -0.92329566 -0.2443818 -0.1235096\n## Petal.Length  0.5804131 -0.02449161 -0.1421264 -0.8014492\n## Petal.Width   0.5648565 -0.06694199 -0.6342727  0.5235971\n\n主成分就是根据这几个系数(这几个系数也叫主成分载荷)算出来的：(这两个并不是一个概念，只是因为数据计算导致此时恰好相等而已，这里的Rotation就是载荷矩阵)\nPC1 = 0.5210659Sepal.Length - 0.2693474Sepal.Width + 0.5804131Petal.Length + 0.5648565Petal.Width\n后面的主成分计算方法以此类推。\n\n# 样本得分score（这个应该是PCA之后每个点的坐标）\nhead(pca.res$x)\n##            PC1        PC2         PC3          PC4\n## [1,] -2.257141 -0.4784238  0.12727962  0.024087508\n## [2,] -2.074013  0.6718827  0.23382552  0.102662845\n## [3,] -2.356335  0.3407664 -0.04405390  0.028282305\n## [4,] -2.291707  0.5953999 -0.09098530 -0.065735340\n## [5,] -2.381863 -0.6446757 -0.01568565 -0.035802870\n## [6,] -2.068701 -1.4842053 -0.02687825  0.006586116\n\n\npca.res$center\n## Sepal.Length  Sepal.Width Petal.Length  Petal.Width \n##     5.843333     3.057333     3.758000     1.199333\n\n\n# 查看标准差、方差贡献率、累积方差贡献率\nsummary(pca.res)\n## Importance of components:\n##                           PC1    PC2     PC3     PC4\n## Standard deviation     1.7084 0.9560 0.38309 0.14393\n## Proportion of Variance 0.7296 0.2285 0.03669 0.00518\n## Cumulative Proportion  0.7296 0.9581 0.99482 1.00000\n\n\nStandard deviation:标准差\nProportion of Variance:方差贡献率\nCumulative Proportion:累积方差贡献率\n\n关于主成分分析中的各种术语解读，我推荐知乎上的一篇文章：主成分分析各类术语的白话解读",
    "crumbs": [
      "代码实战",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>主成分分析</span>"
    ]
  },
  {
    "objectID": "主成分分析.html#结果可视化",
    "href": "主成分分析.html#结果可视化",
    "title": "9  主成分分析",
    "section": "9.5 结果可视化",
    "text": "9.5 结果可视化\n默认的主成分分析结果可视化：\n\nbiplot(pca.res)\n\n\n\n\n\n\n\n\n碎石图可以帮助确认最佳的主成分个数，可以使用默认的screeplot()实现：\n\n# 默认是条形图，我们改为折线图，其实就是方差贡献度的可视化\nscreeplot(pca.res, type = \"lines\")\n\n\n\n\n\n\n\n\n可以看到用2-3个主成分就挺好了。\n\n一般来说，主成分的保留个数可以按照以下原则确定： 1. 以累积贡献率确定，当前K个主成分的累积贡献率达到某一特定值（一般选70%或者80%都行）时，则保留前K个主成分； 2. 以特征值大小来确定：如果主成分的特征值大于1，就保留这个主成分。\n\n但是保留几个主成分并没有绝对的标准，大家根据自己的实际情况来！",
    "crumbs": [
      "代码实战",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>主成分分析</span>"
    ]
  },
  {
    "objectID": "主成分分析可视化.html",
    "href": "主成分分析可视化.html",
    "title": "10  主成分分析可视化",
    "section": "",
    "text": "10.1 factoextra",
    "crumbs": [
      "代码实战",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>主成分分析可视化</span>"
    ]
  },
  {
    "objectID": "主成分分析可视化.html#factoextra",
    "href": "主成分分析可视化.html#factoextra",
    "title": "10  主成分分析可视化",
    "section": "",
    "text": "10.1.1 进行PCA分析\n使用R语言自带的iris鸢尾花数据进行演示。\n\nrm(list = ls())\nlibrary(factoextra)\n## Loading required package: ggplot2\n## Welcome! Want to learn more? See two factoextra-related books at https://goo.gl/ve3WBa\nlibrary(FactoMineR)\n\npca.res &lt;- PCA(iris[,-5], graph = F, scale.unit = T) # 简简单单1行代码实现主成分分析\npca.res\n## **Results for the Principal Component Analysis (PCA)**\n## The analysis was performed on 150 individuals, described by 4 variables\n## *The results are available in the following objects:\n## \n##    name               description                          \n## 1  \"$eig\"             \"eigenvalues\"                        \n## 2  \"$var\"             \"results for the variables\"          \n## 3  \"$var$coord\"       \"coord. for the variables\"           \n## 4  \"$var$cor\"         \"correlations variables - dimensions\"\n## 5  \"$var$cos2\"        \"cos2 for the variables\"             \n## 6  \"$var$contrib\"     \"contributions of the variables\"     \n## 7  \"$ind\"             \"results for the individuals\"        \n## 8  \"$ind$coord\"       \"coord. for the individuals\"         \n## 9  \"$ind$cos2\"        \"cos2 for the individuals\"           \n## 10 \"$ind$contrib\"     \"contributions of the individuals\"   \n## 11 \"$call\"            \"summary statistics\"                 \n## 12 \"$call$centre\"     \"mean of the variables\"              \n## 13 \"$call$ecart.type\" \"standard error of the variables\"    \n## 14 \"$call$row.w\"      \"weights for the individuals\"        \n## 15 \"$call$col.w\"      \"weights for the variables\"\n\n结果信息丰富，可以通过不断的$获取，也可以通过特定函数提取，下面介绍。\n\n\n10.1.2 特征值可视化\n获取特征值、方差贡献率和累积方差贡献率，可以看到和上一篇的结果是一样的：\n\nget_eigenvalue(pca.res)\n##       eigenvalue variance.percent cumulative.variance.percent\n## Dim.1 2.91849782       72.9624454                    72.96245\n## Dim.2 0.91403047       22.8507618                    95.81321\n## Dim.3 0.14675688        3.6689219                    99.48213\n## Dim.4 0.02071484        0.5178709                   100.00000\n\n结果中的这几个概念解释请参考前面。\n通过这几个值，可以确定主成分个数，当然也可以通过碎石图（就是方差解释度的可视化）直观的观察：\n\nfviz_eig(pca.res,addlabels = T,ylim=c(0,100))\n\n\n\n\n\n\n\n\n\n\n10.1.3 提取变量结果\n通过get_pca_var()`函数实现：\n\nres.var &lt;- get_pca_var(pca.res)\nres.var$cor\n##                   Dim.1      Dim.2       Dim.3       Dim.4\n## Sepal.Length  0.8901688 0.36082989 -0.27565767 -0.03760602\n## Sepal.Width  -0.4601427 0.88271627  0.09361987  0.01777631\n## Petal.Length  0.9915552 0.02341519  0.05444699  0.11534978\n## Petal.Width   0.9649790 0.06399985  0.24298265 -0.07535950\nres.var$coord          \n##                   Dim.1      Dim.2       Dim.3       Dim.4\n## Sepal.Length  0.8901688 0.36082989 -0.27565767 -0.03760602\n## Sepal.Width  -0.4601427 0.88271627  0.09361987  0.01777631\n## Petal.Length  0.9915552 0.02341519  0.05444699  0.11534978\n## Petal.Width   0.9649790 0.06399985  0.24298265 -0.07535950\nres.var$contrib       \n##                  Dim.1       Dim.2     Dim.3     Dim.4\n## Sepal.Length 27.150969 14.24440565 51.777574  6.827052\n## Sepal.Width   7.254804 85.24748749  5.972245  1.525463\n## Petal.Length 33.687936  0.05998389  2.019990 64.232089\n## Petal.Width  31.906291  0.44812296 40.230191 27.415396\nres.var$cos2        \n##                  Dim.1       Dim.2       Dim.3        Dim.4\n## Sepal.Length 0.7924004 0.130198208 0.075987149 0.0014142127\n## Sepal.Width  0.2117313 0.779188012 0.008764681 0.0003159971\n## Petal.Length 0.9831817 0.000548271 0.002964475 0.0133055723\n## Petal.Width  0.9311844 0.004095980 0.059040571 0.0056790544\n\n\nres.var$cor:变量和主成分的相关系数\nres.var$coord: 变量在主成分投影上的坐标，下面会结合图说明，因为进行了标准化，所以和相关系数结果一样，其数值代表了主成分和变量之间的相关性\nres.var$cos2: 是coord的平方，也是表示主成分和变量间的相关性，同一个变量所有cos2的总和是1\nres.var$contrib: 变量对主成分的贡献\n\n这几个结果都可以进行可视化。\n\n\n10.1.4 变量结果可视化\n使用fviz_pca_var()对变量结果进行可视化：\n\nfviz_pca_var(pca.res)\n\n\n\n\n\n\n\n\nres.var$coord是变量在主成分投影上的坐标，Sepal.Width在Dim.1的坐标是-0.4601427，在Dim.2的坐标是0.88271627，根据这两个坐标就画出来Sepal.Width那根线了，以此类推~\n\n10.1.4.1 变量和主成分的cos2可视化\ncos2是coord的平方，也是表示主成分和变量间的相关性，所以首先可以画相关图：\n\nlibrary(\"corrplot\")\n## corrplot 0.92 loaded\ncorrplot(res.var$cos2, is.corr = F)\n\n\n\n\n\n\n\n\n可以看到Petal.Length、Petal.Width和Dim1的相关性比较强，Sepal.Width和Dim2的相关性比较强。\n通过fviz_cos2()查看变量在不同主成分的总和，以下是不同变量在第1和第2主成分的加和，如果把axes = 1:2改成axes = 1:4，就会变成都是1（这个数据最多4个主成分，同一变量的cos2在所有主成分的总和是1）。\n\nfviz_cos2(pca.res, choice = \"var\", axes = 1:2)\n\n\n\n\n\n\n\n\n可以通过col.var = \"cos2\"参数给不同变量按照cos2的数值大小上色：\n\nfviz_pca_var(pca.res, col.var = \"cos2\",\n             gradient.cols = c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\"), \n             repel = TRUE \n             )\n\n\n\n\n\n\n\n\n\n# 黑白版本\nfviz_pca_var(pca.res, alpha.var = \"cos2\")\n\n\n\n\n\n\n\n\n\n\n10.1.4.2 变量对主成分的贡献可视化\n\nres.var$contrib\n##                  Dim.1       Dim.2     Dim.3     Dim.4\n## Sepal.Length 27.150969 14.24440565 51.777574  6.827052\n## Sepal.Width   7.254804 85.24748749  5.972245  1.525463\n## Petal.Length 33.687936  0.05998389  2.019990 64.232089\n## Petal.Width  31.906291  0.44812296 40.230191 27.415396\n\n首先也是可以通过画相关性图进行可视化：\n\nlibrary(\"corrplot\")\ncorrplot(res.var$contrib, is.corr=FALSE) \n\n\n\n\n\n\n\n\n通过fviz_contrib()可视化变量对不同主成分的贡献：\n\n# 对第1主成分的贡献\nfviz_contrib(pca.res, choice = \"var\", axes = 1)\n\n\n\n\n\n\n\n\n\n# 对第1和第2主成分的贡献\nfviz_contrib(pca.res, choice = \"var\", axes = 1:2)\n\n\n\n\n\n\n\n\n通过col.var = \"contrib\"参数给不同变量按照contrib的数值大小上色：\n\nfviz_pca_var(pca.res, col.var = \"contrib\",\n             gradient.cols = c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\")\n             )\n\n\n\n\n\n\n\n\n\n\n\n10.1.5 Dimension description\n\nres.desc &lt;- dimdesc(pca.res, axes = c(1,2), proba = 0.05)\n# Description of dimension 1\nres.desc$Dim.1\n## \n## Link between the variable and the continuous variables (R-square)\n## =================================================================================\n##              correlation       p.value\n## Petal.Length   0.9915552 3.369916e-133\n## Petal.Width    0.9649790  6.609632e-88\n## Sepal.Length   0.8901688  2.190813e-52\n## Sepal.Width   -0.4601427  3.139724e-09\n\n\n\n10.1.6 提取样本结果\n使用get_pca_ind()提取样本结果，和变量结果类似：\n\nres.ind &lt;- get_pca_ind(pca.res)\n\nhead(res.ind$coord)          \n##       Dim.1      Dim.2       Dim.3       Dim.4\n## 1 -2.264703  0.4800266 -0.12770602 -0.02416820\n## 2 -2.080961 -0.6741336 -0.23460885 -0.10300677\n## 3 -2.364229 -0.3419080  0.04420148 -0.02837705\n## 4 -2.299384 -0.5973945  0.09129011  0.06595556\n## 5 -2.389842  0.6468354  0.01573820  0.03592281\n## 6 -2.075631  1.4891775  0.02696829 -0.00660818\nhead(res.ind$contrib)      \n##       Dim.1      Dim.2       Dim.3       Dim.4\n## 1 1.1715796 0.16806554 0.074085470 0.018798188\n## 2 0.9891845 0.33146674 0.250034006 0.341474919\n## 3 1.2768164 0.08526419 0.008875320 0.025915633\n## 4 1.2077372 0.26029781 0.037858004 0.140000650\n## 5 1.3046313 0.30516562 0.001125175 0.041530572\n## 6 0.9841236 1.61748779 0.003303827 0.001405371\nhead(res.ind$cos2)          \n##       Dim.1      Dim.2        Dim.3        Dim.4\n## 1 0.9539975 0.04286032 0.0030335249 1.086460e-04\n## 2 0.8927725 0.09369248 0.0113475382 2.187482e-03\n## 3 0.9790410 0.02047578 0.0003422122 1.410446e-04\n## 4 0.9346682 0.06308947 0.0014732682 7.690193e-04\n## 5 0.9315095 0.06823959 0.0000403979 2.104697e-04\n## 6 0.6600989 0.33978301 0.0001114335 6.690714e-06\n\n3个概念和变量的解释也是类似的，只不过上面是变量（列）和主成分的关系，现在是样本（观测，行）和主成分的关系。\n\n\n10.1.7 样本结果可视化\n样本的结果可视化可能是更常见的PCA图形，通过fviz_pca_ind()实现：\n\nfviz_pca_ind(pca.res)\n\n\n\n\n\n\n\n\n这个图是通过res.ind$coord里面的坐标实现的，其实就是不同样本在不同主成分的上面的得分score。\n默认的可视化比较简陋，但是可以通过超多参数实现各种精细化的控制，比如把不同的属性映射给点的大小和颜色，实现各种花里胡哨的效果。\n比如通过组别上色，就是大家最常见的PCA可视化图形：\n\n# 经典图形，是不是很熟悉？\nfviz_pca_ind(pca.res,\n             geom.ind = \"point\", # 只显示点，不要文字\n             col.ind = iris$Species, # 按照组别上色\n             palette = c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\"), # 自己提供颜色，或者使用主题\n             addEllipses = TRUE, # 添加置信椭圆\n             legend.title = \"Groups\"\n             )\n\n\n\n\n\n\n\n\n\n10.1.7.1 样本的cos2可视化\n使用方法和变量的cos2可视化基本一样，通过更改参数值即可实现：\n\nfviz_pca_ind(pca.res,\n             col.ind = \"cos2\", # 按照cos2上色\n             gradient.cols = c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\"),\n             repel = TRUE    \n             )\n\n\n\n\n\n\n\n\n可以更改点的大小、颜色等，只要设置合适的参数即可：\n\nfviz_pca_ind(pca.res, \n             pointsize = \"cos2\", # 把cos2的大小映射给点的大小\n             pointshape = 21, \n             fill = \"#E7B800\",\n             repel = TRUE \n             )\n\n\n\n\n\n\n\n\n同时更改点的大小和颜色当然也是支持的：\n\nfviz_pca_ind(pca.res, \n             col.ind = \"cos2\", # 控制颜色\n             pointsize = \"contrib\", # 控制大小\n             gradient.cols = c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\"),\n             repel = TRUE \n             )\n\n\n\n\n\n\n\n\n使用参数choice = \"ind\"可视化样本对不同主成分的cos2：\n\n# axes选择主成分\nfviz_cos2(pca.res, choice = \"ind\", axes = 1:2)\n\n\n\n\n\n\n\n\n\n\n10.1.7.2 样本对主成分的贡献可视化\n和变量对主成分的贡献可视化非常类似，简单演示下：\n\nfviz_contrib(pca.res, choice = \"ind\", axes = 1:2)\n\n\n\n\n\n\n\n\n\n\n\n10.1.8 biplot\n双标图…\n同时展示变量和样本和主成分的关系，超级多的自定义可视化细节。\n\n# 同时有箭头和椭圆\nfviz_pca_biplot(pca.res, \n                col.ind = iris$Species, \n                palette = \"jco\", \n                addEllipses = TRUE, \n                label = \"var\",\n                col.var = \"black\", \n                repel = TRUE,\n                legend.title = \"Species\"\n                ) \n\n\n\n\n\n\n\n\n\nfviz_pca_biplot(pca.res, \n                # 组别映射给点的填充色\n                geom.ind = \"point\",\n                pointshape = 21,\n                pointsize = 2.5,\n                fill.ind = iris$Species,\n                col.ind = \"black\",\n                # 通过自定义分组给变量上色\n                col.var = factor(c(\"sepal\", \"sepal\", \"petal\", \"petal\")),\n                # 自定义图例标题\n                legend.title = list(fill = \"Species\", color = \"Clusters\"),\n                repel = TRUE        \n             )+\n  ggpubr::fill_palette(\"jco\")+ # 选择点的填充色的配色\n  ggpubr::color_palette(\"npg\") # 选择变量颜色的配色\n\n\n\n\n\n\n\n\n\nfviz_pca_biplot(pca.res, \n                # 自定义样本部分\n                geom.ind = \"point\",\n                fill.ind = iris$Species, # 填充色\n                col.ind = \"black\", # 边框色\n                pointshape = 21, # 点的形状\n                pointsize = 2, \n                palette = \"jco\",\n                addEllipses = TRUE,\n                # 自定义变量部分\n                alpha.var =\"contrib\", col.var = \"contrib\",\n                gradient.cols = \"RdYlBu\",\n                \n                # 自定义图例标题\n                legend.title = list(fill = \"Species\", color = \"Contrib\",\n                                    alpha = \"Contrib\")\n                )\n\n\n\n\n\n\n\n\nfviz_xxx系列可视化函数底层是ggscatter的封装，这个函数来自ggpubr包，所有ggpubr支持的特性都可以给fviz_xxx函数使用，这也是这几个函数功能强大的原因，毕竟底层都是ggplot2!\n下载会继续给大家介绍如何提取PCA的数据，并使用ggplot2可视化，以及三维PCA图的实现。\nfactoextra和factoMineR在聚类分析、主成分分析、因子分析等方面都可以使用。",
    "crumbs": [
      "代码实战",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>主成分分析可视化</span>"
    ]
  },
  {
    "objectID": "主成分分析可视化.html#ggplot2可视化pca",
    "href": "主成分分析可视化.html#ggplot2可视化pca",
    "title": "10  主成分分析可视化",
    "section": "10.2 ggplot2可视化PCA",
    "text": "10.2 ggplot2可视化PCA\n下面说一下如何提取数据用ggplot2画PCA图，以及三维PCA图。\n还是使用鸢尾花数据集。\n\nrm(list = ls())\n\npca.res &lt;- prcomp(iris[,-5], scale. = T, center = T)\npca.res\n## Standard deviations (1, .., p=4):\n## [1] 1.7083611 0.9560494 0.3830886 0.1439265\n## \n## Rotation (n x k) = (4 x 4):\n##                     PC1         PC2        PC3        PC4\n## Sepal.Length  0.5210659 -0.37741762  0.7195664  0.2612863\n## Sepal.Width  -0.2693474 -0.92329566 -0.2443818 -0.1235096\n## Petal.Length  0.5804131 -0.02449161 -0.1421264 -0.8014492\n## Petal.Width   0.5648565 -0.06694199 -0.6342727  0.5235971\n\n在上一篇中提到过，经典的PCA图的横纵坐标其实就是不同样本在不同主成分中的得分，只要提取出来就可以用ggplot2画了。\n\n# 提取得分\ntmp &lt;- as.data.frame(pca.res$x)\nhead(tmp)\n##         PC1        PC2         PC3          PC4\n## 1 -2.257141 -0.4784238  0.12727962  0.024087508\n## 2 -2.074013  0.6718827  0.23382552  0.102662845\n## 3 -2.356335  0.3407664 -0.04405390  0.028282305\n## 4 -2.291707  0.5953999 -0.09098530 -0.065735340\n## 5 -2.381863 -0.6446757 -0.01568565 -0.035802870\n## 6 -2.068701 -1.4842053 -0.02687825  0.006586116\n\n和原数据拼到一起就可以画图了：\n\ntmp$species &lt;- iris$Species\nhead(tmp)\n##         PC1        PC2         PC3          PC4 species\n## 1 -2.257141 -0.4784238  0.12727962  0.024087508  setosa\n## 2 -2.074013  0.6718827  0.23382552  0.102662845  setosa\n## 3 -2.356335  0.3407664 -0.04405390  0.028282305  setosa\n## 4 -2.291707  0.5953999 -0.09098530 -0.065735340  setosa\n## 5 -2.381863 -0.6446757 -0.01568565 -0.035802870  setosa\n## 6 -2.068701 -1.4842053 -0.02687825  0.006586116  setosa\n\n\nlibrary(ggplot2)\nlibrary(ggsci)\n\nggplot(tmp, aes(PC1, PC2))+\n  geom_point(aes(color = species))+\n  stat_ellipse(aes(fill=species), alpha = 0.2,\n               geom =\"polygon\",type = \"norm\")+\n  scale_fill_aaas()+\n  scale_color_aaas()+\n  theme_bw()",
    "crumbs": [
      "代码实战",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>主成分分析可视化</span>"
    ]
  },
  {
    "objectID": "主成分分析可视化.html#d版pca可视化",
    "href": "主成分分析可视化.html#d版pca可视化",
    "title": "10  主成分分析可视化",
    "section": "10.3 3d版PCA可视化",
    "text": "10.3 3d版PCA可视化\n其实就是使用3个主成分，之前介绍过一种：使用R语言美化PCA图，使用方法非常简单，也是在文献中学习到的。\n\n今天再介绍下scatterplot3d包。\n\nlibrary(scatterplot3d)\n\nscatterplot3d(tmp[,1:3], # 第1-3主成分\n              # 颜色长度要和样本长度一样，且对应！\n              color = rep(c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\"),each=50),\n              pch = 15,\n              lty.hide = 2\n              )\nlegend(\"topleft\",c('Setosa','Versicolor','Virginica'),\nfill=c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\"),box.col=NA)\n\n\n\n\n\n\n\n\n是不是很简单呢？",
    "crumbs": [
      "代码实战",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>主成分分析可视化</span>"
    ]
  },
  {
    "objectID": "多元线性回归.html",
    "href": "多元线性回归.html",
    "title": "11  多元线性回归",
    "section": "",
    "text": "11.1 建立模型\n使用一个医学中常见的数据进行演示：探索不同因素对空腹血糖的影响，数据录入如下：\ndf15_1 &lt;- data.frame(\n  cho = c(5.68,3.79,6.02,4.85,4.60,6.05,4.90,7.08,3.85,4.65,4.59,4.29,7.97,\n      6.19,6.13,5.71,6.40,6.06,5.09,6.13,5.78,5.43,6.50,7.98,11.54,5.84,\n      3.84),\n  tg = c(1.90,1.64,3.56,1.07,2.32,0.64,8.50,3.00,2.11,0.63,1.97,1.97,1.93,\n      1.18,2.06,1.78,2.40,3.67,1.03,1.71,3.36,1.13,6.21,7.92,10.89,0.92,\n      1.20),\n  ri = c(4.53, 7.32,6.95,5.88,4.05,1.42,12.60,6.75,16.28,6.59,3.61,6.61,7.57,\n      1.42,10.35,8.53,4.53,12.79,2.53,5.28,2.96,4.31,3.47,3.37,1.20,8.61,\n      6.45),\n  hba = c(8.2,6.9,10.8,8.3,7.5,13.6,8.5,11.5,7.9,7.1,8.7,7.8,9.9,6.9,10.5,8.0,\n      10.3,7.1,8.9,9.9,8.0,11.3,12.3,9.8,10.5,6.4,9.6),\n  fpg = c(11.2,8.8,12.3,11.6,13.4,18.3,11.1,12.1,9.6,8.4,9.3,10.6,8.4,9.6,10.9,\n     10.1,14.8,9.1,10.8,10.2,13.6,14.9,16.0,13.2,20.0,13.3,10.4)\n  )\n\nstr(df15_1)\n## 'data.frame':    27 obs. of  5 variables:\n##  $ cho: num  5.68 3.79 6.02 4.85 4.6 6.05 4.9 7.08 3.85 4.65 ...\n##  $ tg : num  1.9 1.64 3.56 1.07 2.32 0.64 8.5 3 2.11 0.63 ...\n##  $ ri : num  4.53 7.32 6.95 5.88 4.05 ...\n##  $ hba: num  8.2 6.9 10.8 8.3 7.5 13.6 8.5 11.5 7.9 7.1 ...\n##  $ fpg: num  11.2 8.8 12.3 11.6 13.4 18.3 11.1 12.1 9.6 8.4 ...\nhead(df15_1)\n##    cho   tg   ri  hba  fpg\n## 1 5.68 1.90 4.53  8.2 11.2\n## 2 3.79 1.64 7.32  6.9  8.8\n## 3 6.02 3.56 6.95 10.8 12.3\n## 4 4.85 1.07 5.88  8.3 11.6\n## 5 4.60 2.32 4.05  7.5 13.4\n## 6 6.05 0.64 1.42 13.6 18.3\n数据一共5列，第1列是总胆固醇，第2列是甘油三酯，第3列是胰岛素，第4列是糖化血红蛋白，第5列是空腹血糖（因变量）。\n在建立回归方程前，先简单探索下数据：\nlibrary(GGally)\n## Warning: package 'GGally' was built under R version 4.3.3\n\nggpairs(df15_1) + theme_bw()\n从这幅图来看，血糖和糖化血红蛋白相关性最大，和甘油三酯关系最小。\n接下来建立回归方程：\nf &lt;- lm(fpg ~ cho + tg + ri + hba, data = df15_1)\n\nsummary(f)\n## \n## Call:\n## lm(formula = fpg ~ cho + tg + ri + hba, data = df15_1)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -3.6268 -1.2004 -0.2276  1.5389  4.4467 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)  \n## (Intercept)   5.9433     2.8286   2.101   0.0473 *\n## cho           0.1424     0.3657   0.390   0.7006  \n## tg            0.3515     0.2042   1.721   0.0993 .\n## ri           -0.2706     0.1214  -2.229   0.0363 *\n## hba           0.6382     0.2433   2.623   0.0155 *\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 2.01 on 22 degrees of freedom\n## Multiple R-squared:  0.6008, Adjusted R-squared:  0.5282 \n## F-statistic: 8.278 on 4 and 22 DF,  p-value: 0.0003121\n这个结果信息很丰富，给出了截距，各自变量的系数以及标准误、t值、P值，最下方给出了决定系数 R2，调整后的 R2，F值，总体方程的P值等。",
    "crumbs": [
      "代码实战",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>多元线性回归</span>"
    ]
  },
  {
    "objectID": "多元线性回归.html#模型评价",
    "href": "多元线性回归.html#模型评价",
    "title": "11  多元线性回归",
    "section": "11.2 模型评价",
    "text": "11.2 模型评价\n回归模型可以通过 R2、AIC、BIC、RMSE等评价，R2范围在0~1之间，越接近1说明结果越好。AIC、BIC、RMSE是越小越好。\n\nlibrary(performance)\nr2(f)\n## # R2 for Linear Regression\n##        R2: 0.601\n##   adj. R2: 0.528\nAIC(f)\n## [1] 120.78\nBIC(f)\n## [1] 128.5551\nrmse(f)\n## [1] 1.81395\n\n或者直接输出所有结果：\n\nmodel_performance(f)\n## # Indices of model performance\n## \n## AIC     |    AICc |     BIC |    R2 | R2 (adj.) |  RMSE | Sigma\n## ---------------------------------------------------------------\n## 120.780 | 124.980 | 128.555 | 0.601 |     0.528 | 1.814 | 2.010",
    "crumbs": [
      "代码实战",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>多元线性回归</span>"
    ]
  },
  {
    "objectID": "多元线性回归.html#回归诊断",
    "href": "多元线性回归.html#回归诊断",
    "title": "11  多元线性回归",
    "section": "11.3 回归诊断",
    "text": "11.3 回归诊断\n判断数据是否满足多元线性回归的条件，也就是4个条件：\n\n正态性\n独立性\n等方差性\n线性\n\n可以通过回归诊断图判断。\n\nopar &lt;- par(mfrow = c(2,2))\n\nplot(f)\n\n\n\n\n\n\n\n\npar(opar)\n\n\n第1幅图（左上）是残差拟合图，展示真实残差和拟合残差的关系，判读是否满足线性这个条件，如果满足，则应该为一条直线，但是本图明显是一条曲线，说明不是很满足线性这个条件，可能需要加二次项。\n第2幅图（右上）是正态Q-Q图，判断是否满足正态性这个条件，通过这个图来看，基本满足。\n第3幅图（左下）是位置尺度图，判读是否满足同方差性，如果满足，水平线两侧的点应该随机分布，从此图来看基本满足。\n第4幅图（右下）是残差杠杆图，用于识别离群点等。\n\n上面是比较原始的方法，下面介绍一个非常现代化的R包，用于实现以上图形：\n\nlibrary(performance)\ncheck_model(f)\n\n\n\n\n\n\n\n\n是不是更加好看了呢？\n这几个图也可以单独画出来，使用以下代码即可：\n\ndiagnostic_plots &lt;- plot(check_model(f, panel = FALSE))\n\n首先看第一个图。这个图是基于check_predictions()函数的，属于事后检验，是检查真实数据和模型数据的拟合情况的。下图中绿色粗线是真实的预测变量的分布情况，蓝色线条表示模拟的分布，理想的情况应该是完全重合的。从下图来看，其实是有些问题的，这说明我们用的模型可能不太合适。\n\ndiagnostic_plots[[1]]\n\n\n\n\n\n\n\n\n下面看第2张图。这张图是检查预测变量和结果变量是否符合线性关系的。合理的情况是残差完全随机地分布在参考线两侧。从这张图来看我们的数据其实不太完美。\n\ndiagnostic_plots[[2]]\n\n\n\n\n\n\n\n\n下面是第3幅图，是用来检查方差齐性的，同上面介绍过的位置尺度图。\n\ndiagnostic_plots[[3]]\n\n\n\n\n\n\n\n\n第4幅图是用来观察强影响点或者离群值、异常值的。使用的是库克距离（cook’s distance）来计算的，图中在虚线（库克距离）外的点可被认为是异常值。\n\ndiagnostic_plots[[4]]\n\n\n\n\n\n\n\n\n第5幅图是关于多重共线性的。是通过方差膨胀因子来评价的，下图中展示了4个变量的VIF，基本都在3以下，可认为不存在多重共线性：\n\ndiagnostic_plots[[5]]\n\n\n\n\n\n\n\n\n第6幅图是看正态性的。理想情况下数据点应该均匀的分布在横线上，最好是和横线重合，尤其是尾部，我们这个数据还算可以。\n\ndiagnostic_plots[[6]]\n\n\n\n\n\n\n\n\n也可以通过统计方法判断，比如gvlma包可以实现对线性模型的综合判断：\n\nlibrary(gvlma)\ngvmodel&lt;-gvlma(f)\nsummary(gvmodel)\n## \n## Call:\n## lm(formula = fpg ~ cho + tg + ri + hba, data = df15_1)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -3.6268 -1.2004 -0.2276  1.5389  4.4467 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)  \n## (Intercept)   5.9433     2.8286   2.101   0.0473 *\n## cho           0.1424     0.3657   0.390   0.7006  \n## tg            0.3515     0.2042   1.721   0.0993 .\n## ri           -0.2706     0.1214  -2.229   0.0363 *\n## hba           0.6382     0.2433   2.623   0.0155 *\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 2.01 on 22 degrees of freedom\n## Multiple R-squared:  0.6008, Adjusted R-squared:  0.5282 \n## F-statistic: 8.278 on 4 and 22 DF,  p-value: 0.0003121\n## \n## \n## ASSESSMENT OF THE LINEAR MODEL ASSUMPTIONS\n## USING THE GLOBAL TEST ON 4 DEGREES-OF-FREEDOM:\n## Level of Significance =  0.05 \n## \n## Call:\n##  gvlma(x = f) \n## \n##                      Value  p-value                   Decision\n## Global Stat        9.68910 0.046003 Assumptions NOT satisfied!\n## Skewness           0.65344 0.418886    Assumptions acceptable.\n## Kurtosis           0.04015 0.841193    Assumptions acceptable.\n## Link Function      7.68064 0.005582 Assumptions NOT satisfied!\n## Heteroscedasticity 1.31487 0.251515    Assumptions acceptable.\n\n全局统计量：粗略估计结果变量和预测变量是否符合线性关系，结果是不符合 偏度和峰度：检验残差分布，结果是符合 连接函数：检测结果变量是连续型还是二分类，结果是不符合 异方差：检验方差齐性，结果是符合\n以上是多个条件一起输出判断，也可以针对单独的条件进行判断。\n首先看下正态性的判断。\n\nlibrary(car)\n## Loading required package: carData\n\n# 验证正态性\nqqPlot(f,labels = row.names(df15_1), id.method = \"identify\",simulate = T,\n       main = \"Q-Q plot\")   \n\n\n\n\n\n\n\n## [1] 13 26\n\n从图中可看出正态性基本满足。\n当然也可以使用非常好用的performance包实现：\n\ncheck_normality(f)\n## OK: residuals appear as normally distributed (p = 0.671).\n\n检测离群值，基于cook距离：\n\ncheck_outliers(f)\n## 1 outlier detected: case 25.\n## - Based on the following method and threshold: cook (0.9).\n## - For variable: (Whole model).\n\n检测残差（或者因变量）独立性：\n\nset.seed(123)\ncheck_autocorrelation(f)\n## OK: Residuals appear to be independent and not autocorrelated (p = 0.296).\n\n或者通过car包：\n\nset.seed(123)\n# 验证因变量独立性\ndurbinWatsonTest(f)   \n##  lag Autocorrelation D-W Statistic p-value\n##    1       0.1778885      1.634654   0.296\n##  Alternative hypothesis: rho != 0\n\nP值大于0.05，满足条件。\n\n# 验证线性\ncrPlots(f)\n\n\n\n\n\n\n\n\n通过观察成分残差图，线性基本满足。\n下面是检测方差齐性：\n\n# 验证方差齐性\nncvTest(f)   \n## Non-constant Variance Score Test \n## Variance formula: ~ fitted.values \n## Chisquare = 0.0004274839, Df = 1, p = 0.9835\n\nP值大于0.05，方差齐性满足。\n\n# performance检测方差齐性\ncheck_heteroscedasticity(f)\n## OK: Error variance appears to be homoscedastic (p = 0.984).\n\n结果一样的~\n下面是多重共线性的检验，通过计算方差膨胀因子检验\n\nvif(f)\n##      cho       tg       ri      hba \n## 2.185539 1.779862 1.278364 1.266730\nvif(f)&gt;4\n##   cho    tg    ri   hba \n## FALSE FALSE FALSE FALSE\n\n都小于4（标准有争议），基本不存在多重共线性。\n或者通过performance实现：\n\ncheck_collinearity(f)\n## # Check for Multicollinearity\n## \n## Low Correlation\n## \n##  Term  VIF   VIF 95% CI Increased SE Tolerance Tolerance 95% CI\n##   cho 2.19 [1.54, 3.62]         1.48      0.46     [0.28, 0.65]\n##    tg 1.78 [1.31, 2.95]         1.33      0.56     [0.34, 0.76]\n##    ri 1.28 [1.06, 2.32]         1.13      0.78     [0.43, 0.94]\n##   hba 1.27 [1.05, 2.32]         1.13      0.79     [0.43, 0.95]",
    "crumbs": [
      "代码实战",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>多元线性回归</span>"
    ]
  },
  {
    "objectID": "多元线性回归.html#参考资料",
    "href": "多元线性回归.html#参考资料",
    "title": "11  多元线性回归",
    "section": "11.4 参考资料",
    "text": "11.4 参考资料\n\nR语言实战\n孙振球版医学统计学第四版\nperformance包官方文档",
    "crumbs": [
      "代码实战",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>多元线性回归</span>"
    ]
  },
  {
    "objectID": "逻辑回归.html",
    "href": "逻辑回归.html",
    "title": "12  逻辑回归",
    "section": "",
    "text": "12.1 二项逻辑回归\nR语言中的factor()函数可以把变量变为因子类型，默认是没有等级之分的（可以理解为无序分类变量，nominal）！当然也可以通过添加参数ordered=T变成有序因子（等级资料，有序分类，ordinal）。\n因变量是二分类变量时，可以使用二项逻辑回归（binomial logistic regression），自变量可以是数值变量、无序多分类变量、有序多分类变量。\n本次数据使用孙振球版《医学统计学》第4版例16-2的数据，直接读取。\n为了探讨冠心病发生的危险因素，对26例冠心病患者和28例对照者进行病例-对照研究，试用逻辑回归筛选危险因素。\ndf16_2 &lt;- foreign::read.spss(\"datasets/例16-02.sav\", \n                             to.data.frame = T,\n                             use.value.labels = F,\n                             reencode  = \"utf-8\")\n## re-encoding from utf-8\n\nstr(df16_2)\n## 'data.frame':    54 obs. of  11 variables:\n##  $ .... : num  1 2 3 4 5 6 7 8 9 10 ...\n##  $ x1   : num  3 2 2 2 3 3 2 3 2 1 ...\n##  $ x2   : num  1 0 1 0 0 0 0 0 0 0 ...\n##  $ x3   : num  0 1 0 0 0 1 1 1 0 0 ...\n##  $ x4   : num  1 1 1 1 1 1 0 1 0 1 ...\n##  $ x5   : num  0 0 0 0 0 0 0 1 0 0 ...\n##  $ x6   : num  0 0 0 0 1 0 0 0 0 0 ...\n##  $ x7   : num  1 1 1 1 1 2 1 1 1 1 ...\n##  $ x8   : num  1 0 0 0 1 1 0 0 1 0 ...\n##  $ y    : num  0 0 0 0 0 0 0 0 0 0 ...\n##  $ PGR_1: num  1 0 0 0 1 1 0 0 0 0 ...\n##  - attr(*, \"variable.labels\")= Named chr [1:11] \"\" \"\" \"\" \"\" ...\n##   ..- attr(*, \"names\")= chr [1:11] \"....\" \"x1\" \"x2\" \"x3\" ...\n数据一共11列，第1列是编号，第2-9列是自变量，第10列是因变量。\n具体说明： - x1：年龄，小于45岁是1,45-55是2,55-65是3,65以上是4； - x2：高血压病史，1代表有，0代表无； - x3：高血压家族史，1代表有，0代表无； - x4：吸烟，1代表吸烟，0代表不吸烟； - x5：高血脂病史，1代表有，0代表无； - x6：动物脂肪摄入，0表示低，1表示高 - x7：BMI，小于24是1,24-26是2，大于26是3； - x8：A型性格，1代表是，0代表否； - y：是否是冠心病，1代表是，0代表否\n这里的x1~y虽然是数值型，但并不是真的代表数字大小，只是为了方便标识，进行了转换，因此在进行logistic回归之前，我们要把数值型变量变成无序分类或有序分类变量，在R语言中可以通过factor()函数变成因子型实现。\n# 变成因子型\ndf16_2[,c(2:10)] &lt;- lapply(df16_2[,c(2:10)], factor)\nstr(df16_2)\n## 'data.frame':    54 obs. of  11 variables:\n##  $ .... : num  1 2 3 4 5 6 7 8 9 10 ...\n##  $ x1   : Factor w/ 4 levels \"1\",\"2\",\"3\",\"4\": 3 2 2 2 3 3 2 3 2 1 ...\n##  $ x2   : Factor w/ 2 levels \"0\",\"1\": 2 1 2 1 1 1 1 1 1 1 ...\n##  $ x3   : Factor w/ 2 levels \"0\",\"1\": 1 2 1 1 1 2 2 2 1 1 ...\n##  $ x4   : Factor w/ 2 levels \"0\",\"1\": 2 2 2 2 2 2 1 2 1 2 ...\n##  $ x5   : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 2 1 1 ...\n##  $ x6   : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 2 1 1 1 1 1 ...\n##  $ x7   : Factor w/ 3 levels \"1\",\"2\",\"3\": 1 1 1 1 1 2 1 1 1 1 ...\n##  $ x8   : Factor w/ 2 levels \"0\",\"1\": 2 1 1 1 2 2 1 1 2 1 ...\n##  $ y    : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 1 ...\n##  $ PGR_1: num  1 0 0 0 1 1 0 0 0 0 ...\n##  - attr(*, \"variable.labels\")= Named chr [1:11] \"\" \"\" \"\" \"\" ...\n##   ..- attr(*, \"names\")= chr [1:11] \"....\" \"x1\" \"x2\" \"x3\" ...\n需要注意的是自变量x1和x7，这两个应该是有序分类变量，这种自变量在进行逻辑回归时，可以进行哑变量设置，即给定一个参考，让其他所有组都和参考相比，比如这里，我们把x1变成因子型后，R语言在进行logistic回归时，会默认选择第一个为参考。\n接下来进行二项逻辑回归，在R语言中，默认是以因子的第一个为参考的，不仅是自变量，因变量也是如此！ 和SPSS的默认方式不太一样。\nf &lt;- glm(y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8, \n         data = df16_2, \n         family = binomial())\n\nsummary(f)\n## \n## Call:\n## glm(formula = y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8, family = binomial(), \n##     data = df16_2)\n## \n## Coefficients:\n##             Estimate Std. Error z value Pr(&gt;|z|)   \n## (Intercept) -5.46026    2.07370  -2.633  0.00846 **\n## x12          0.85285    1.54399   0.552  0.58070   \n## x13          0.47754    1.59320   0.300  0.76438   \n## x14          3.44227    2.10985   1.632  0.10278   \n## x21          1.14905    0.93176   1.233  0.21750   \n## x31          1.66039    1.16857   1.421  0.15535   \n## x41          0.85994    1.32437   0.649  0.51613   \n## x51          0.73600    0.97088   0.758  0.44840   \n## x61          3.92067    1.57004   2.497  0.01252 * \n## x72         -0.03467    1.13363  -0.031  0.97560   \n## x73         -0.38230    1.61710  -0.236  0.81311   \n## x81          2.46322    1.10484   2.229  0.02578 * \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for binomial family taken to be 1)\n## \n##     Null deviance: 74.786  on 53  degrees of freedom\n## Residual deviance: 40.028  on 42  degrees of freedom\n## AIC: 64.028\n## \n## Number of Fisher Scoring iterations: 6\n结果详解：\nDeviance Residuals:表示偏差残差统计量。在理想情况下应服从正态分布，均值应为0。\n在此例中，中位数的符号为负（-0.01406），表明整体向左偏移，中位数的大小表明偏移的程度。第一个四分位数（1Q）和第三个四分位数（3Q）为两侧“尾巴”分布的幅度。这里3Q大于1Q（绝对值），表明这个曲线是向右倾斜的。最大和最小残差可用来检验数据中的离群值。\n结果中Estimate是回归系数和截距，Std. Error表示回归系数的标准误，z value是统计量值（z的平方就是Wald值），Pr(&gt;|z|)是P值。\nβ值（这里就是Estimate）是指回归系数和截距（常数项），可以是负数（负相关时回归系数出现负值）；\nOR是比值比（odds ratio），OR = exp(β)，其取值范围是0至正无穷，不可能是负数；\nWald是一个卡方值，等于β除以它的标准误（这里是Std. Error），然后取平方（也就是z值的平方），因此也不可能是负数。Wald用于对β值进行检验，考察β值是否等于0。若β值等于0，其对应的OR值，也就是Exp(β)为1，表明两组没有显著差异。Wald值越大，β值越不可能等于0。\n结果中出现了x12/x13/x14这种，这是因为R语言在做回归时，如果设置了哑变量，默认是以第一个为参考的，其余都是和第一个进行比较，这也是R中自动进行哑变量编码的方式。\nNull deviance:无效偏差（零偏差），Residual deviance:残差偏差，无效偏差和残差偏差之间的差异越大越好，用来评价模型与实际数据的吻合情况。\nAIC：赤池信息准则，表示模型拟合程度的好坏，AIC越低表示模型拟合越好。\n最后还有一个Fisher Scoring的迭代次数，这个和最大似然函数的计算有关。\n我们可以通过函数的方式分别获取模型信息。\n# β值\ncoefficients(f)\n## (Intercept)         x12         x13         x14         x21         x31 \n## -5.46025547  0.85285212  0.47754497  3.44227124  1.14905003  1.66039360 \n##         x41         x51         x61         x72         x73         x81 \n##  0.85994185  0.73600239  3.92067487 -0.03467497 -0.38230011  2.46321944\n\n# β值\ncoef(f)\n## (Intercept)         x12         x13         x14         x21         x31 \n## -5.46025547  0.85285212  0.47754497  3.44227124  1.14905003  1.66039360 \n##         x41         x51         x61         x72         x73         x81 \n##  0.85994185  0.73600239  3.92067487 -0.03467497 -0.38230011  2.46321944\n\n# β值的95%可信区间\nconfint(f)\n## Waiting for profiling to be done...\n##                   2.5 %    97.5 %\n## (Intercept) -10.3696980 -1.983104\n## x12          -2.0317236  4.405067\n## x13          -2.5429244  4.085370\n## x14          -0.2319302  8.343123\n## x21          -0.6458070  3.099838\n## x31          -0.5431686  4.205175\n## x41          -1.6713365  3.801261\n## x51          -1.1846658  2.725051\n## x61           1.3290533  7.677657\n## x72          -2.3618580  2.224863\n## x73          -3.8303437  2.725470\n## x81           0.5105394  4.997352\n\n# OR值\nexp(coef(f))\n##  (Intercept)          x12          x13          x14          x21          x31 \n##  0.004252469  2.346329320  1.612111759 31.257871683  3.155194147  5.261381340 \n##          x41          x51          x61          x72          x73          x81 \n##  2.363023282  2.087573511 50.434470096  0.965919321  0.682290259 11.742555242\n\n# OR值的95%的可信区间\nexp(confint(f))\n## Waiting for profiling to be done...\n##                    2.5 %       97.5 %\n## (Intercept) 3.136876e-05    0.1376413\n## x12         1.311093e-01   81.8646261\n## x13         7.863610e-02   59.4639513\n## x14         7.930015e-01 4201.1887167\n## x21         5.242393e-01   22.1943589\n## x31         5.809047e-01   67.0323349\n## x41         1.879956e-01   44.7576059\n## x51         3.058484e-01   15.2571993\n## x61         3.777465e+00 2159.5535363\n## x72         9.424495e-02    9.2522177\n## x73         2.170216e-02   15.2635868\n## x81         1.666190e+00  148.0206875\n这里x21的OR值是2.346329320，代表：45~55岁的人群患冠心病的风险是小于45岁人群的2.346329320倍，但是这个结果并没有统计学意义！\n# Wald值\nsummary(f)$coefficients[,3]^2\n## (Intercept)         x12         x13         x14         x21         x31 \n## 6.933188870 0.305111544 0.089843733 2.661883233 1.520790277 2.018903576 \n##         x41         x51         x61         x72         x73         x81 \n## 0.421615676 0.574682148 6.235929079 0.000935592 0.055890396 4.970577395\n\n# P值\nsummary(f)$coefficients[,4]\n## (Intercept)         x12         x13         x14         x21         x31 \n##  0.00846107  0.58069555  0.76437591  0.10277898  0.21749994  0.15535128 \n##         x41         x51         x61         x72         x73         x81 \n##  0.51613195  0.44840433  0.01251839  0.97559855  0.81311338  0.02578204\n\n# 预测值\nfitted(f) # 或者 predict(f,type = \"response\")\n##           1           2           3           4           5           6 \n## 0.375076515 0.110360122 0.069240725 0.023034427 0.905605901 0.491543165 \n##           7           8           9          10          11          12 \n## 0.049878030 0.151052146 0.104875967 0.009948712 0.208062753 0.046013662 \n##          13          14          15          16          17          18 \n## 0.009879122 0.497751927 0.500211100 0.074509703 0.023034427 0.105543397 \n##          19          20          21          22          23          24 \n## 0.359548891 0.441102099 0.048627400 0.734770361 0.366272916 0.009879122 \n##          25          26          27          28          29          30 \n## 0.049878030 0.366272916 0.009879122 0.101665098 0.995553588 0.950848767 \n##          31          32          33          34          35          36 \n## 0.712839656 0.995611072 0.216828996 0.984826081 0.543195397 0.905612594 \n##          37          38          39          40          41          42 \n## 0.868286980 0.993760333 0.868286980 0.034813473 0.902606657 0.966930037 \n##          43          44          45          46          47          48 \n## 0.375076515 0.964725296 0.840087511 0.818110300 0.881331876 0.676305952 \n##          49          50          51          52          53          54 \n## 0.780828686 0.555921773 0.986103872 0.816157300 0.466253375 0.655579178\n\n# 偏差\ndeviance(f)\n## [1] 40.02758\n\n# 残差自由度\ndf.residual(f)\n## [1] 42\n\n# 伪R^2\nDescTools::PseudoR2(f, which = c(\"McFadden\", \"CoxSnell\", \"Nagelkerke\"))\n##   McFadden   CoxSnell Nagelkerke \n##  0.4647704  0.4746397  0.6331426\n这里需要说明以下fitted(f)，也就是predict(f,type = \"response\")得到的结果是预测概率，范围是0-1之间的。\n对于logistic回归来说，如果不使用type函数，默认是type = \"link\"，返回的是logit(P)的值。\n# 默认返回logit(P)的值\npredict(f)\n\n# 返回概率\npredict(f, type = \"response\")\n模型整体的假设检验：\n# 先构建一个只有截距的模型\nf0 &lt;- glm(y ~ 1, data = df16_2, family = binomial())\n\n# 原模型和这个空模型进行比较\nanova(f0,f,test=\"Chisq\")\n## Analysis of Deviance Table\n## \n## Model 1: y ~ 1\n## Model 2: y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8\n##   Resid. Df Resid. Dev Df Deviance  Pr(&gt;Chi)    \n## 1        53     74.786                          \n## 2        42     40.028 11   34.758 0.0002716 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nP&lt;0.001，说明我们的模型是有意义的。\n逐步回归法的logistic回归，可以使用step()函数：\n# 向前\nf1 &lt;- step(f, direction = \"forward\")\n## Start:  AIC=64.03\n## y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8\nsummary(f1)\n## \n## Call:\n## glm(formula = y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8, family = binomial(), \n##     data = df16_2)\n## \n## Coefficients:\n##             Estimate Std. Error z value Pr(&gt;|z|)   \n## (Intercept) -5.46026    2.07370  -2.633  0.00846 **\n## x12          0.85285    1.54399   0.552  0.58070   \n## x13          0.47754    1.59320   0.300  0.76438   \n## x14          3.44227    2.10985   1.632  0.10278   \n## x21          1.14905    0.93176   1.233  0.21750   \n## x31          1.66039    1.16857   1.421  0.15535   \n## x41          0.85994    1.32437   0.649  0.51613   \n## x51          0.73600    0.97088   0.758  0.44840   \n## x61          3.92067    1.57004   2.497  0.01252 * \n## x72         -0.03467    1.13363  -0.031  0.97560   \n## x73         -0.38230    1.61710  -0.236  0.81311   \n## x81          2.46322    1.10484   2.229  0.02578 * \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for binomial family taken to be 1)\n## \n##     Null deviance: 74.786  on 53  degrees of freedom\n## Residual deviance: 40.028  on 42  degrees of freedom\n## AIC: 64.028\n## \n## Number of Fisher Scoring iterations: 6\n\n# 向后\nf2 &lt;- step(f, direction = \"backward\")\n## Start:  AIC=64.03\n## y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8\n## \n##        Df Deviance    AIC\n## - x7    2   40.086 60.086\n## - x1    3   43.933 61.933\n## - x4    1   40.466 62.466\n## - x5    1   40.605 62.605\n## - x2    1   41.600 63.600\n## &lt;none&gt;      40.028 64.028\n## - x3    1   42.196 64.196\n## - x8    1   46.365 68.365\n## - x6    1   50.469 72.469\n## \n## Step:  AIC=60.09\n## y ~ x1 + x2 + x3 + x4 + x5 + x6 + x8\n## \n##        Df Deviance    AIC\n## - x1    3   44.541 58.541\n## - x5    1   40.611 58.611\n## - x4    1   40.814 58.814\n## - x2    1   41.616 59.616\n## &lt;none&gt;      40.086 60.086\n## - x3    1   42.747 60.747\n## - x8    1   47.255 65.255\n## - x6    1   51.415 69.415\n## \n## Step:  AIC=58.54\n## y ~ x2 + x3 + x4 + x5 + x6 + x8\n## \n##        Df Deviance    AIC\n## - x5    1   45.746 57.746\n## - x4    1   45.779 57.779\n## - x3    1   45.853 57.853\n## &lt;none&gt;      44.541 58.541\n## - x2    1   46.763 58.763\n## - x8    1   50.136 62.136\n## - x6    1   54.588 66.588\n## \n## Step:  AIC=57.75\n## y ~ x2 + x3 + x4 + x6 + x8\n## \n##        Df Deviance    AIC\n## - x4    1   47.537 57.537\n## &lt;none&gt;      45.746 57.746\n## - x2    1   48.470 58.470\n## - x3    1   49.083 59.083\n## - x8    1   51.976 61.976\n## - x6    1   56.634 66.634\n## \n## Step:  AIC=57.54\n## y ~ x2 + x3 + x6 + x8\n## \n##        Df Deviance    AIC\n## &lt;none&gt;      47.537 57.537\n## - x3    1   50.276 58.276\n## - x2    1   51.418 59.418\n## - x8    1   53.869 61.869\n## - x6    1   59.649 67.649\nsummary(f2)\n## \n## Call:\n## glm(formula = y ~ x2 + x3 + x6 + x8, family = binomial(), data = df16_2)\n## \n## Coefficients:\n##             Estimate Std. Error z value Pr(&gt;|z|)    \n## (Intercept)  -3.0314     0.8965  -3.381 0.000722 ***\n## x21           1.4715     0.7656   1.922 0.054617 .  \n## x31           1.2251     0.7543   1.624 0.104359    \n## x61           3.6124     1.3391   2.698 0.006985 ** \n## x81           1.8639     0.8045   2.317 0.020505 *  \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for binomial family taken to be 1)\n## \n##     Null deviance: 74.786  on 53  degrees of freedom\n## Residual deviance: 47.537  on 49  degrees of freedom\n## AIC: 57.537\n## \n## Number of Fisher Scoring iterations: 5\n\n# 步进法\nf3 &lt;- step(f, direction = \"both\")\n## Start:  AIC=64.03\n## y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8\n## \n##        Df Deviance    AIC\n## - x7    2   40.086 60.086\n## - x1    3   43.933 61.933\n## - x4    1   40.466 62.466\n## - x5    1   40.605 62.605\n## - x2    1   41.600 63.600\n## &lt;none&gt;      40.028 64.028\n## - x3    1   42.196 64.196\n## - x8    1   46.365 68.365\n## - x6    1   50.469 72.469\n## \n## Step:  AIC=60.09\n## y ~ x1 + x2 + x3 + x4 + x5 + x6 + x8\n## \n##        Df Deviance    AIC\n## - x1    3   44.541 58.541\n## - x5    1   40.611 58.611\n## - x4    1   40.814 58.814\n## - x2    1   41.616 59.616\n## &lt;none&gt;      40.086 60.086\n## - x3    1   42.747 60.747\n## + x7    2   40.028 64.028\n## - x8    1   47.255 65.255\n## - x6    1   51.415 69.415\n## \n## Step:  AIC=58.54\n## y ~ x2 + x3 + x4 + x5 + x6 + x8\n## \n##        Df Deviance    AIC\n## - x5    1   45.746 57.746\n## - x4    1   45.779 57.779\n## - x3    1   45.853 57.853\n## &lt;none&gt;      44.541 58.541\n## - x2    1   46.763 58.763\n## + x1    3   40.086 60.086\n## + x7    2   43.933 61.933\n## - x8    1   50.136 62.136\n## - x6    1   54.588 66.588\n## \n## Step:  AIC=57.75\n## y ~ x2 + x3 + x4 + x6 + x8\n## \n##        Df Deviance    AIC\n## - x4    1   47.537 57.537\n## &lt;none&gt;      45.746 57.746\n## - x2    1   48.470 58.470\n## + x5    1   44.541 58.541\n## + x1    3   40.611 58.611\n## - x3    1   49.083 59.083\n## + x7    2   44.697 60.697\n## - x8    1   51.976 61.976\n## - x6    1   56.634 66.634\n## \n## Step:  AIC=57.54\n## y ~ x2 + x3 + x6 + x8\n## \n##        Df Deviance    AIC\n## &lt;none&gt;      47.537 57.537\n## + x1    3   41.625 57.625\n## + x4    1   45.746 57.746\n## + x5    1   45.779 57.779\n## - x3    1   50.276 58.276\n## - x2    1   51.418 59.418\n## + x7    2   46.792 60.792\n## - x8    1   53.869 61.869\n## - x6    1   59.649 67.649\nsummary(f3)\n## \n## Call:\n## glm(formula = y ~ x2 + x3 + x6 + x8, family = binomial(), data = df16_2)\n## \n## Coefficients:\n##             Estimate Std. Error z value Pr(&gt;|z|)    \n## (Intercept)  -3.0314     0.8965  -3.381 0.000722 ***\n## x21           1.4715     0.7656   1.922 0.054617 .  \n## x31           1.2251     0.7543   1.624 0.104359    \n## x61           3.6124     1.3391   2.698 0.006985 ** \n## x81           1.8639     0.8045   2.317 0.020505 *  \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for binomial family taken to be 1)\n## \n##     Null deviance: 74.786  on 53  degrees of freedom\n## Residual deviance: 47.537  on 49  degrees of freedom\n## AIC: 57.537\n## \n## Number of Fisher Scoring iterations: 5\n按照步进法最终纳入的自变量是x2,x6,x8。",
    "crumbs": [
      "代码实战",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>逻辑回归</span>"
    ]
  },
  {
    "objectID": "逻辑回归.html#多项逻辑回归",
    "href": "逻辑回归.html#多项逻辑回归",
    "title": "12  逻辑回归",
    "section": "12.2 多项逻辑回归",
    "text": "12.2 多项逻辑回归\n因变量是无序多分类资料（＞2）时，可使用多分类逻辑回归（multinomial logistic regression）。\n使用课本例16-5的数据，课本电子版及数据已上传到QQ群，自行下载即可。\n某研究人员欲了解不同社区和性别之间居民获取健康知识的途径是否相同，对2个社区的314名成人进行了调查，其中X1是社区，社区1用0表示，社区2用1表示；X2是性别，0是男，1是女，Y是获取健康知识途径，1是传统大众传媒，2是网络，3是社区宣传。\n\ndf &lt;- read.csv(\"datasets/例16-05.csv\",header = T)\n\npsych::headtail(df)\n## Warning: headtail is deprecated.  Please use the headTail function\n##      X1  X2   Y\n## 1     0   0   1\n## 2     0   0   1\n## 3     0   0   1\n## 4     0   0   1\n## ... ... ... ...\n## 311   1   1   3\n## 312   1   1   3\n## 313   1   1   3\n## 314   1   1   3\n\n首先变为因子型，无需多分类的逻辑回归需要对因变量设置参考，我们这里直接用factor()函数变为因子，这样在进行无序多分类的逻辑回归时默认是以第一个为参考。也可以使用relevel()重新设置参考。\n\ndf$X1 &lt;- factor(df$X1,levels = c(0,1),labels = c(\"社区1\",\"社区2\"))\ndf$X2 &lt;- factor(df$X2,levels = c(0,1),labels = c(\"男\",\"女\"))\n\n# 因变量设置参考，这里选择第1个（传统大众传媒）为参考\ndf$Y &lt;- factor(df$Y,levels = c(1,2,3),labels = c(\"传统大众传媒\",\"网络\",\"社区宣传\"))\n\nstr(df)\n## 'data.frame':    314 obs. of  3 variables:\n##  $ X1: Factor w/ 2 levels \"社区1\",\"社区2\": 1 1 1 1 1 1 1 1 1 1 ...\n##  $ X2: Factor w/ 2 levels \"男\",\"女\": 1 1 1 1 1 1 1 1 1 1 ...\n##  $ Y : Factor w/ 3 levels \"传统大众传媒\",..: 1 1 1 1 1 1 1 1 1 1 ...\n\n使用nnet::multinom进行无序多分类的逻辑回归：\n\nlibrary(nnet)\n\nfit &lt;- multinom(Y ~ X1 + X2, data = df, model = T)\n## # weights:  12 (6 variable)\n## initial  value 344.964259 \n## iter  10 value 316.575399\n## iter  10 value 316.575399\n## iter  10 value 316.575399\n## final  value 316.575399 \n## converged\n\nsummary(fit)\n## Call:\n## multinom(formula = Y ~ X1 + X2, data = df, model = T)\n## \n## Coefficients:\n##          (Intercept)    X1社区2      X2女\n## 网络       0.5484998 -1.3743147 0.4321069\n## 社区宣传   0.3940422 -0.9933526 1.2266459\n## \n## Std. Errors:\n##          (Intercept)   X1社区2      X2女\n## 网络       0.2583299 0.3201514 0.3265384\n## 社区宣传   0.2574175 0.2952083 0.2991714\n## \n## Residual Deviance: 633.1508 \n## AIC: 645.1508\n\n可以看到结果比二项逻辑回归的结果简洁多了，少了很多信息，只给出了Coefficients/Std. Errors/Residual Deviance/AIC。\n不过也是两个模型的结果，分别是 社区宣传 和 传统大众传媒 比，网络 和 传统大众传媒 比。\n自变量的Z值（wald Z, Z-score）和P值需要手动计算:\n\nz_stats &lt;- summary(fit)$coefficients/summary(fit)$standard.errors\n\np_values &lt;- (1 - pnorm(abs(z_stats)))*2\n\np_values\n##          (Intercept)      X1社区2         X2女\n## 网络      0.03373263 1.765117e-05 1.857371e-01\n## 社区宣传  0.12583082 7.656564e-04 4.128929e-05\n\n但其实可以调包解决：\n\nres &lt;- broom::tidy(fit)\nres\n## # A tibble: 6 × 6\n##   y.level  term        estimate std.error statistic   p.value\n##   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n## 1 网络     (Intercept)    0.548     0.258      2.12 0.0337   \n## 2 网络     X1社区2       -1.37      0.320     -4.29 0.0000177\n## 3 网络     X2女           0.432     0.327      1.32 0.186    \n## 4 社区宣传 (Intercept)    0.394     0.257      1.53 0.126    \n## 5 社区宣传 X1社区2       -0.993     0.295     -3.36 0.000766 \n## 6 社区宣传 X2女           1.23      0.299      4.10 0.0000413\n\nOR值及其95%的可信区间也没有给出来，需要手动计算OR值和可信区间：\n\n# 计算OR值\nOR &lt;- exp(coef(fit))\nOR\n##          (Intercept)   X1社区2     X2女\n## 网络        1.730655 0.2530129 1.540500\n## 社区宣传    1.482963 0.3703330 3.409774\n\n# 计算OR值的95%的可信区间\nOR.confi &lt;- exp(confint(fit))\nOR.confi\n## , , 网络\n## \n##                 2.5 %    97.5 %\n## (Intercept) 1.0430848 2.8714501\n## X1社区2     0.1350919 0.4738666\n## X2女        0.8122910 2.9215386\n## \n## , , 社区宣传\n## \n##                 2.5 %    97.5 %\n## (Intercept) 0.8953982 2.4560912\n## X1社区2     0.2076398 0.6605021\n## X2女        1.8970133 6.1288742\n\n模型整体的假设检验：\n\n# 先构建一个只有截距的模型\nfit0 &lt;- multinom(Y ~ 1, data = df, model = T)\n## # weights:  6 (2 variable)\n## initial  value 344.964259 \n## final  value 338.603448 \n## converged\n\n# 两个模型比较,Likelihood ratio tests\nanova(fit0, fit)\n## Likelihood ratio tests of Multinomial Models\n## \n## Response: Y\n##     Model Resid. df Resid. Dev   Test    Df LR stat.      Pr(Chi)\n## 1       1       626   677.2069                                   \n## 2 X1 + X2       622   633.1508 1 vs 2     4  44.0561 6.245931e-09\n\nP&lt;0.001，模型具有统计学意义。\n获取模型预测的类别：\n\npred &lt;- predict(fit, df, type = \"class\")\nhead(pred)\n## [1] 网络 网络 网络 网络 网络 网络\n## Levels: 传统大众传媒 网络 社区宣传\n\n获取模型预测的概率：\n\nprob &lt;- predict(fit, df, type = \"probs\") # 或者使用 fitted(fit)\nhead(prob)\n##   传统大众传媒      网络  社区宣传\n## 1    0.2373257 0.4107289 0.3519453\n## 2    0.2373257 0.4107289 0.3519453\n## 3    0.2373257 0.4107289 0.3519453\n## 4    0.2373257 0.4107289 0.3519453\n## 5    0.2373257 0.4107289 0.3519453\n## 6    0.2373257 0.4107289 0.3519453\n\n模型拟合优度的检验，这里使用卡方检验：\n\nchisq.test(df$Y, pred)\n## \n##  Pearson's Chi-squared test\n## \n## data:  df$Y and pred\n## X-squared = 39.521, df = 4, p-value = 5.436e-08\n\n计算伪 R2：\n\nDescTools::PseudoR2(fit, which = \"all\")\n##        McFadden     McFaddenAdj        CoxSnell      Nagelkerke   AldrichNelson \n##      0.06505559      0.04733575      0.13090778      0.14803636              NA \n## VeallZimmermann           Efron McKelveyZavoina            Tjur             AIC \n##              NA              NA              NA              NA    645.15079819 \n##             BIC          logLik         logLik0              G2 \n##    667.64715610   -316.57539909   -338.60344772     44.05609725\n\n不仅给出了伪R^2，还给出了超多的值，每一项的意义可以参考下面这张图：\n\n结果解读可以参考二项逻辑回归。",
    "crumbs": [
      "代码实战",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>逻辑回归</span>"
    ]
  },
  {
    "objectID": "逻辑回归.html#有序逻辑回归",
    "href": "逻辑回归.html#有序逻辑回归",
    "title": "12  逻辑回归",
    "section": "12.3 有序逻辑回归",
    "text": "12.3 有序逻辑回归\nordinal logistic regression适用于因变量为等级资料。使用课本例16-4的数据。\n随机选取84例患者做临床试验，探讨性别和治疗方法对该病的影响。变量赋值为：性别（X1，男=0，女=1），治疗方法（X2，传统疗法=0，新型疗法=1），疗效（Y，无效=1，有效=2，痊愈=3）。\n\ndf &lt;- read.csv(\"datasets/例16-04.csv\",header = T)\npsych::headtail(df)\n## Warning: headtail is deprecated.  Please use the headTail function\n##      X1  X2   Y\n## 1     0   0   1\n## 2     0   0   1\n## 3     0   0   1\n## 4     0   0   1\n## ... ... ... ...\n## 81    1   1   3\n## 82    1   1   3\n## 83    1   1   3\n## 84    1   1   3\n\n变为因子型：\n\n# 因变量变为有序因子\ndf$Y &lt;- factor(df$Y, levels = c(1,2,3),\n               labels = c(\"无效\",\"有效\",\"痊愈\"),\n               ordered = T)\n\n# 自变量变为无序因子\ndf$X1 &lt;- factor(df$X1,levels = c(0,1),labels = c(\"男\",\"女\"))\ndf$X2 &lt;- factor(df$X2,levels = c(0,1),labels = c(\"传统疗法\",\"新型疗法\"))\n\nstr(df)\n## 'data.frame':    84 obs. of  3 variables:\n##  $ X1: Factor w/ 2 levels \"男\",\"女\": 1 1 1 1 1 1 1 1 1 1 ...\n##  $ X2: Factor w/ 2 levels \"传统疗法\",\"新型疗法\": 1 1 1 1 1 1 1 1 1 1 ...\n##  $ Y : Ord.factor w/ 3 levels \"无效\"&lt;\"有效\"&lt;..: 1 1 1 1 1 1 1 1 1 1 ...\n\n使用MASS::polr拟合有序逻辑回归：\n\nlibrary(MASS)\n\nfit &lt;- polr(Y ~ X1 + X2, data = df,Hess = TRUE,method = \"logistic\")\nsummary(fit)\n## Call:\n## polr(formula = Y ~ X1 + X2, data = df, Hess = TRUE, method = \"logistic\")\n## \n## Coefficients:\n##            Value Std. Error t value\n## X1女       1.319     0.5381   2.451\n## X2新型疗法 1.797     0.4718   3.809\n## \n## Intercepts:\n##           Value  Std. Error t value\n## 无效|有效 1.8128 0.5654     3.2061 \n## 有效|痊愈 2.6672 0.6065     4.3979 \n## \n## Residual Deviance: 150.0294 \n## AIC: 158.0294\n\n结果也是没有给出P值，手动计算P值：\n\np &lt;- pnorm(abs(coef(summary(fit))[, \"t value\"]),lower.tail = F)*2\np\n##         X1女   X2新型疗法    无效|有效    有效|痊愈 \n## 1.425572e-02 1.392807e-04 1.345300e-03 1.092866e-05\n\n这次broom::tidy(fit)并没有直接给出P值：\n\nbroom::tidy(fit)\n## # A tibble: 4 × 5\n##   term       estimate std.error statistic coef.type  \n##   &lt;chr&gt;         &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;      \n## 1 X1女           1.32     0.538      2.45 coefficient\n## 2 X2新型疗法     1.80     0.472      3.81 coefficient\n## 3 无效|有效      1.81     0.565      3.21 scale      \n## 4 有效|痊愈      2.67     0.606      4.40 scale\n\nOR值：\n\nOR &lt;- exp(coef(fit))\nOR\n##       X1女 X2新型疗法 \n##   3.738765   6.033338\n\n平行线检验(Brant-Wald test)：\n\nbrant::brant(fit)\n## -------------------------------------------- \n## Test for X2  df  probability \n## -------------------------------------------- \n## Omnibus      1.83    2   0.4\n## X1女      1.59    1   0.21\n## X2新型疗法       0.01    1   0.94\n## -------------------------------------------- \n## \n## H0: Parallel Regression Assumption holds\n## Warning in brant::brant(fit): 1 combinations in table(dv,ivs) do not occur.\n## Because of that, the test results might be invalid.\n\nP值&gt;0.05，平行线检验通过，可以使用有序逻辑回归。\n模型整体的显著性检验：\n\n# 先构建一个只有截距的模型\nfit0 &lt;- polr(Y ~ 1, data = df,Hess = TRUE,method = \"logistic\")\n\n# 两个模型比较\nanova(fit0, fit)\n## Likelihood ratio tests of ordinal regression models\n## \n## Response: Y\n##     Model Resid. df Resid. Dev   Test    Df LR stat.     Pr(Chi)\n## 1       1        82   169.9159                                  \n## 2 X1 + X2        80   150.0294 1 vs 2     2  19.8865 4.80508e-05\n\nP值＜0.01，模型是有意义的。\n获取模型预测的类别：\n\npred &lt;- predict(fit, df, type = \"class\")\nhead(pred)\n## [1] 无效 无效 无效 无效 无效 无效\n## Levels: 无效 有效 痊愈\n\n获取模型预测的概率：\n\nprob &lt;- predict(fit, df, type = \"probs\") # 或者使用 fitted(fit)\nhead(prob)\n##        无效       有效       痊愈\n## 1 0.8597003 0.07536263 0.06493706\n## 2 0.8597003 0.07536263 0.06493706\n## 3 0.8597003 0.07536263 0.06493706\n## 4 0.8597003 0.07536263 0.06493706\n## 5 0.8597003 0.07536263 0.06493706\n## 6 0.8597003 0.07536263 0.06493706\n\n模型拟合优度的检验，这里使用卡方检验：\n\nchisq.test(df$Y, pred)\n## Warning in chisq.test(df$Y, pred): Chi-squared approximation may be incorrect\n## \n##  Pearson's Chi-squared test\n## \n## data:  df$Y and pred\n## X-squared = 14.246, df = 2, p-value = 0.0008065\n\n计算伪R2：\n\nDescTools::PseudoR2(fit, which = \"all\")\n##        McFadden        CoxSnell      Nagelkerke   AldrichNelson VeallZimmermann \n##       0.1170373       0.2108068       0.2429443              NA              NA \n##           Efron McKelveyZavoina            Tjur             AIC             BIC \n##              NA              NA              NA     158.0294131     167.7526803 \n##          logLik         logLik0              G2 \n##     -75.0147065     -84.9579583      19.8865036",
    "crumbs": [
      "代码实战",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>逻辑回归</span>"
    ]
  },
  {
    "objectID": "逻辑回归.html#条件逻辑回归",
    "href": "逻辑回归.html#条件逻辑回归",
    "title": "12  逻辑回归",
    "section": "12.4 条件逻辑回归",
    "text": "12.4 条件逻辑回归\nconditional logistic regression是针对配对数据资料分析的一种方法。在一些病例-对照研究中，把病例和对照按照年龄、性别等进行配对，形成多个匹配组，各匹配组的病例数和对照数是任意的，并不是1个对1个，常用的是每组中有一个病例和多个对照，即1：M配对研究。\n使用课本例16-3的数据。某北方城市研究喉癌发病的危险因素，用1:2配对研究，现选取了6个可能的危险因素并记录了25对数据，试做条件逻辑回归。\n\n\ndf &lt;- foreign::read.spss(\"datasets/例16-03.sav\",to.data.frame = T)\npsych::headtail(df)\n## Warning: headtail is deprecated.  Please use the headTail function\n##       i   y  x1  x2  x3  x4  x5  x6\n## 1     1   1   3   5   1   1   1   0\n## 2     1   0   1   1   1   3   3   0\n## 3     1   0   1   1   1   3   3   0\n## 4     2   1   1   3   1   1   3   0\n## ... ... ... ... ... ... ... ... ...\n## 72   24   0   1   1   2   3   2   0\n## 73   25   1   1   4   1   1   1   1\n## 74   25   0   1   1   1   3   2   0\n## 75   25   0   1   1   1   3   3   0\nstr(df)\n## 'data.frame':    75 obs. of  8 variables:\n##  $ i : num  1 1 1 2 2 2 3 3 3 4 ...\n##  $ y : num  1 0 0 1 0 0 1 0 0 1 ...\n##  $ x1: num  3 1 1 1 1 1 1 1 1 1 ...\n##  $ x2: num  5 1 1 3 1 2 4 5 4 4 ...\n##  $ x3: num  1 1 1 1 1 1 1 1 1 1 ...\n##  $ x4: num  1 3 3 1 3 3 3 3 3 2 ...\n##  $ x5: num  1 3 3 3 2 2 2 2 2 1 ...\n##  $ x6: num  0 0 0 0 0 0 0 0 0 1 ...\n\ni是配对的对子数，不需要变成因子型。\n使用survival::clogit进行条件逻辑回归：\n\nlibrary(survival)\n\nfit &lt;- clogit(y ~ x1+x2+x3+x4+x5+x6+strata(i), data = df, method = \"exact\")\n\nsummary(fit)\n## Call:\n## coxph(formula = Surv(rep(1, 75L), y) ~ x1 + x2 + x3 + x4 + x5 + \n##     x6 + strata(i), data = df, method = \"exact\")\n## \n##   n= 75, number of events= 25 \n## \n##        coef exp(coef) se(coef)      z Pr(&gt;|z|)  \n## x1  2.58880  13.31380  2.50172  1.035   0.3008  \n## x2  1.68796   5.40843  0.68545  2.463   0.0138 *\n## x3  2.31944  10.16995  1.26096  1.839   0.0659 .\n## x4 -3.88886   0.02047  1.90656 -2.040   0.0414 *\n## x5 -0.49102   0.61200  1.19020 -0.413   0.6799  \n## x6  3.50899  33.41447  2.13723  1.642   0.1006  \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n##    exp(coef) exp(-coef) lower .95 upper .95\n## x1  13.31380    0.07511 0.0988170 1793.7921\n## x2   5.40843    0.18490 1.4112830   20.7266\n## x3  10.16995    0.09833 0.8589963  120.4056\n## x4   0.02047   48.85506 0.0004878    0.8589\n## x5   0.61200    1.63399 0.0593818    6.3074\n## x6  33.41447    0.02993 0.5066653 2203.6771\n## \n## Concordance= 0.91  (se = 0.064 )\n## Likelihood ratio test= 42.21  on 6 df,   p=2e-07\n## Wald test            = 7.71  on 6 df,   p=0.3\n## Score (logrank) test = 29.13  on 6 df,   p=6e-05\n\n结果非常齐全，β值，OR值，P值等信息都有了。",
    "crumbs": [
      "代码实战",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>逻辑回归</span>"
    ]
  },
  {
    "objectID": "逻辑回归.html#参考资料",
    "href": "逻辑回归.html#参考资料",
    "title": "12  逻辑回归",
    "section": "12.5 参考资料",
    "text": "12.5 参考资料\n\nhttps://blog.csdn.net/weixin_41744624/article/details/105506951\nhttps://zhuanlan.zhihu.com/p/113403422\nhttps://duanku.pai-hang-bang.cn/kuzi_1046977453210716059\nhttps://bookdown.org/chua/ber642_advanced_regression/\nhttps://peopleanalytics-regression-book.org/\n孙振球版医学统计学第四版",
    "crumbs": [
      "代码实战",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>逻辑回归</span>"
    ]
  },
  {
    "objectID": "glmnet.html",
    "href": "glmnet.html",
    "title": "13  lasso回归",
    "section": "",
    "text": "13.1 安装\ninstall.packages(\"glmnet\")",
    "crumbs": [
      "代码实战",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>lasso回归</span>"
    ]
  },
  {
    "objectID": "glmnet.html#建模",
    "href": "glmnet.html#建模",
    "title": "13  lasso回归",
    "section": "13.2 建模",
    "text": "13.2 建模\n\nlibrary(glmnet)\n## Warning: package 'glmnet' was built under R version 4.3.3\n## Loading required package: Matrix\n## Loaded glmnet 4.1-8\n\n用一个二分类数据进行演示，因为大家最常用的就是二分类数据和生存数据了。\n\ndata(BinomialExample)\nx &lt;- BinomialExample$x\ny &lt;- BinomialExample$y\n\ndim(x)\n## [1] 100  30\nclass(x)\n## [1] \"matrix\" \"array\"\nx[1:4,1:4]\n##            [,1]       [,2]       [,3]       [,4]\n## [1,] -0.6192614 0.01624409 -0.6260683  0.4126846\n## [2,]  1.0942728 0.47257285 -1.3371470 -0.6405813\n## [3,] -0.3567040 0.30121334  0.1905619  0.2340268\n## [4,] -2.4690701 2.84771447  1.6602435  1.5688130\n\nclass(y)\n## [1] \"integer\"\nhead(y)\n## [1] 0 1 1 0 1 0\n\n注意glmnet需要的自变量格式，需要是matrix或者稀疏矩阵格式！\nfamily用来指定不同的模型类型，对于二分类数据，应该选择binomial。\nfamily的其他选项如下：“gaussian”（默认）, “poisson”, “multinomial”, “cox”, “mgaussian”。\n建立模型就是1句代码，非常简单：\n\nfit &lt;- glmnet(x, y, family = \"binomial\")\n\n官方不建议直接提取fit中的元素，因为提供了plot，print，coef，predict方法帮助大家探索结果。",
    "crumbs": [
      "代码实战",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>lasso回归</span>"
    ]
  },
  {
    "objectID": "glmnet.html#可视化",
    "href": "glmnet.html#可视化",
    "title": "13  lasso回归",
    "section": "13.3 可视化",
    "text": "13.3 可视化\n可视化各个变量系数的变化，这个图是大家最常见的图形之一：\n\nplot(fit,label = T)\n\n\n\n\n\n\n\n\n这个图形中的每一条线都代表1个变量，并且展示了在不同的L1范数（L1 Norm）下该变量的系数变化。这个图下面的横坐标是L1范数，上面的横坐标是L1范数下对应的非零系数的个数，比如当L1范数是20时，对应的非零系数有27个，也就是此时可以有27个变量保留下来。左侧纵坐标是变量的系数值。\n这里的plot()函数还有一个xvar参数，可以用于指定不同的横坐标：\n\nnorm：横坐标是L1 norm，这个是默认值；\nlambda：横坐标是log-lambda；\ndev：横坐标是模型解释的%deviance\n\n\nplot(fit, xvar = \"lambda\")\n\n\n\n\n\n\n\n\n这里的横坐标是log-lambda，可以看做是正则化程度。\n上面这幅图展示了随着lambda值的变化，每个变量系数的变化，可以看到随着lambda值变大，系数值逐渐减小，直至为0，上面的横坐标也显示随着lambda值变大，保留的变量数量也越来越少。\n\nplot(fit, xvar = \"dev\", label = TRUE)\n\n\n\n\n\n\n\n\n这幅图和上面图的解释是一样的，只有下面的横坐标不一样。\n最后一幅图下面的横坐标是模型解释的偏差百分比，也可以用来衡量模型复杂度。可以看出在图形的右侧部分，模型能够解释的偏差百分比基本变化不大，但是模型系数基本都是往上或往下“飘”的很厉害。\n虽然官方不建议提取数据，但是很明显大家都喜欢提取数据再自己美化图片，我之前也介绍过一种简便方法，可以实现自定义美化图形：lasso回归结果美化",
    "crumbs": [
      "代码实战",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>lasso回归</span>"
    ]
  },
  {
    "objectID": "glmnet.html#打印结果",
    "href": "glmnet.html#打印结果",
    "title": "13  lasso回归",
    "section": "13.4 打印结果",
    "text": "13.4 打印结果\n使用print(fit)可以查看不同lambda值对应的自由度和模型能够解释的偏差百分比：\n\nprint(fit) # 直接fit也可\n## \n## Call:  glmnet(x = x, y = y, family = \"binomial\") \n## \n##    Df  %Dev   Lambda\n## 1   0  0.00 0.240500\n## 2   1  2.90 0.219100\n## 3   1  5.34 0.199600\n## 4   2  8.86 0.181900\n## 5   2 11.95 0.165800\n## 6   2 14.59 0.151000\n## 7   2 16.88 0.137600\n## 8   3 18.95 0.125400\n## 9   7 22.38 0.114200\n## 10  8 26.26 0.104100\n## 11  8 29.73 0.094850\n## 12  8 32.77 0.086420\n## 13  9 35.58 0.078750\n## 14 11 38.98 0.071750\n## 15 12 42.23 0.065380\n## 16 12 45.29 0.059570\n## 17 13 48.09 0.054280\n## 18 13 50.63 0.049450\n## 19 14 53.00 0.045060\n## 20 14 55.19 0.041060\n## 21 15 57.33 0.037410\n## 22 15 59.43 0.034090\n## 23 16 61.36 0.031060\n## 24 17 63.15 0.028300\n## 25 17 64.85 0.025790\n## 26 18 66.42 0.023490\n## 27 19 67.98 0.021410\n## 28 20 69.44 0.019510\n## 29 20 70.80 0.017770\n## 30 21 72.10 0.016190\n## 31 21 73.33 0.014760\n## 32 23 74.52 0.013440\n## 33 23 75.65 0.012250\n## 34 24 76.72 0.011160\n## 35 24 77.77 0.010170\n## 36 25 78.77 0.009267\n## 37 25 79.73 0.008444\n## 38 26 80.66 0.007693\n## 39 26 81.57 0.007010\n## 40 27 82.48 0.006387\n## 41 27 83.39 0.005820\n## 42 27 84.30 0.005303\n## 43 27 85.21 0.004832\n## 44 27 86.12 0.004402\n## 45 27 87.05 0.004011\n## 46 28 87.96 0.003655\n## 47 28 88.87 0.003330\n## 48 28 89.76 0.003034\n## 49 28 90.61 0.002765\n## 50 28 91.41 0.002519\n## 51 28 92.16 0.002295\n## 52 28 92.86 0.002092\n## 53 28 93.50 0.001906\n## 54 28 94.08 0.001736\n## 55 29 94.61 0.001582\n## 56 29 95.10 0.001442\n## 57 29 95.54 0.001314\n## 58 29 95.95 0.001197\n## 59 29 96.31 0.001091\n## 60 29 96.64 0.000994\n## 61 29 96.94 0.000905\n## 62 29 97.22 0.000825\n## 63 29 97.47 0.000752\n## 64 29 97.69 0.000685\n## 65 29 97.90 0.000624\n## 66 29 98.09 0.000569\n## 67 29 98.26 0.000518\n## 68 29 98.41 0.000472\n## 69 29 98.55 0.000430\n## 70 29 98.68 0.000392\n## 71 29 98.80 0.000357\n## 72 30 98.91 0.000325\n## 73 30 99.00 0.000296\n## 74 30 99.09 0.000270\n## 75 30 99.17 0.000246\n## 76 30 99.25 0.000224\n## 77 30 99.31 0.000204\n## 78 30 99.37 0.000186\n## 79 30 99.43 0.000170\n## 80 30 99.48 0.000155\n## 81 30 99.52 0.000141\n## 82 30 99.57 0.000128\n## 83 30 99.61 0.000117\n## 84 30 99.64 0.000106\n## 85 30 99.67 0.000097\n## 86 30 99.70 0.000088\n## 87 30 99.73 0.000081\n## 88 30 99.75 0.000073\n## 89 30 99.77 0.000067\n## 90 30 99.79 0.000061\n## 91 30 99.81 0.000056\n## 92 30 99.83 0.000051\n## 93 30 99.84 0.000046\n## 94 30 99.86 0.000042\n## 95 30 99.87 0.000038\n## 96 30 99.88 0.000035\n## 97 30 99.89 0.000032\n## 98 30 99.90 0.000029\n\n左侧的df是非零系数的个数，中间的%Dev是模型解释的偏差百分比，右侧的Lambda是总惩罚值大小。\n默认情况下，glmnet()函数中的nlambda参数的取值是100，也就是会取100个不同的Lambda值，但是如果%Dev变化不大或者不再变化，它可能会提前停止，取不到100个值，比如我们这个例子就是这样。",
    "crumbs": [
      "代码实战",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>lasso回归</span>"
    ]
  },
  {
    "objectID": "glmnet.html#查看变量系数",
    "href": "glmnet.html#查看变量系数",
    "title": "13  lasso回归",
    "section": "13.5 查看变量系数",
    "text": "13.5 查看变量系数\n我们可以通过coef()查看某个Lambda值下的变量系数：\n\ncoef(fit, s = 0.065380)\n## 31 x 1 sparse Matrix of class \"dgCMatrix\"\n##                       s1\n## (Intercept)  0.210158382\n## V1           .          \n## V2           0.193006823\n## V3          -0.069820214\n## V4          -0.606741531\n## V5          -0.081962193\n## V6          -0.285761723\n## V7           .          \n## V8          -0.165879158\n## V9           0.092678665\n## V10         -0.595865115\n## V11          .          \n## V12          .          \n## V13          .          \n## V14          .          \n## V15          .          \n## V16          .          \n## V17          .          \n## V18          .          \n## V19          .          \n## V20          .          \n## V21          .          \n## V22          0.054956208\n## V23          0.001474751\n## V24          .          \n## V25          0.187112112\n## V26         -0.113782733\n## V27          .          \n## V28          .          \n## V29          .          \n## V30          .\n\n可以看到此时一共有12个变量的系数不是0，和上面print(fit)的结果是一样的。\n这里使用了s表示lambda，为什么不直接用lambda呢？这是作者为了以后的某些功能做准备（截止到20240428也没见到这些功能），但是这一点在tidymodels中大受诟病…\n也可以同时指定多个lambda值：\n\ncoef(fit, s = c(0.065380,0.078750))\n## 31 x 2 sparse Matrix of class \"dgCMatrix\"\n##                       s1          s2\n## (Intercept)  0.210158382  0.22467551\n## V1           .            .         \n## V2           0.193006823  0.13578915\n## V3          -0.069820214  .         \n## V4          -0.606741531 -0.55088786\n## V5          -0.081962193 -0.08588769\n## V6          -0.285761723 -0.18303729\n## V7           .            .         \n## V8          -0.165879158 -0.12710236\n## V9           0.092678665  .         \n## V10         -0.595865115 -0.50054790\n## V11          .            .         \n## V12          .            .         \n## V13          .            .         \n## V14          .            .         \n## V15          .            .         \n## V16          .            .         \n## V17          .            .         \n## V18          .            .         \n## V19          .            .         \n## V20          .            .         \n## V21          .            .         \n## V22          0.054956208  0.01466017\n## V23          0.001474751  .         \n## V24          .            .         \n## V25          0.187112112  0.13534486\n## V26         -0.113782733 -0.08255906\n## V27          .            .         \n## V28          .            .         \n## V29          .            .         \n## V30          .            .\n\n除此之外，coef()还有一个exact参数，如果exact = TRUE，那么当一个lambda不在默认的lambda值中时，函数会重新使用这个lambda值拟合模型然后给出结果，如果exact = FALSE（默认值），那么会使用线性插值给出结果。\n举个例子，0.08并不在lambda值向量中：\n\n# 可以看前面的print(fit)的结果，看看lambda的取值有哪些\nany(fit$lambda == 0.08)\n## [1] FALSE\n\n此时两种情况下的系数是不太一样的：\n\ncoef.apprx &lt;- coef(fit, s = 0.08, exact = FALSE)\ncoef.exact &lt;- coef(fit, s = 0.08, exact = TRUE, x=x, y=y)\ncbind2(coef.exact[which(coef.exact != 0)], \n       coef.apprx[which(coef.apprx != 0)])\n##              [,1]        [,2]\n##  [1,]  0.22549572  0.22541853\n##  [2,]  0.13138628  0.13159475\n##  [3,] -0.54737500 -0.54723674\n##  [4,] -0.08464614 -0.08430109\n##  [5,] -0.17544453 -0.17586695\n##  [6,] -0.12334038 -0.12323991\n##  [7,] -0.49261301 -0.49314684\n##  [8,]  0.01036968  0.01227180\n##  [9,]  0.13183895  0.13169100\n## [10,] -0.07909589 -0.07914430\n\n注意在使用exact = TRUE时，需要提供x和y，因为需要重新拟合模型。",
    "crumbs": [
      "代码实战",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>lasso回归</span>"
    ]
  },
  {
    "objectID": "glmnet.html#预测新数据",
    "href": "glmnet.html#预测新数据",
    "title": "13  lasso回归",
    "section": "13.6 预测新数据",
    "text": "13.6 预测新数据\n对于新数据，可直接使用predict()进行预测，此时也是可以指定lambda值的：\n\nnx &lt;- head(x) #随便准备的新的测试数据\n\npredict(fit, newx = nx, s = c(0.065380,0.078750))\n##              s1         s2\n## [1,] -0.7609757 -0.5755105\n## [2,]  1.4563904  1.1266031\n## [3,]  0.4415409  0.3981256\n## [4,] -1.1676684 -0.9923334\n## [5,]  0.5730604  0.5612494\n## [6,]  0.3064590  0.1926588\n\n由于glmnet包可以用于线性回归、逻辑回归、cox回归、泊松回归、多项式回归等（通过参数family指定即可，默认值是gaussian，可通过?glmnet查看帮助文档），所以在predict()时，type参数略有不同，对于逻辑回归，type可以是以下3种：\n\nlink：线性预测值，默认是这个\nresponse：预测概率\nclass：预测类别\n\n如果要获得预测概率：\n\npredict(fit, newx = nx, s = c(0.065380,0.078750), type = \"response\")\n##             s1        s2\n## [1,] 0.3184345 0.3599663\n## [2,] 0.8109800 0.7552115\n## [3,] 0.6086261 0.5982372\n## [4,] 0.2372767 0.2704514\n## [5,] 0.6394690 0.6367416\n## [6,] 0.5760207 0.5480163\n\n可以通过?predict.glmnet查看帮助文档。",
    "crumbs": [
      "代码实战",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>lasso回归</span>"
    ]
  },
  {
    "objectID": "glmnet.html#交叉验证",
    "href": "glmnet.html#交叉验证",
    "title": "13  lasso回归",
    "section": "13.7 交叉验证",
    "text": "13.7 交叉验证\nglmnet()函数会返回多个模型（因为会使用多个lambda值，这个过程其实就是超参数调优的过程），但是很多情况下，用户并不知道到底选择哪一个lambda值，即不知道到底保留哪些变量，或者希望函数能自动给出结果。\n所以glmnet包提供了交叉验证法，帮助用户做出选择，使用方法也非常简单：\n\ncvfit &lt;- cv.glmnet(x, y)\n\n除了glmnet()中的参数之外，cv.glmnet()还有一些独有的参数：\n\nnfolds：交叉验证的折数，默认是10折交叉验证；\nfoldid：指定哪个观测在哪一折中，一般用不到；\ntype.measure：模型性能指标，对于不同的family，也是略有不同，可查看帮助文档\n\n对于逻辑回归，type.measure可以是以下取值：\n\nmse：均方误差；\ndeviance：偏差；\nmae：平均绝对误差，mean absolute error；\nclass：错分率；\nauc：只能用于二分类逻辑回归\n\n\n13.7.1 plot方法\n对于cv.glmnet()的结果，也提供了plot，print，coef，predict方法。\n\nplot(cvfit)\n\n\n\n\n\n\n\n\n该图形下面的横坐标是log10(lambda)，上面的横坐标是非零系数的数量，左侧的纵坐标是MSE（均方误差），改图展示了不同lambda取值下MSE的变化以及MSE±1倍标准差的置信区间。\n图中的两条竖线就是函数帮你挑选的两个结果，一个是lambda.min，此时的lambda值可以使得MSE最小，另外一个是lambda.1se，此时的lambda值可以使得MSE在最小MSE的1倍标准误区间内，但是同时可以使模型的复杂度降低。（在模型误差之间的差距不是很大的时候，我们肯定是喜欢更简单的模型啦，这个不难理解吧？）\n查看这两个lambda值：\n\ncvfit$lambda.min\n## [1] 0.02578548\ncvfit$lambda.1se\n## [1] 0.05427596\n\n换一个type.measure试试看：\n\ncvfit1 &lt;- cv.glmnet(x, y, family = \"binomial\", type.measure = \"auc\")\nplot(cvfit1)\n\n\n\n\n\n\n\n\n这个图的解读和上面那个图的解读也是一样的，只不过左侧纵坐标不一样而已。\n交叉验证的图形也是可以自己美化的，参考推文：lasso回归结果美化\n\n\n13.7.2 coef方法\n查看这两个取值下保留的非零系数情况：\n\n# 此时s不能同时使用多个值\ncoef(cvfit, s = \"lambda.min\")\n## 31 x 1 sparse Matrix of class \"dgCMatrix\"\n##                        s1\n## (Intercept)  0.5373184556\n## V1           .           \n## V2           0.0559536686\n## V3          -0.0458094357\n## V4          -0.1480322885\n## V5          -0.0236452249\n## V6          -0.1071385139\n## V7           .           \n## V8          -0.0595271041\n## V9           0.0769279945\n## V10         -0.1495936951\n## V11         -0.0112943091\n## V12         -0.0011493679\n## V13         -0.0050063723\n## V14          .           \n## V15          .           \n## V16          0.0171557239\n## V17         -0.0011634211\n## V18          .           \n## V19          .           \n## V20          .           \n## V21          .           \n## V22          0.0334634499\n## V23          0.0374468815\n## V24         -0.0010914314\n## V25          0.0720698621\n## V26         -0.0523560751\n## V27          .           \n## V28          0.0239733147\n## V29         -0.0231471189\n## V30          0.0003738725\ncoef(cvfit, s = \"lambda.1se\") # 这个是默认值\n## 31 x 1 sparse Matrix of class \"dgCMatrix\"\n##                       s1\n## (Intercept)  0.542143513\n## V1           .          \n## V2           0.043727971\n## V3          -0.022970497\n## V4          -0.130364268\n## V5          -0.019367732\n## V6          -0.073722441\n## V7           .          \n## V8          -0.041158945\n## V9           0.036334549\n## V10         -0.129015485\n## V11          .          \n## V12          .          \n## V13          .          \n## V14          .          \n## V15          .          \n## V16          .          \n## V17          .          \n## V18          .          \n## V19          .          \n## V20          .          \n## V21          .          \n## V22          0.019758902\n## V23          0.010973260\n## V24          .          \n## V25          0.048527124\n## V26         -0.033946925\n## V27          .          \n## V28          .          \n## V29         -0.002931655\n## V30          .\n\n可以看到coef()的结果都是稀疏矩阵格式，这种格式计算效率更高，但是不方便后续使用，可以使用as.matrix()转换为矩阵格式：\n\nas.matrix(coef(cvfit))\n##                       s1\n## (Intercept)  0.542143513\n## V1           0.000000000\n## V2           0.043727971\n## V3          -0.022970497\n## V4          -0.130364268\n## V5          -0.019367732\n## V6          -0.073722441\n## V7           0.000000000\n## V8          -0.041158945\n## V9           0.036334549\n## V10         -0.129015485\n## V11          0.000000000\n## V12          0.000000000\n## V13          0.000000000\n## V14          0.000000000\n## V15          0.000000000\n## V16          0.000000000\n## V17          0.000000000\n## V18          0.000000000\n## V19          0.000000000\n## V20          0.000000000\n## V21          0.000000000\n## V22          0.019758902\n## V23          0.010973260\n## V24          0.000000000\n## V25          0.048527124\n## V26         -0.033946925\n## V27          0.000000000\n## V28          0.000000000\n## V29         -0.002931655\n## V30          0.000000000\n\n\n\n13.7.3 predict方法\n对新数据进行预测也是一样的用法：\n\npredict(cvfit, newx = x[1:5,], s = \"lambda.min\")\n##      lambda.min\n## [1,]  0.2831792\n## [2,]  0.9497722\n## [3,]  0.6203134\n## [4,]  0.1532634\n## [5,]  0.5932432",
    "crumbs": [
      "代码实战",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>lasso回归</span>"
    ]
  },
  {
    "objectID": "glmnet.html#一些参数解释",
    "href": "glmnet.html#一些参数解释",
    "title": "13  lasso回归",
    "section": "13.8 一些参数解释",
    "text": "13.8 一些参数解释\n\nalpha：可以看做是L1正则化的比例，当alpha=1时，就是lasso，当alpha=0时，就是岭回归，当0&lt;alpha&lt;1时，就是弹性网络。\nweights：不同观测的权重，默认都是1。（glmnet会自动对权重进行重新标准化，使得所有观测的权重相加等于样本数量）。\nnlambda：lambda的取值个数，默认是100。\nlambda：用户可以通过这个参数自己指定lambda的取值。\nstandardize：逻辑值，是否在拟合模型前对自变量进行标准化，默认是TRUE。\n\n下面是一个对不同观测自定义权重的示例。\n我们这个示例中，样本量是100，所以我们为100个观测自定义以下权重：\n\n# 简单定义一下，前50个是1，后50个是2\nwts &lt;-  c(rep(1,50), rep(2,50))\nfit1 &lt;- glmnet(x, y, alpha = 0.2, weights = wts, nlambda = 20)\n\nprint(fit1)\n## \n## Call:  glmnet(x = x, y = y, weights = wts, alpha = 0.2, nlambda = 20) \n## \n##    Df  %Dev  Lambda\n## 1   0  0.00 1.18600\n## 2   2 11.40 0.73050\n## 3  10 31.21 0.44990\n## 4  11 48.89 0.27710\n## 5  15 59.86 0.17060\n## 6  21 66.72 0.10510\n## 7  26 71.32 0.06471\n## 8  28 73.71 0.03985\n## 9  29 74.84 0.02454\n## 10 29 75.37 0.01512\n## 11 29 75.58 0.00931\n## 12 29 75.66 0.00573\n## 13 30 75.70 0.00353\n## 14 30 75.71 0.00217\n## 15 30 75.72 0.00134\n## 16 30 75.72 0.00082\n## 17 30 75.72 0.00051\n\n可以看到结果中只有17个lambda值，少于我们指定的20个，原因已经在前面解释过了。",
    "crumbs": [
      "代码实战",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>lasso回归</span>"
    ]
  },
  {
    "objectID": "glmnet.html#在测试集评估模型",
    "href": "glmnet.html#在测试集评估模型",
    "title": "13  lasso回归",
    "section": "13.9 在测试集评估模型",
    "text": "13.9 在测试集评估模型\n模型建立后，我们可能会使用测试集检测模型性能，glmnet包为我们提供了assess.glmnet，roc.glmnet，confusion.glmnet，帮助我们快速在衡量模型性能。\n\n13.9.1 assess.glmnet()\n还是使用这个二分类数据，我们把前70个观测作为训练集，用来建模，后30个观测作为测试集。\n\ndata(BinomialExample)\nx &lt;- BinomialExample$x\ny &lt;- BinomialExample$y\nitrain &lt;- 1:70 # 前70个作为训练集\nfit &lt;- glmnet(x[itrain, ], y[itrain], family = \"binomial\", nlambda = 6)\n\n# 在测试集评估模型\nassess.glmnet(fit, newx = x[-itrain, ], newy = y[-itrain])\n## $deviance\n##        s0        s1        s2        s3        s4        s5 \n## 1.3877348 0.8547044 1.2031017 2.3732041 3.1831559 3.7565310 \n## attr(,\"measure\")\n## [1] \"Binomial Deviance\"\n## \n## $class\n##        s0        s1        s2        s3        s4        s5 \n## 0.4666667 0.1666667 0.2000000 0.2000000 0.1666667 0.1666667 \n## attr(,\"measure\")\n## [1] \"Misclassification Error\"\n## \n## $auc\n## [1] 0.5000000 0.8973214 0.8794643 0.8214286 0.8169643 0.8303571\n## attr(,\"measure\")\n## [1] \"AUC\"\n## \n## $mse\n##        s0        s1        s2        s3        s4        s5 \n## 0.5006803 0.2620161 0.3157726 0.3570313 0.3500126 0.3482634 \n## attr(,\"measure\")\n## [1] \"Mean-Squared Error\"\n## \n## $mae\n##        s0        s1        s2        s3        s4        s5 \n## 0.9904762 0.5650890 0.4609257 0.4227314 0.3865725 0.3745569 \n## attr(,\"measure\")\n## [1] \"Mean Absolute Error\"\n\n结果是一个列表，里面是family = \"binomial\"时，5个性能指标在不同lambda值下的结果，由于我们这里指定了只使用6个lambda值，所以结果就是6个，你指定几个，结果就会有几个。\n不同的family对应着不同的性能指标，可以通过glmnet.measures()查看每个family对应的性能指标：\n\nglmnet.measures()\n## $gaussian\n## [1] \"mse\" \"mae\"\n## \n## $binomial\n## [1] \"deviance\" \"class\"    \"auc\"      \"mse\"      \"mae\"     \n## \n## $poisson\n## [1] \"deviance\" \"mse\"      \"mae\"     \n## \n## $cox\n## [1] \"deviance\" \"C\"       \n## \n## $multinomial\n## [1] \"deviance\" \"class\"    \"mse\"      \"mae\"     \n## \n## $mgaussian\n## [1] \"mse\" \"mae\"\n## \n## $GLM\n## [1] \"deviance\" \"mse\"      \"mae\"\n\n交叉验证同样也是适用的：\n\ncfit &lt;- cv.glmnet(x[itrain, ], y[itrain], family = \"binomial\", nlambda = 30)\nassess.glmnet(cfit, newx = x[-itrain, ], newy = y[-itrain])\n## $deviance\n## lambda.1se \n##   1.062957 \n## attr(,\"measure\")\n## [1] \"Binomial Deviance\"\n## \n## $class\n## lambda.1se \n##  0.2333333 \n## attr(,\"measure\")\n## [1] \"Misclassification Error\"\n## \n## $auc\n## [1] 0.8392857\n## attr(,\"measure\")\n## [1] \"AUC\"\n## \n## $mse\n## lambda.1se \n##  0.3509219 \n## attr(,\"measure\")\n## [1] \"Mean-Squared Error\"\n## \n## $mae\n## lambda.1se \n##  0.7730458 \n## attr(,\"measure\")\n## [1] \"Mean Absolute Error\"\n\n不过此时默认使用的lambda值是lambda.1se，也可以使用lambda.min：\n\nassess.glmnet(cfit, newx = x[-itrain, ],newy = y[-itrain], s = \"lambda.min\")\n## $deviance\n## lambda.min \n##  0.8561849 \n## attr(,\"measure\")\n## [1] \"Binomial Deviance\"\n## \n## $class\n## lambda.min \n##  0.1666667 \n## attr(,\"measure\")\n## [1] \"Misclassification Error\"\n## \n## $auc\n## [1] 0.9017857\n## attr(,\"measure\")\n## [1] \"AUC\"\n## \n## $mse\n## lambda.min \n##  0.2612625 \n## attr(,\"measure\")\n## [1] \"Mean-Squared Error\"\n## \n## $mae\n## lambda.min \n##  0.5574297 \n## attr(,\"measure\")\n## [1] \"Mean Absolute Error\"\n\n当然也可以获取训练集的各种指标，只要在建模时使用keep=TRUE参数即可：\n\ncfit &lt;- cv.glmnet(x, y, family = \"binomial\", keep = TRUE, nlambda = 3)\nassess.glmnet(cfit$fit.preval, newy = y, family = \"binomial\")\n## $deviance\n##       s0       s1       s2 \n## 1.343650 1.851108 3.491101 \n## attr(,\"measure\")\n## [1] \"Binomial Deviance\"\n## \n## $class\n##   s0   s1   s2 \n## 0.41 0.19 0.22 \n## attr(,\"measure\")\n## [1] \"Misclassification Error\"\n## \n## $auc\n## [1] 0.5229302 0.8652597 0.8612013\n## attr(,\"measure\")\n## [1] \"AUC\"\n## \n## $mse\n##        s0        s1        s2 \n## 0.4790506 0.3183461 0.4083493 \n## attr(,\"measure\")\n## [1] \"Mean-Squared Error\"\n## \n## $mae\n##        s0        s1        s2 \n## 0.9610673 0.4023389 0.4356022 \n## attr(,\"measure\")\n## [1] \"Mean Absolute Error\"\n\n\n\n13.9.2 roc.glmnet()\n对于二分类数据，ROC曲线是非常重要的模型衡量工具。\nroc.glmnet()可以快速计算出画ROC曲线需要的数据，然后使用plot()画图即可。\n\nfit &lt;- glmnet(x[itrain,], y[itrain], family = \"binomial\")\n\nrocs &lt;- roc.glmnet(fit, newx = x[-itrain,], newy=y[-itrain])\n\n这个rocs是一个列表，其长度就是lambda值的数量，对于每一个lambda取值，它都计算了可以用来画ROC曲线的数据。\n我们随便取其中一个画出来：\n\nplot(rocs[[3]],type = \"l\",xlim=c(0,1),ylim=c(0,1))\ninvisible(sapply(rocs, lines)) # 把所有的ROC都画出来\nabline(0,1,col=\"grey\")\n\n\n\n\n\n\n\n\n交叉验证的结果当然也是可以的：\n\n# 建立模型\ncfit &lt;- cv.glmnet(x, y, family = \"binomial\", type.measure = \"auc\", \n                  keep = TRUE)\n\n# 计算画ROC曲线需要的数据\nrocs &lt;- roc.glmnet(cfit$fit.preval, newy = y)\n\nclass(rocs)\n## [1] \"list\"\nlength(rocs)\n## [1] 98\ndim(rocs[[1]])\n## [1] 33  2\nhead(rocs[[1]])\n##                          FPR        TPR\n## 0.602820476452258 0.00000000 0.01785714\n## 0.485855667892452 0.02272727 0.01785714\n## 0.485121378823122 0.02272727 0.03571429\n## 0.410184620811708 0.02272727 0.05357143\n## 0.363185555479677 0.02272727 0.07142857\n## 0.363142299612302 0.02272727 0.08928571\n\n这个rocs也是一个列表，其长度就是lambda值的数量，对于每一个lambda取值，它都计算了可以用来画ROC曲线的数据。\n下面我们把AUC最大的ROC曲线画出来，用红色标记，并把其他ROC曲线也画在一起：\n\nbest &lt;- cvfit$index[\"min\",] # 提取AUC最大的lambda值\nplot(rocs[[best]], type = \"l\") # 画出AUC最大的ROC曲线\ninvisible(sapply(rocs, lines, col=\"grey\")) # 把所有的ROC都画出来\nlines(rocs[[best]], lwd = 2,col = \"red\") # 把AUC最大的标红\n\n\n\n\n\n\n\n\n\n\n13.9.3 confusion.glmnet()\n混淆矩阵作为分类数据必不可少的工具，可以通过confusion.glmnet()实现。\n用一个多分类数据进行演示。\n\ndata(MultinomialExample)\nx &lt;- MultinomialExample$x\ny &lt;- MultinomialExample$y\nset.seed(101)\nitrain &lt;- sample(1:500, 400, replace = FALSE)\ncfit &lt;- cv.glmnet(x[itrain, ], y[itrain], family = \"multinomial\")\n\n# 默认lambda值是lambda.1se\ncnf &lt;- confusion.glmnet(cfit, newx = x[-itrain, ], newy = y[-itrain]) \n\nprint(cnf)\n##          True\n## Predicted  1  2  3 Total\n##     1     13  6  4    23\n##     2      7 25  5    37\n##     3      4  3 33    40\n##     Total 24 34 42   100\n## \n##  Percent Correct:  0.71\n\n如果使用keep=TRUE，那么结果也是多个混淆矩阵，此时也可以选择任意一个进行展示：\n\ncfit &lt;- cv.glmnet(x, y, family = \"multinomial\", type = \"class\", keep = TRUE)\ncnf &lt;- confusion.glmnet(cfit$fit.preval, newy = y, family = \"multinomial\")\nbest &lt;- cfit$index[\"min\",]\nprint(cnf[[best]])\n##          True\n## Predicted   1   2   3 Total\n##     1      76  22  14   112\n##     2      39 129  23   191\n##     3      27  23 147   197\n##     Total 142 174 184   500\n## \n##  Percent Correct:  0.704\n\n虽然glmnet包提供了这3个函数帮助我们查看模型性能，但是很明显不能满足大家的需求，所以一般情况下我们都用其他的R包来代替这几个函数了，比如caret，yardstick，pROC等。",
    "crumbs": [
      "代码实战",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>lasso回归</span>"
    ]
  },
  {
    "objectID": "glmnet.html#其他功能",
    "href": "glmnet.html#其他功能",
    "title": "13  lasso回归",
    "section": "13.10 其他功能",
    "text": "13.10 其他功能\n\n13.10.1 拟合非正则化的广义线性模型\nglmnet包提供了bigGlm()函数，可以对大型数据拟合非正则化的广义线性模型，类似于常规的glm()，但是支持glmnet中的所有参数。其实此时的lambda=0，也就是不进行正则化。如果你的数据巨大，使用glm很慢，或者你需要其他参数，可以尝试一下bigGlm()。\n以下是一个使用示例：\n\ndata(BinomialExample)\nx &lt;- BinomialExample$x\ny &lt;- BinomialExample$y\nfit &lt;- bigGlm(x, y, family = \"binomial\", lower.limits = -1)\nprint(fit)\n## \n## Call:  bigGlm(x = x, y = y, family = \"binomial\", lower.limits = -1) \n## \n##   Df  %Dev Lambda\n## 1 30 77.57      0\n\n\n\n13.10.2 修改自变量矩阵格式\nglmnet包提供了一个makeX()函数，可以对自变量的格式进行修改，比如，如果你提供了1个数据框data.frame，这个格式是不行的，它可以帮你转换为matrix格式，除此之外，还可以进行如下操作：\n\n对因子型或字符型变量进行独热编码；\n使用均值填补缺失值；\n可以直接变为稀疏矩阵，适合大数据；\n可以直接提供训练集和测试集两个数据集，这样可以保证两个数据集的因子水平对应，以及使用训练集中的均值对测试集进行插补\n\n先展示下独热编码转换功能，我们建立一个数据框，其中两列是字符型，makeX可以帮我们进行独热编码，并把数据变为稀疏矩阵格式：\n\nset.seed(101)\nX &lt;- matrix(rnorm(5), nrow = 5)\nX2 &lt;- sample(letters[1:3], 5, replace = TRUE)\nX3 &lt;- sample(LETTERS[1:3], 5, replace = TRUE)\ndf &lt;- data.frame(X, X2, X3)\ndf\n##            X X2 X3\n## 1 -0.3260365  c  C\n## 2  0.5524619  b  C\n## 3 -0.6749438  c  B\n## 4  0.2143595  c  C\n## 5  0.3107692  a  C\nmakeX(df) # 转换\n##            X X2a X2b X2c X3B X3C\n## 1 -0.3260365   0   0   1   0   1\n## 2  0.5524619   0   1   0   0   1\n## 3 -0.6749438   0   0   1   1   0\n## 4  0.2143595   0   0   1   0   1\n## 5  0.3107692   1   0   0   0   1\n\n添加sparse=T可以返回稀疏矩阵格式：\n\nmakeX(df, sparse = TRUE)\n## 5 x 6 sparse Matrix of class \"dgCMatrix\"\n##            X X2a X2b X2c X3B X3C\n## 1 -0.3260365   .   .   1   .   1\n## 2  0.5524619   .   1   .   .   1\n## 3 -0.6749438   .   .   1   1   .\n## 4  0.2143595   .   .   1   .   1\n## 5  0.3107692   1   .   .   .   1\n\n下面我们对原数据框添加一些缺失值，用来演示makeX的缺失值插补功能：\n\nXn  &lt;- X ; Xn[3,1] &lt;- NA\nX2n &lt;- X2; X2n[1]  &lt;- NA\nX3n &lt;- X3; X3n[5]  &lt;- NA\ndfn &lt;- data.frame(Xn, X2n, X3n)\ndfn\n##           Xn  X2n  X3n\n## 1 -0.3260365 &lt;NA&gt;    C\n## 2  0.5524619    b    C\n## 3         NA    c    B\n## 4  0.2143595    c    C\n## 5  0.3107692    a &lt;NA&gt;\n\n通过添加na.impute=T可以进行插补：\n\nmakeX(dfn,na.impute = T)\n##           Xn X2na X2nb X2nc X3nB X3nC\n## 1 -0.3260365 0.25 0.25  0.5 0.00 1.00\n## 2  0.5524619 0.00 1.00  0.0 0.00 1.00\n## 3  0.1878885 0.00 0.00  1.0 1.00 0.00\n## 4  0.2143595 0.00 0.00  1.0 0.00 1.00\n## 5  0.3107692 1.00 0.00  0.0 0.25 0.75\n## attr(,\"means\")\n##        Xn      X2na      X2nb      X2nc      X3nB      X3nC \n## 0.1878885 0.2500000 0.2500000 0.5000000 0.2500000 0.7500000\n\n这个函数总体来说还是挺方便的。\n\n\n13.10.3 添加进度条\nglmnet()和cv.glmnet()都可以通过添加trace.it=TRUE实现进度条功能：\n\nfit &lt;- glmnet(x, y, trace.it = TRUE)\n## \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |=                                                                     |   1%\n  |                                                                            \n  |=                                                                     |   2%\n  |                                                                            \n  |==                                                                    |   3%\n  |                                                                            \n  |===                                                                   |   4%\n  |                                                                            \n  |====                                                                  |   5%\n  |                                                                            \n  |====                                                                  |   6%\n  |                                                                            \n  |=====                                                                 |   7%\n  |                                                                            \n  |======                                                                |   8%\n  |                                                                            \n  |======                                                                |   9%\n  |                                                                            \n  |=======                                                               |  10%\n  |                                                                            \n  |========                                                              |  11%\n  |                                                                            \n  |========                                                              |  12%\n  |                                                                            \n  |=========                                                             |  13%\n  |                                                                            \n  |==========                                                            |  14%\n  |                                                                            \n  |==========                                                            |  15%\n  |                                                                            \n  |===========                                                           |  16%\n  |                                                                            \n  |============                                                          |  17%\n  |                                                                            \n  |=============                                                         |  18%\n  |                                                                            \n  |=============                                                         |  19%\n  |                                                                            \n  |==============                                                        |  20%\n  |                                                                            \n  |===============                                                       |  21%\n  |                                                                            \n  |===============                                                       |  22%\n  |                                                                            \n  |================                                                      |  23%\n  |                                                                            \n  |=================                                                     |  24%\n  |                                                                            \n  |==================                                                    |  25%\n  |                                                                            \n  |==================                                                    |  26%\n  |                                                                            \n  |===================                                                   |  27%\n  |                                                                            \n  |====================                                                  |  28%\n  |                                                                            \n  |====================                                                  |  29%\n  |                                                                            \n  |=====================                                                 |  30%\n  |                                                                            \n  |======================                                                |  31%\n  |                                                                            \n  |======================                                                |  32%\n  |                                                                            \n  |=======================                                               |  33%\n  |                                                                            \n  |========================                                              |  34%\n  |                                                                            \n  |========================                                              |  35%\n  |                                                                            \n  |=========================                                             |  36%\n  |                                                                            \n  |==========================                                            |  37%\n  |                                                                            \n  |===========================                                           |  38%\n  |                                                                            \n  |===========================                                           |  39%\n  |                                                                            \n  |============================                                          |  40%\n  |                                                                            \n  |=============================                                         |  41%\n  |                                                                            \n  |=============================                                         |  42%\n  |                                                                            \n  |==============================                                        |  43%\n  |                                                                            \n  |===============================                                       |  44%\n  |                                                                            \n  |================================                                      |  45%\n  |                                                                            \n  |================================                                      |  46%\n  |                                                                            \n  |=================================                                     |  47%\n  |                                                                            \n  |==================================                                    |  48%\n  |                                                                            \n  |==================================                                    |  49%\n  |                                                                            \n  |===================================                                   |  50%\n  |                                                                            \n  |====================================                                  |  51%\n  |                                                                            \n  |====================================                                  |  52%\n  |                                                                            \n  |=====================================                                 |  53%\n  |                                                                            \n  |======================================                                |  54%\n  |                                                                            \n  |======================================                                |  55%\n  |                                                                            \n  |=======================================                               |  56%\n  |                                                                            \n  |========================================                              |  57%\n  |                                                                            \n  |=========================================                             |  58%\n  |                                                                            \n  |=========================================                             |  59%\n  |                                                                            \n  |==========================================                            |  60%\n  |                                                                            \n  |===========================================                           |  61%\n  |                                                                            \n  |===========================================                           |  62%\n  |                                                                            \n  |============================================                          |  63%\n  |                                                                            \n  |=============================================                         |  64%\n  |                                                                            \n  |==============================================                        |  65%\n  |                                                                            \n  |==============================================                        |  66%\n  |                                                                            \n  |===============================================                       |  67%\n  |                                                                            \n  |================================================                      |  68%\n  |                                                                            \n  |======================================================================| 100%\n\n\nfit &lt;- cv.glmnet(x, y, trace.it = TRUE)\n## Training\n## \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |=                                                                     |   1%\n  |                                                                            \n  |=                                                                     |   2%\n  |                                                                            \n  |==                                                                    |   3%\n  |                                                                            \n  |===                                                                   |   4%\n  |                                                                            \n  |====                                                                  |   5%\n  |                                                                            \n  |====                                                                  |   6%\n  |                                                                            \n  |=====                                                                 |   7%\n  |                                                                            \n  |======                                                                |   8%\n  |                                                                            \n  |======                                                                |   9%\n  |                                                                            \n  |=======                                                               |  10%\n  |                                                                            \n  |========                                                              |  11%\n  |                                                                            \n  |========                                                              |  12%\n  |                                                                            \n  |=========                                                             |  13%\n  |                                                                            \n  |==========                                                            |  14%\n  |                                                                            \n  |==========                                                            |  15%\n  |                                                                            \n  |===========                                                           |  16%\n  |                                                                            \n  |============                                                          |  17%\n  |                                                                            \n  |=============                                                         |  18%\n  |                                                                            \n  |=============                                                         |  19%\n  |                                                                            \n  |==============                                                        |  20%\n  |                                                                            \n  |===============                                                       |  21%\n  |                                                                            \n  |===============                                                       |  22%\n  |                                                                            \n  |================                                                      |  23%\n  |                                                                            \n  |=================                                                     |  24%\n  |                                                                            \n  |==================                                                    |  25%\n  |                                                                            \n  |==================                                                    |  26%\n  |                                                                            \n  |===================                                                   |  27%\n  |                                                                            \n  |====================                                                  |  28%\n  |                                                                            \n  |====================                                                  |  29%\n  |                                                                            \n  |=====================                                                 |  30%\n  |                                                                            \n  |======================                                                |  31%\n  |                                                                            \n  |======================                                                |  32%\n  |                                                                            \n  |=======================                                               |  33%\n  |                                                                            \n  |========================                                              |  34%\n  |                                                                            \n  |========================                                              |  35%\n  |                                                                            \n  |=========================                                             |  36%\n  |                                                                            \n  |==========================                                            |  37%\n  |                                                                            \n  |===========================                                           |  38%\n  |                                                                            \n  |===========================                                           |  39%\n  |                                                                            \n  |============================                                          |  40%\n  |                                                                            \n  |=============================                                         |  41%\n  |                                                                            \n  |=============================                                         |  42%\n  |                                                                            \n  |==============================                                        |  43%\n  |                                                                            \n  |===============================                                       |  44%\n  |                                                                            \n  |================================                                      |  45%\n  |                                                                            \n  |================================                                      |  46%\n  |                                                                            \n  |=================================                                     |  47%\n  |                                                                            \n  |==================================                                    |  48%\n  |                                                                            \n  |==================================                                    |  49%\n  |                                                                            \n  |===================================                                   |  50%\n  |                                                                            \n  |====================================                                  |  51%\n  |                                                                            \n  |====================================                                  |  52%\n  |                                                                            \n  |=====================================                                 |  53%\n  |                                                                            \n  |======================================                                |  54%\n  |                                                                            \n  |======================================                                |  55%\n  |                                                                            \n  |=======================================                               |  56%\n  |                                                                            \n  |========================================                              |  57%\n  |                                                                            \n  |=========================================                             |  58%\n  |                                                                            \n  |=========================================                             |  59%\n  |                                                                            \n  |==========================================                            |  60%\n  |                                                                            \n  |===========================================                           |  61%\n  |                                                                            \n  |===========================================                           |  62%\n  |                                                                            \n  |============================================                          |  63%\n  |                                                                            \n  |=============================================                         |  64%\n  |                                                                            \n  |==============================================                        |  65%\n  |                                                                            \n  |==============================================                        |  66%\n  |                                                                            \n  |===============================================                       |  67%\n  |                                                                            \n  |================================================                      |  68%\n  |                                                                            \n  |======================================================================| 100%\n## Fold: 1/10\n## \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |=                                                                     |   1%\n  |                                                                            \n  |=                                                                     |   2%\n  |                                                                            \n  |==                                                                    |   3%\n  |                                                                            \n  |===                                                                   |   4%\n  |                                                                            \n  |====                                                                  |   5%\n  |                                                                            \n  |====                                                                  |   6%\n  |                                                                            \n  |=====                                                                 |   7%\n  |                                                                            \n  |======                                                                |   8%\n  |                                                                            \n  |======                                                                |   9%\n  |                                                                            \n  |=======                                                               |  10%\n  |                                                                            \n  |========                                                              |  11%\n  |                                                                            \n  |========                                                              |  12%\n  |                                                                            \n  |=========                                                             |  13%\n  |                                                                            \n  |==========                                                            |  14%\n  |                                                                            \n  |==========                                                            |  15%\n  |                                                                            \n  |===========                                                           |  16%\n  |                                                                            \n  |============                                                          |  17%\n  |                                                                            \n  |=============                                                         |  18%\n  |                                                                            \n  |=============                                                         |  19%\n  |                                                                            \n  |==============                                                        |  20%\n  |                                                                            \n  |===============                                                       |  21%\n  |                                                                            \n  |===============                                                       |  22%\n  |                                                                            \n  |================                                                      |  23%\n  |                                                                            \n  |=================                                                     |  24%\n  |                                                                            \n  |==================                                                    |  25%\n  |                                                                            \n  |==================                                                    |  26%\n  |                                                                            \n  |===================                                                   |  27%\n  |                                                                            \n  |====================                                                  |  28%\n  |                                                                            \n  |====================                                                  |  29%\n  |                                                                            \n  |=====================                                                 |  30%\n  |                                                                            \n  |======================                                                |  31%\n  |                                                                            \n  |======================                                                |  32%\n  |                                                                            \n  |=======================                                               |  33%\n  |                                                                            \n  |========================                                              |  34%\n  |                                                                            \n  |========================                                              |  35%\n  |                                                                            \n  |=========================                                             |  36%\n  |                                                                            \n  |==========================                                            |  37%\n  |                                                                            \n  |===========================                                           |  38%\n  |                                                                            \n  |===========================                                           |  39%\n  |                                                                            \n  |============================                                          |  40%\n  |                                                                            \n  |=============================                                         |  41%\n  |                                                                            \n  |=============================                                         |  42%\n  |                                                                            \n  |==============================                                        |  43%\n  |                                                                            \n  |===============================                                       |  44%\n  |                                                                            \n  |================================                                      |  45%\n  |                                                                            \n  |================================                                      |  46%\n  |                                                                            \n  |=================================                                     |  47%\n  |                                                                            \n  |==================================                                    |  48%\n  |                                                                            \n  |==================================                                    |  49%\n  |                                                                            \n  |===================================                                   |  50%\n  |                                                                            \n  |====================================                                  |  51%\n  |                                                                            \n  |====================================                                  |  52%\n  |                                                                            \n  |=====================================                                 |  53%\n  |                                                                            \n  |======================================                                |  54%\n  |                                                                            \n  |======================================                                |  55%\n  |                                                                            \n  |=======================================                               |  56%\n  |                                                                            \n  |========================================                              |  57%\n  |                                                                            \n  |=========================================                             |  58%\n  |                                                                            \n  |=========================================                             |  59%\n  |                                                                            \n  |==========================================                            |  60%\n  |                                                                            \n  |===========================================                           |  61%\n  |                                                                            \n  |===========================================                           |  62%\n  |                                                                            \n  |============================================                          |  63%\n  |                                                                            \n  |=============================================                         |  64%\n  |                                                                            \n  |==============================================                        |  65%\n  |                                                                            \n  |==============================================                        |  66%\n  |                                                                            \n  |===============================================                       |  67%\n  |                                                                            \n  |================================================                      |  68%\n  |                                                                            \n  |======================================================================| 100%\n## Fold: 2/10\n## \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |=                                                                     |   1%\n  |                                                                            \n  |=                                                                     |   2%\n  |                                                                            \n  |==                                                                    |   3%\n  |                                                                            \n  |===                                                                   |   4%\n  |                                                                            \n  |====                                                                  |   5%\n  |                                                                            \n  |====                                                                  |   6%\n  |                                                                            \n  |=====                                                                 |   7%\n  |                                                                            \n  |======                                                                |   8%\n  |                                                                            \n  |======                                                                |   9%\n  |                                                                            \n  |=======                                                               |  10%\n  |                                                                            \n  |========                                                              |  11%\n  |                                                                            \n  |========                                                              |  12%\n  |                                                                            \n  |=========                                                             |  13%\n  |                                                                            \n  |==========                                                            |  14%\n  |                                                                            \n  |==========                                                            |  15%\n  |                                                                            \n  |===========                                                           |  16%\n  |                                                                            \n  |============                                                          |  17%\n  |                                                                            \n  |=============                                                         |  18%\n  |                                                                            \n  |=============                                                         |  19%\n  |                                                                            \n  |==============                                                        |  20%\n  |                                                                            \n  |===============                                                       |  21%\n  |                                                                            \n  |===============                                                       |  22%\n  |                                                                            \n  |================                                                      |  23%\n  |                                                                            \n  |=================                                                     |  24%\n  |                                                                            \n  |==================                                                    |  25%\n  |                                                                            \n  |==================                                                    |  26%\n  |                                                                            \n  |===================                                                   |  27%\n  |                                                                            \n  |====================                                                  |  28%\n  |                                                                            \n  |====================                                                  |  29%\n  |                                                                            \n  |=====================                                                 |  30%\n  |                                                                            \n  |======================                                                |  31%\n  |                                                                            \n  |======================                                                |  32%\n  |                                                                            \n  |=======================                                               |  33%\n  |                                                                            \n  |========================                                              |  34%\n  |                                                                            \n  |========================                                              |  35%\n  |                                                                            \n  |=========================                                             |  36%\n  |                                                                            \n  |==========================                                            |  37%\n  |                                                                            \n  |===========================                                           |  38%\n  |                                                                            \n  |===========================                                           |  39%\n  |                                                                            \n  |============================                                          |  40%\n  |                                                                            \n  |=============================                                         |  41%\n  |                                                                            \n  |=============================                                         |  42%\n  |                                                                            \n  |==============================                                        |  43%\n  |                                                                            \n  |===============================                                       |  44%\n  |                                                                            \n  |================================                                      |  45%\n  |                                                                            \n  |================================                                      |  46%\n  |                                                                            \n  |=================================                                     |  47%\n  |                                                                            \n  |==================================                                    |  48%\n  |                                                                            \n  |==================================                                    |  49%\n  |                                                                            \n  |===================================                                   |  50%\n  |                                                                            \n  |====================================                                  |  51%\n  |                                                                            \n  |====================================                                  |  52%\n  |                                                                            \n  |=====================================                                 |  53%\n  |                                                                            \n  |======================================                                |  54%\n  |                                                                            \n  |======================================                                |  55%\n  |                                                                            \n  |=======================================                               |  56%\n  |                                                                            \n  |========================================                              |  57%\n  |                                                                            \n  |=========================================                             |  58%\n  |                                                                            \n  |=========================================                             |  59%\n  |                                                                            \n  |==========================================                            |  60%\n  |                                                                            \n  |===========================================                           |  61%\n  |                                                                            \n  |===========================================                           |  62%\n  |                                                                            \n  |============================================                          |  63%\n  |                                                                            \n  |=============================================                         |  64%\n  |                                                                            \n  |==============================================                        |  65%\n  |                                                                            \n  |==============================================                        |  66%\n  |                                                                            \n  |===============================================                       |  67%\n  |                                                                            \n  |================================================                      |  68%\n  |                                                                            \n  |======================================================================| 100%\n## Fold: 3/10\n## \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |=                                                                     |   1%\n  |                                                                            \n  |=                                                                     |   2%\n  |                                                                            \n  |==                                                                    |   3%\n  |                                                                            \n  |===                                                                   |   4%\n  |                                                                            \n  |====                                                                  |   5%\n  |                                                                            \n  |====                                                                  |   6%\n  |                                                                            \n  |=====                                                                 |   7%\n  |                                                                            \n  |======                                                                |   8%\n  |                                                                            \n  |======                                                                |   9%\n  |                                                                            \n  |=======                                                               |  10%\n  |                                                                            \n  |========                                                              |  11%\n  |                                                                            \n  |========                                                              |  12%\n  |                                                                            \n  |=========                                                             |  13%\n  |                                                                            \n  |==========                                                            |  14%\n  |                                                                            \n  |==========                                                            |  15%\n  |                                                                            \n  |===========                                                           |  16%\n  |                                                                            \n  |============                                                          |  17%\n  |                                                                            \n  |=============                                                         |  18%\n  |                                                                            \n  |=============                                                         |  19%\n  |                                                                            \n  |==============                                                        |  20%\n  |                                                                            \n  |===============                                                       |  21%\n  |                                                                            \n  |===============                                                       |  22%\n  |                                                                            \n  |================                                                      |  23%\n  |                                                                            \n  |=================                                                     |  24%\n  |                                                                            \n  |==================                                                    |  25%\n  |                                                                            \n  |==================                                                    |  26%\n  |                                                                            \n  |===================                                                   |  27%\n  |                                                                            \n  |====================                                                  |  28%\n  |                                                                            \n  |====================                                                  |  29%\n  |                                                                            \n  |=====================                                                 |  30%\n  |                                                                            \n  |======================                                                |  31%\n  |                                                                            \n  |======================                                                |  32%\n  |                                                                            \n  |=======================                                               |  33%\n  |                                                                            \n  |========================                                              |  34%\n  |                                                                            \n  |========================                                              |  35%\n  |                                                                            \n  |=========================                                             |  36%\n  |                                                                            \n  |==========================                                            |  37%\n  |                                                                            \n  |===========================                                           |  38%\n  |                                                                            \n  |===========================                                           |  39%\n  |                                                                            \n  |============================                                          |  40%\n  |                                                                            \n  |=============================                                         |  41%\n  |                                                                            \n  |=============================                                         |  42%\n  |                                                                            \n  |==============================                                        |  43%\n  |                                                                            \n  |===============================                                       |  44%\n  |                                                                            \n  |================================                                      |  45%\n  |                                                                            \n  |================================                                      |  46%\n  |                                                                            \n  |=================================                                     |  47%\n  |                                                                            \n  |==================================                                    |  48%\n  |                                                                            \n  |==================================                                    |  49%\n  |                                                                            \n  |===================================                                   |  50%\n  |                                                                            \n  |====================================                                  |  51%\n  |                                                                            \n  |====================================                                  |  52%\n  |                                                                            \n  |=====================================                                 |  53%\n  |                                                                            \n  |======================================                                |  54%\n  |                                                                            \n  |======================================                                |  55%\n  |                                                                            \n  |=======================================                               |  56%\n  |                                                                            \n  |========================================                              |  57%\n  |                                                                            \n  |=========================================                             |  58%\n  |                                                                            \n  |=========================================                             |  59%\n  |                                                                            \n  |==========================================                            |  60%\n  |                                                                            \n  |===========================================                           |  61%\n  |                                                                            \n  |===========================================                           |  62%\n  |                                                                            \n  |============================================                          |  63%\n  |                                                                            \n  |=============================================                         |  64%\n  |                                                                            \n  |==============================================                        |  65%\n  |                                                                            \n  |==============================================                        |  66%\n  |                                                                            \n  |===============================================                       |  67%\n  |                                                                            \n  |================================================                      |  68%\n  |                                                                            \n  |======================================================================| 100%\n## Fold: 4/10\n## \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |=                                                                     |   1%\n  |                                                                            \n  |=                                                                     |   2%\n  |                                                                            \n  |==                                                                    |   3%\n  |                                                                            \n  |===                                                                   |   4%\n  |                                                                            \n  |====                                                                  |   5%\n  |                                                                            \n  |====                                                                  |   6%\n  |                                                                            \n  |=====                                                                 |   7%\n  |                                                                            \n  |======                                                                |   8%\n  |                                                                            \n  |======                                                                |   9%\n  |                                                                            \n  |=======                                                               |  10%\n  |                                                                            \n  |========                                                              |  11%\n  |                                                                            \n  |========                                                              |  12%\n  |                                                                            \n  |=========                                                             |  13%\n  |                                                                            \n  |==========                                                            |  14%\n  |                                                                            \n  |==========                                                            |  15%\n  |                                                                            \n  |===========                                                           |  16%\n  |                                                                            \n  |============                                                          |  17%\n  |                                                                            \n  |=============                                                         |  18%\n  |                                                                            \n  |=============                                                         |  19%\n  |                                                                            \n  |==============                                                        |  20%\n  |                                                                            \n  |===============                                                       |  21%\n  |                                                                            \n  |===============                                                       |  22%\n  |                                                                            \n  |================                                                      |  23%\n  |                                                                            \n  |=================                                                     |  24%\n  |                                                                            \n  |==================                                                    |  25%\n  |                                                                            \n  |==================                                                    |  26%\n  |                                                                            \n  |===================                                                   |  27%\n  |                                                                            \n  |====================                                                  |  28%\n  |                                                                            \n  |====================                                                  |  29%\n  |                                                                            \n  |=====================                                                 |  30%\n  |                                                                            \n  |======================                                                |  31%\n  |                                                                            \n  |======================                                                |  32%\n  |                                                                            \n  |=======================                                               |  33%\n  |                                                                            \n  |========================                                              |  34%\n  |                                                                            \n  |========================                                              |  35%\n  |                                                                            \n  |=========================                                             |  36%\n  |                                                                            \n  |==========================                                            |  37%\n  |                                                                            \n  |===========================                                           |  38%\n  |                                                                            \n  |===========================                                           |  39%\n  |                                                                            \n  |============================                                          |  40%\n  |                                                                            \n  |=============================                                         |  41%\n  |                                                                            \n  |=============================                                         |  42%\n  |                                                                            \n  |==============================                                        |  43%\n  |                                                                            \n  |===============================                                       |  44%\n  |                                                                            \n  |================================                                      |  45%\n  |                                                                            \n  |================================                                      |  46%\n  |                                                                            \n  |=================================                                     |  47%\n  |                                                                            \n  |==================================                                    |  48%\n  |                                                                            \n  |==================================                                    |  49%\n  |                                                                            \n  |===================================                                   |  50%\n  |                                                                            \n  |====================================                                  |  51%\n  |                                                                            \n  |====================================                                  |  52%\n  |                                                                            \n  |=====================================                                 |  53%\n  |                                                                            \n  |======================================                                |  54%\n  |                                                                            \n  |======================================                                |  55%\n  |                                                                            \n  |=======================================                               |  56%\n  |                                                                            \n  |========================================                              |  57%\n  |                                                                            \n  |=========================================                             |  58%\n  |                                                                            \n  |=========================================                             |  59%\n  |                                                                            \n  |==========================================                            |  60%\n  |                                                                            \n  |===========================================                           |  61%\n  |                                                                            \n  |===========================================                           |  62%\n  |                                                                            \n  |============================================                          |  63%\n  |                                                                            \n  |=============================================                         |  64%\n  |                                                                            \n  |==============================================                        |  65%\n  |                                                                            \n  |==============================================                        |  66%\n  |                                                                            \n  |===============================================                       |  67%\n  |                                                                            \n  |================================================                      |  68%\n  |                                                                            \n  |======================================================================| 100%\n## Fold: 5/10\n## \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |=                                                                     |   1%\n  |                                                                            \n  |=                                                                     |   2%\n  |                                                                            \n  |==                                                                    |   3%\n  |                                                                            \n  |===                                                                   |   4%\n  |                                                                            \n  |====                                                                  |   5%\n  |                                                                            \n  |====                                                                  |   6%\n  |                                                                            \n  |=====                                                                 |   7%\n  |                                                                            \n  |======                                                                |   8%\n  |                                                                            \n  |======                                                                |   9%\n  |                                                                            \n  |=======                                                               |  10%\n  |                                                                            \n  |========                                                              |  11%\n  |                                                                            \n  |========                                                              |  12%\n  |                                                                            \n  |=========                                                             |  13%\n  |                                                                            \n  |==========                                                            |  14%\n  |                                                                            \n  |==========                                                            |  15%\n  |                                                                            \n  |===========                                                           |  16%\n  |                                                                            \n  |============                                                          |  17%\n  |                                                                            \n  |=============                                                         |  18%\n  |                                                                            \n  |=============                                                         |  19%\n  |                                                                            \n  |==============                                                        |  20%\n  |                                                                            \n  |===============                                                       |  21%\n  |                                                                            \n  |===============                                                       |  22%\n  |                                                                            \n  |================                                                      |  23%\n  |                                                                            \n  |=================                                                     |  24%\n  |                                                                            \n  |==================                                                    |  25%\n  |                                                                            \n  |==================                                                    |  26%\n  |                                                                            \n  |===================                                                   |  27%\n  |                                                                            \n  |====================                                                  |  28%\n  |                                                                            \n  |====================                                                  |  29%\n  |                                                                            \n  |=====================                                                 |  30%\n  |                                                                            \n  |======================                                                |  31%\n  |                                                                            \n  |======================                                                |  32%\n  |                                                                            \n  |=======================                                               |  33%\n  |                                                                            \n  |========================                                              |  34%\n  |                                                                            \n  |========================                                              |  35%\n  |                                                                            \n  |=========================                                             |  36%\n  |                                                                            \n  |==========================                                            |  37%\n  |                                                                            \n  |===========================                                           |  38%\n  |                                                                            \n  |===========================                                           |  39%\n  |                                                                            \n  |============================                                          |  40%\n  |                                                                            \n  |=============================                                         |  41%\n  |                                                                            \n  |=============================                                         |  42%\n  |                                                                            \n  |==============================                                        |  43%\n  |                                                                            \n  |===============================                                       |  44%\n  |                                                                            \n  |================================                                      |  45%\n  |                                                                            \n  |================================                                      |  46%\n  |                                                                            \n  |=================================                                     |  47%\n  |                                                                            \n  |==================================                                    |  48%\n  |                                                                            \n  |==================================                                    |  49%\n  |                                                                            \n  |===================================                                   |  50%\n  |                                                                            \n  |====================================                                  |  51%\n  |                                                                            \n  |====================================                                  |  52%\n  |                                                                            \n  |=====================================                                 |  53%\n  |                                                                            \n  |======================================                                |  54%\n  |                                                                            \n  |======================================                                |  55%\n  |                                                                            \n  |=======================================                               |  56%\n  |                                                                            \n  |========================================                              |  57%\n  |                                                                            \n  |=========================================                             |  58%\n  |                                                                            \n  |=========================================                             |  59%\n  |                                                                            \n  |==========================================                            |  60%\n  |                                                                            \n  |===========================================                           |  61%\n  |                                                                            \n  |===========================================                           |  62%\n  |                                                                            \n  |============================================                          |  63%\n  |                                                                            \n  |=============================================                         |  64%\n  |                                                                            \n  |==============================================                        |  65%\n  |                                                                            \n  |==============================================                        |  66%\n  |                                                                            \n  |===============================================                       |  67%\n  |                                                                            \n  |================================================                      |  68%\n  |                                                                            \n  |================================================                      |  69%\n  |                                                                            \n  |======================================================================| 100%\n## Fold: 6/10\n## \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |=                                                                     |   1%\n  |                                                                            \n  |=                                                                     |   2%\n  |                                                                            \n  |==                                                                    |   3%\n  |                                                                            \n  |===                                                                   |   4%\n  |                                                                            \n  |====                                                                  |   5%\n  |                                                                            \n  |====                                                                  |   6%\n  |                                                                            \n  |=====                                                                 |   7%\n  |                                                                            \n  |======                                                                |   8%\n  |                                                                            \n  |======                                                                |   9%\n  |                                                                            \n  |=======                                                               |  10%\n  |                                                                            \n  |========                                                              |  11%\n  |                                                                            \n  |========                                                              |  12%\n  |                                                                            \n  |=========                                                             |  13%\n  |                                                                            \n  |==========                                                            |  14%\n  |                                                                            \n  |==========                                                            |  15%\n  |                                                                            \n  |===========                                                           |  16%\n  |                                                                            \n  |============                                                          |  17%\n  |                                                                            \n  |=============                                                         |  18%\n  |                                                                            \n  |=============                                                         |  19%\n  |                                                                            \n  |==============                                                        |  20%\n  |                                                                            \n  |===============                                                       |  21%\n  |                                                                            \n  |===============                                                       |  22%\n  |                                                                            \n  |================                                                      |  23%\n  |                                                                            \n  |=================                                                     |  24%\n  |                                                                            \n  |==================                                                    |  25%\n  |                                                                            \n  |==================                                                    |  26%\n  |                                                                            \n  |===================                                                   |  27%\n  |                                                                            \n  |====================                                                  |  28%\n  |                                                                            \n  |====================                                                  |  29%\n  |                                                                            \n  |=====================                                                 |  30%\n  |                                                                            \n  |======================                                                |  31%\n  |                                                                            \n  |======================                                                |  32%\n  |                                                                            \n  |=======================                                               |  33%\n  |                                                                            \n  |========================                                              |  34%\n  |                                                                            \n  |========================                                              |  35%\n  |                                                                            \n  |=========================                                             |  36%\n  |                                                                            \n  |==========================                                            |  37%\n  |                                                                            \n  |===========================                                           |  38%\n  |                                                                            \n  |===========================                                           |  39%\n  |                                                                            \n  |============================                                          |  40%\n  |                                                                            \n  |=============================                                         |  41%\n  |                                                                            \n  |=============================                                         |  42%\n  |                                                                            \n  |==============================                                        |  43%\n  |                                                                            \n  |===============================                                       |  44%\n  |                                                                            \n  |================================                                      |  45%\n  |                                                                            \n  |================================                                      |  46%\n  |                                                                            \n  |=================================                                     |  47%\n  |                                                                            \n  |==================================                                    |  48%\n  |                                                                            \n  |==================================                                    |  49%\n  |                                                                            \n  |===================================                                   |  50%\n  |                                                                            \n  |====================================                                  |  51%\n  |                                                                            \n  |====================================                                  |  52%\n  |                                                                            \n  |=====================================                                 |  53%\n  |                                                                            \n  |======================================                                |  54%\n  |                                                                            \n  |======================================                                |  55%\n  |                                                                            \n  |=======================================                               |  56%\n  |                                                                            \n  |========================================                              |  57%\n  |                                                                            \n  |=========================================                             |  58%\n  |                                                                            \n  |=========================================                             |  59%\n  |                                                                            \n  |==========================================                            |  60%\n  |                                                                            \n  |===========================================                           |  61%\n  |                                                                            \n  |===========================================                           |  62%\n  |                                                                            \n  |============================================                          |  63%\n  |                                                                            \n  |=============================================                         |  64%\n  |                                                                            \n  |==============================================                        |  65%\n  |                                                                            \n  |==============================================                        |  66%\n  |                                                                            \n  |===============================================                       |  67%\n  |                                                                            \n  |======================================================================| 100%\n## Fold: 7/10\n## \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |=                                                                     |   1%\n  |                                                                            \n  |=                                                                     |   2%\n  |                                                                            \n  |==                                                                    |   3%\n  |                                                                            \n  |===                                                                   |   4%\n  |                                                                            \n  |====                                                                  |   5%\n  |                                                                            \n  |====                                                                  |   6%\n  |                                                                            \n  |=====                                                                 |   7%\n  |                                                                            \n  |======                                                                |   8%\n  |                                                                            \n  |======                                                                |   9%\n  |                                                                            \n  |=======                                                               |  10%\n  |                                                                            \n  |========                                                              |  11%\n  |                                                                            \n  |========                                                              |  12%\n  |                                                                            \n  |=========                                                             |  13%\n  |                                                                            \n  |==========                                                            |  14%\n  |                                                                            \n  |==========                                                            |  15%\n  |                                                                            \n  |===========                                                           |  16%\n  |                                                                            \n  |============                                                          |  17%\n  |                                                                            \n  |=============                                                         |  18%\n  |                                                                            \n  |=============                                                         |  19%\n  |                                                                            \n  |==============                                                        |  20%\n  |                                                                            \n  |===============                                                       |  21%\n  |                                                                            \n  |===============                                                       |  22%\n  |                                                                            \n  |================                                                      |  23%\n  |                                                                            \n  |=================                                                     |  24%\n  |                                                                            \n  |==================                                                    |  25%\n  |                                                                            \n  |==================                                                    |  26%\n  |                                                                            \n  |===================                                                   |  27%\n  |                                                                            \n  |====================                                                  |  28%\n  |                                                                            \n  |====================                                                  |  29%\n  |                                                                            \n  |=====================                                                 |  30%\n  |                                                                            \n  |======================                                                |  31%\n  |                                                                            \n  |======================                                                |  32%\n  |                                                                            \n  |=======================                                               |  33%\n  |                                                                            \n  |========================                                              |  34%\n  |                                                                            \n  |========================                                              |  35%\n  |                                                                            \n  |=========================                                             |  36%\n  |                                                                            \n  |==========================                                            |  37%\n  |                                                                            \n  |===========================                                           |  38%\n  |                                                                            \n  |===========================                                           |  39%\n  |                                                                            \n  |============================                                          |  40%\n  |                                                                            \n  |=============================                                         |  41%\n  |                                                                            \n  |=============================                                         |  42%\n  |                                                                            \n  |==============================                                        |  43%\n  |                                                                            \n  |===============================                                       |  44%\n  |                                                                            \n  |================================                                      |  45%\n  |                                                                            \n  |================================                                      |  46%\n  |                                                                            \n  |=================================                                     |  47%\n  |                                                                            \n  |==================================                                    |  48%\n  |                                                                            \n  |==================================                                    |  49%\n  |                                                                            \n  |===================================                                   |  50%\n  |                                                                            \n  |====================================                                  |  51%\n  |                                                                            \n  |====================================                                  |  52%\n  |                                                                            \n  |=====================================                                 |  53%\n  |                                                                            \n  |======================================                                |  54%\n  |                                                                            \n  |======================================                                |  55%\n  |                                                                            \n  |=======================================                               |  56%\n  |                                                                            \n  |========================================                              |  57%\n  |                                                                            \n  |=========================================                             |  58%\n  |                                                                            \n  |=========================================                             |  59%\n  |                                                                            \n  |==========================================                            |  60%\n  |                                                                            \n  |===========================================                           |  61%\n  |                                                                            \n  |===========================================                           |  62%\n  |                                                                            \n  |============================================                          |  63%\n  |                                                                            \n  |=============================================                         |  64%\n  |                                                                            \n  |==============================================                        |  65%\n  |                                                                            \n  |==============================================                        |  66%\n  |                                                                            \n  |===============================================                       |  67%\n  |                                                                            \n  |================================================                      |  68%\n  |                                                                            \n  |======================================================================| 100%\n## Fold: 8/10\n## \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |=                                                                     |   1%\n  |                                                                            \n  |=                                                                     |   2%\n  |                                                                            \n  |==                                                                    |   3%\n  |                                                                            \n  |===                                                                   |   4%\n  |                                                                            \n  |====                                                                  |   5%\n  |                                                                            \n  |====                                                                  |   6%\n  |                                                                            \n  |=====                                                                 |   7%\n  |                                                                            \n  |======                                                                |   8%\n  |                                                                            \n  |======                                                                |   9%\n  |                                                                            \n  |=======                                                               |  10%\n  |                                                                            \n  |========                                                              |  11%\n  |                                                                            \n  |========                                                              |  12%\n  |                                                                            \n  |=========                                                             |  13%\n  |                                                                            \n  |==========                                                            |  14%\n  |                                                                            \n  |==========                                                            |  15%\n  |                                                                            \n  |===========                                                           |  16%\n  |                                                                            \n  |============                                                          |  17%\n  |                                                                            \n  |=============                                                         |  18%\n  |                                                                            \n  |=============                                                         |  19%\n  |                                                                            \n  |==============                                                        |  20%\n  |                                                                            \n  |===============                                                       |  21%\n  |                                                                            \n  |===============                                                       |  22%\n  |                                                                            \n  |================                                                      |  23%\n  |                                                                            \n  |=================                                                     |  24%\n  |                                                                            \n  |==================                                                    |  25%\n  |                                                                            \n  |==================                                                    |  26%\n  |                                                                            \n  |===================                                                   |  27%\n  |                                                                            \n  |====================                                                  |  28%\n  |                                                                            \n  |====================                                                  |  29%\n  |                                                                            \n  |=====================                                                 |  30%\n  |                                                                            \n  |======================                                                |  31%\n  |                                                                            \n  |======================                                                |  32%\n  |                                                                            \n  |=======================                                               |  33%\n  |                                                                            \n  |========================                                              |  34%\n  |                                                                            \n  |========================                                              |  35%\n  |                                                                            \n  |=========================                                             |  36%\n  |                                                                            \n  |==========================                                            |  37%\n  |                                                                            \n  |===========================                                           |  38%\n  |                                                                            \n  |===========================                                           |  39%\n  |                                                                            \n  |============================                                          |  40%\n  |                                                                            \n  |=============================                                         |  41%\n  |                                                                            \n  |=============================                                         |  42%\n  |                                                                            \n  |==============================                                        |  43%\n  |                                                                            \n  |===============================                                       |  44%\n  |                                                                            \n  |================================                                      |  45%\n  |                                                                            \n  |================================                                      |  46%\n  |                                                                            \n  |=================================                                     |  47%\n  |                                                                            \n  |==================================                                    |  48%\n  |                                                                            \n  |==================================                                    |  49%\n  |                                                                            \n  |===================================                                   |  50%\n  |                                                                            \n  |====================================                                  |  51%\n  |                                                                            \n  |====================================                                  |  52%\n  |                                                                            \n  |=====================================                                 |  53%\n  |                                                                            \n  |======================================                                |  54%\n  |                                                                            \n  |======================================                                |  55%\n  |                                                                            \n  |=======================================                               |  56%\n  |                                                                            \n  |========================================                              |  57%\n  |                                                                            \n  |=========================================                             |  58%\n  |                                                                            \n  |=========================================                             |  59%\n  |                                                                            \n  |==========================================                            |  60%\n  |                                                                            \n  |===========================================                           |  61%\n  |                                                                            \n  |===========================================                           |  62%\n  |                                                                            \n  |============================================                          |  63%\n  |                                                                            \n  |=============================================                         |  64%\n  |                                                                            \n  |==============================================                        |  65%\n  |                                                                            \n  |==============================================                        |  66%\n  |                                                                            \n  |===============================================                       |  67%\n  |                                                                            \n  |================================================                      |  68%\n  |                                                                            \n  |================================================                      |  69%\n  |                                                                            \n  |======================================================================| 100%\n## Fold: 9/10\n## \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |=                                                                     |   1%\n  |                                                                            \n  |=                                                                     |   2%\n  |                                                                            \n  |==                                                                    |   3%\n  |                                                                            \n  |===                                                                   |   4%\n  |                                                                            \n  |====                                                                  |   5%\n  |                                                                            \n  |====                                                                  |   6%\n  |                                                                            \n  |=====                                                                 |   7%\n  |                                                                            \n  |======                                                                |   8%\n  |                                                                            \n  |======                                                                |   9%\n  |                                                                            \n  |=======                                                               |  10%\n  |                                                                            \n  |========                                                              |  11%\n  |                                                                            \n  |========                                                              |  12%\n  |                                                                            \n  |=========                                                             |  13%\n  |                                                                            \n  |==========                                                            |  14%\n  |                                                                            \n  |==========                                                            |  15%\n  |                                                                            \n  |===========                                                           |  16%\n  |                                                                            \n  |============                                                          |  17%\n  |                                                                            \n  |=============                                                         |  18%\n  |                                                                            \n  |=============                                                         |  19%\n  |                                                                            \n  |==============                                                        |  20%\n  |                                                                            \n  |===============                                                       |  21%\n  |                                                                            \n  |===============                                                       |  22%\n  |                                                                            \n  |================                                                      |  23%\n  |                                                                            \n  |=================                                                     |  24%\n  |                                                                            \n  |==================                                                    |  25%\n  |                                                                            \n  |==================                                                    |  26%\n  |                                                                            \n  |===================                                                   |  27%\n  |                                                                            \n  |====================                                                  |  28%\n  |                                                                            \n  |====================                                                  |  29%\n  |                                                                            \n  |=====================                                                 |  30%\n  |                                                                            \n  |======================                                                |  31%\n  |                                                                            \n  |======================                                                |  32%\n  |                                                                            \n  |=======================                                               |  33%\n  |                                                                            \n  |========================                                              |  34%\n  |                                                                            \n  |========================                                              |  35%\n  |                                                                            \n  |=========================                                             |  36%\n  |                                                                            \n  |==========================                                            |  37%\n  |                                                                            \n  |===========================                                           |  38%\n  |                                                                            \n  |===========================                                           |  39%\n  |                                                                            \n  |============================                                          |  40%\n  |                                                                            \n  |=============================                                         |  41%\n  |                                                                            \n  |=============================                                         |  42%\n  |                                                                            \n  |==============================                                        |  43%\n  |                                                                            \n  |===============================                                       |  44%\n  |                                                                            \n  |================================                                      |  45%\n  |                                                                            \n  |================================                                      |  46%\n  |                                                                            \n  |=================================                                     |  47%\n  |                                                                            \n  |==================================                                    |  48%\n  |                                                                            \n  |==================================                                    |  49%\n  |                                                                            \n  |===================================                                   |  50%\n  |                                                                            \n  |====================================                                  |  51%\n  |                                                                            \n  |====================================                                  |  52%\n  |                                                                            \n  |=====================================                                 |  53%\n  |                                                                            \n  |======================================                                |  54%\n  |                                                                            \n  |======================================                                |  55%\n  |                                                                            \n  |=======================================                               |  56%\n  |                                                                            \n  |========================================                              |  57%\n  |                                                                            \n  |=========================================                             |  58%\n  |                                                                            \n  |=========================================                             |  59%\n  |                                                                            \n  |==========================================                            |  60%\n  |                                                                            \n  |===========================================                           |  61%\n  |                                                                            \n  |===========================================                           |  62%\n  |                                                                            \n  |============================================                          |  63%\n  |                                                                            \n  |=============================================                         |  64%\n  |                                                                            \n  |==============================================                        |  65%\n  |                                                                            \n  |==============================================                        |  66%\n  |                                                                            \n  |===============================================                       |  67%\n  |                                                                            \n  |======================================================================| 100%\n## Fold: 10/10\n## \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |=                                                                     |   1%\n  |                                                                            \n  |=                                                                     |   2%\n  |                                                                            \n  |==                                                                    |   3%\n  |                                                                            \n  |===                                                                   |   4%\n  |                                                                            \n  |====                                                                  |   5%\n  |                                                                            \n  |====                                                                  |   6%\n  |                                                                            \n  |=====                                                                 |   7%\n  |                                                                            \n  |======                                                                |   8%\n  |                                                                            \n  |======                                                                |   9%\n  |                                                                            \n  |=======                                                               |  10%\n  |                                                                            \n  |========                                                              |  11%\n  |                                                                            \n  |========                                                              |  12%\n  |                                                                            \n  |=========                                                             |  13%\n  |                                                                            \n  |==========                                                            |  14%\n  |                                                                            \n  |==========                                                            |  15%\n  |                                                                            \n  |===========                                                           |  16%\n  |                                                                            \n  |============                                                          |  17%\n  |                                                                            \n  |=============                                                         |  18%\n  |                                                                            \n  |=============                                                         |  19%\n  |                                                                            \n  |==============                                                        |  20%\n  |                                                                            \n  |===============                                                       |  21%\n  |                                                                            \n  |===============                                                       |  22%\n  |                                                                            \n  |================                                                      |  23%\n  |                                                                            \n  |=================                                                     |  24%\n  |                                                                            \n  |==================                                                    |  25%\n  |                                                                            \n  |==================                                                    |  26%\n  |                                                                            \n  |===================                                                   |  27%\n  |                                                                            \n  |====================                                                  |  28%\n  |                                                                            \n  |====================                                                  |  29%\n  |                                                                            \n  |=====================                                                 |  30%\n  |                                                                            \n  |======================                                                |  31%\n  |                                                                            \n  |======================                                                |  32%\n  |                                                                            \n  |=======================                                               |  33%\n  |                                                                            \n  |========================                                              |  34%\n  |                                                                            \n  |========================                                              |  35%\n  |                                                                            \n  |=========================                                             |  36%\n  |                                                                            \n  |==========================                                            |  37%\n  |                                                                            \n  |===========================                                           |  38%\n  |                                                                            \n  |===========================                                           |  39%\n  |                                                                            \n  |============================                                          |  40%\n  |                                                                            \n  |=============================                                         |  41%\n  |                                                                            \n  |=============================                                         |  42%\n  |                                                                            \n  |==============================                                        |  43%\n  |                                                                            \n  |===============================                                       |  44%\n  |                                                                            \n  |================================                                      |  45%\n  |                                                                            \n  |================================                                      |  46%\n  |                                                                            \n  |=================================                                     |  47%\n  |                                                                            \n  |==================================                                    |  48%\n  |                                                                            \n  |==================================                                    |  49%\n  |                                                                            \n  |===================================                                   |  50%\n  |                                                                            \n  |====================================                                  |  51%\n  |                                                                            \n  |====================================                                  |  52%\n  |                                                                            \n  |=====================================                                 |  53%\n  |                                                                            \n  |======================================                                |  54%\n  |                                                                            \n  |======================================                                |  55%\n  |                                                                            \n  |=======================================                               |  56%\n  |                                                                            \n  |========================================                              |  57%\n  |                                                                            \n  |=========================================                             |  58%\n  |                                                                            \n  |=========================================                             |  59%\n  |                                                                            \n  |==========================================                            |  60%\n  |                                                                            \n  |===========================================                           |  61%\n  |                                                                            \n  |===========================================                           |  62%\n  |                                                                            \n  |============================================                          |  63%\n  |                                                                            \n  |=============================================                         |  64%\n  |                                                                            \n  |==============================================                        |  65%\n  |                                                                            \n  |==============================================                        |  66%\n  |                                                                            \n  |===============================================                       |  67%\n  |                                                                            \n  |================================================                      |  68%\n  |                                                                            \n  |================================================                      |  69%\n  |                                                                            \n  |======================================================================| 100%\n\n也可以通过以下方式实现：\n\nglmnet.control(itrace = 1) # 变成0就不显示了~",
    "crumbs": [
      "代码实战",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>lasso回归</span>"
    ]
  },
  {
    "objectID": "glmnet.html#扩展正则化cox回归",
    "href": "glmnet.html#扩展正则化cox回归",
    "title": "13  lasso回归",
    "section": "13.11 扩展：正则化Cox回归",
    "text": "13.11 扩展：正则化Cox回归\n正则化的COX回归，也就是glmnet在生存分析中的应用，这里我们还是以lasso为例进行演示。\nglmnet包的详细使用介绍已经在前面都介绍过了，正则化的COX回归并没有太大的不同，所以这里简单介绍一下。\n下面是一些理论解释，大家随便看看就好。\n在glmnet中，我们使用弹性网络（elastic net）方法对部分似然的负对数进行惩罚。\n部分似然（partial-likelihood）是一种用于处理生存分析（survival-analysis）中右侧截尾（right-censored）观测的方法。而负对数部分似然（negative-log-partial-likelihood）则是对部分似然取反并求对数，目的是将最大化似然函数的问题转化为最小化负对数似然函数的问题。\n为了进一步约束模型的复杂度和提高模型的泛化能力，我们在负对数部分似然的基础上引入了弹性网络惩罚（elastic-net-penalty）。弹性网惩罚结合了L1正则化（L1-regularization）和L2正则化（L2-regularization）的特性，从而既能产生稀疏解，又能保留一些高度相关的特征。这样我们可以在建立模型时在部分似然的基础上，使用弹性网惩罚来进行模型的优化和参数选择，以提高模型的性能和泛化能力。\n\n13.11.1 基础使用\nglmnet对数据格式是有要求的，之前也说过，x必须是由自变量组成的matrix，y可以是一个两列的matrix，两列的列名必须是time和status，分别表示生存时间和生存状态，其中status必须使用0和1组成，0表示删失，1表示发生终点事件（又叫失效事件，比如死亡）。除此之外，y还可以是由Surv()函数生成的对象。\n下面是一个示例数据：\n\nlibrary(glmnet)\nlibrary(survival)\n\ndata(CoxExample)\nx &lt;- CoxExample$x\ny &lt;- CoxExample$y\n\n# 查看y的数据格式\ny[1:5, ]\n##            time status\n## [1,] 1.76877757      1\n## [2,] 0.54528404      1\n## [3,] 0.04485918      0\n## [4,] 0.85032298      0\n## [5,] 0.61488426      1\n\n建立模型，只需要使用family = \"cox\"即可：\n\nfit &lt;- glmnet(x, y, family = \"cox\")\n\n其中的一些参数比如alpha，weights，nlambda等，在前面已经介绍过了，这里就不再多介绍了。\n可视化、提取系数、预测新数据和之前介绍的用法也是一模一样，这里也不再多说了。\n\n\n13.11.2 交叉验证\n对于正则化的cox来说，cv.glmnet()中的type.measure只能是\"deviance\"（默认值，给出部分似然），或者\"C\"，给出 Harrell-C-index。\n\nset.seed(1)\ncvfit &lt;- cv.glmnet(x, y, family = \"cox\", type.measure = \"C\")\n\nprint(cvfit)\n## \n## Call:  cv.glmnet(x = x, y = y, type.measure = \"C\", family = \"cox\") \n## \n## Measure: C-index \n## \n##      Lambda Index Measure       SE Nonzero\n## min 0.03058    23  0.7304 0.005842      11\n## 1se 0.05865    16  0.7267 0.005993      10\n\n画图也是一样的，下面这幅图的解释在前面也已经详细介绍过了，这里就不再多做解释了：\n\nplot(cvfit)\n\n\n\n\n\n\n\n\n\n在glmnet中，对于生存时间的排列相同（ties）情况，使用的是Breslow近似（Breslow approximation）。这与survival软件包中的coxph函数的默认排列处理方法（tie-handling method）——Efron近似（Efron approximation）不同。\n当存在相同的生存时间观测时，例如多个个体在同一时间发生事件，排列的处理方法对估计结果和推断的准确性至关重要。Breslow近似与Efron近似是最常见的两种处理方法。\n在glmnet中，使用Breslow近似处理排列，该方法假设所有的排列发生在后一事件之前的所有时间上。这种近似方法在计算效率上比较高，但可能会导致估计的偏差。\n而在survival软件包中的coxph函数，默认使用的是Efron近似处理排列。Efron近似方法基于考虑排列发生的时间顺序进行调整，更接近真实的结果，但在计算过程中稍微耗时一些。\n因此，当在glmnet和survival软件包中处理生存分析时，需要注意到在处理排列的方法上的差异，以确保得到准确和一致的结果。\n\n\n\n13.11.3 分层COX\ncoxph()支持strata()函数，因为它是使用公式形式的，但是glmnet不支持公式形式，只能使用x/y形式的输入，所以如果要实现分层，需要使用stratifySurv()。\n继续使用上面的示例数据，我们把1000个观测分成5层：\n\n# 把1000个观测分5层\nstrata &lt;- rep(1:5, length.out = 1000)\ny2 &lt;- stratifySurv(y, strata) # 对y进行分层\nstr(y2[1:6])\n##  'stratifySurv' num [1:6, 1:2] 1.7688  0.5453  0.0449+ 0.8503+ 0.6149  0.2986+\n##  - attr(*, \"dimnames\")=List of 2\n##   ..$ : NULL\n##   ..$ : chr [1:2] \"time\" \"status\"\n##  - attr(*, \"type\")= chr \"right\"\n##  - attr(*, \"strata\")= int [1:6] 1 2 3 4 5 1\n\n接下来把y2提供给glmnet()或者cv.glmnet()就可以实现正则化的分层COX了。\n\nfit &lt;- glmnet(x, y2, family = \"cox\")\n\ncv.fit &lt;- cv.glmnet(x, y2, family = \"cox\", nfolds = 5)\nplot(cv.fit)\n\n\n\n\n\n\n\n\n\n\n13.11.4 生存曲线\nglmnet的结果可以直接提供给survfit()使用，可以用来画生存曲线。这里简单介绍一下，大家知道即可，因为大家在平时写文章时根本不会这么用……\n以下是一个示例。\n\ndata(CoxExample)\nx &lt;- CoxExample$x\ny &lt;- CoxExample$y\n\ny &lt;- Surv(y[,1],y[,2]) # 需要用Surv转换格式\n\nfit &lt;- glmnet(x, y, family = \"cox\")\nsurvival::survfit(fit, s = 0.05, x = x, y = y)\n## Call: survfit.coxnet(formula = fit, s = 0.05, x = x, y = y)\n## \n##         n events median\n## [1,] 1000    692  0.922\n\n直接画图即可：\n\nplot(survival::survfit(fit, s = 0.05, x = x, y = y))\n\n\n\n\n\n\n\n\n这个生存曲线有些奇怪，因为数据原因，大家可以自己尝试下。\n基于新的数据画生存曲线也是可以的：\n\nplot(survival::survfit(fit, s = 0.05, x = x, y = y, newx = x[1:3, ]))",
    "crumbs": [
      "代码实战",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>lasso回归</span>"
    ]
  },
  {
    "objectID": "glmnet.html#其他",
    "href": "glmnet.html#其他",
    "title": "13  lasso回归",
    "section": "13.12 其他",
    "text": "13.12 其他\n\ntidymodels实现lasso回归及超参数调优\nlasso回归的列线图、内部验证、外部验证",
    "crumbs": [
      "代码实战",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>lasso回归</span>"
    ]
  },
  {
    "objectID": "KNN.html",
    "href": "KNN.html",
    "title": "14  K最近邻",
    "section": "",
    "text": "14.1 算法简介\nK最近邻（K-Nearest-Neighbor，KNN）是一种非线性的分类算法，KNN处理分类问题的方法是：找K个距离待遇测样本最近的点，然后根据这几个点的类别来确定新样本的类别。\n下面以一个二分类问题为例说明KNN的思想。\n下图有两个特征可以用来预测肿瘤是”良性”还是”恶性”。图中的X表示我们要预测的新样本。如果算法设定k=3，那么圆圈中包含的3个观测就是样本X的最近邻。因为其中占多数比例的类别是”恶性”，所以样本X被分类为”恶性”。\n思想是不是很简单？K的选择对于KNN的预测结果是非常重要的。\nKNN中另一个需要指出的重要问题是距离的计算方法，或者说是特征空间中数据点的临近度的计算。默认的距离是欧氏距离，也就是从点A到点B的简单直线距离。",
    "crumbs": [
      "代码实战",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>K最近邻</span>"
    ]
  },
  {
    "objectID": "KNN.html#算法简介",
    "href": "KNN.html#算法简介",
    "title": "14  K最近邻",
    "section": "",
    "text": "近朱者赤，近墨者黑。\n\n\n\n\n\n\n\n\n\n\n\n\n注释\n\n\n\n两点间的距离强烈依赖于测量特征时使用的单位，所以必须对其进行标准化，而且要求数据不能有缺失值。",
    "crumbs": [
      "代码实战",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>K最近邻</span>"
    ]
  },
  {
    "objectID": "KNN.html#准备数据",
    "href": "KNN.html#准备数据",
    "title": "14  K最近邻",
    "section": "14.2 准备数据",
    "text": "14.2 准备数据\n演示数据为印第安人糖尿病数据集，这个数据一共有768行，9列，其中diabetes是结果变量，为二分类，其余列是预测变量。\n该数据集的原始版本是有缺失值的，我这里使用的是插补过的版本，详细过程请参考数据准备这一章。\n\nrm(list = ls())\n\nload(file = \"datasets/pimadiabetes.rdata\")\n\ndim(pimadiabetes)\n## [1] 768   9\nstr(pimadiabetes)\n## 'data.frame':    768 obs. of  9 variables:\n##  $ pregnant: num  6 1 8 1 0 5 3 10 2 8 ...\n##  $ glucose : num  148 85 183 89 137 116 78 115 197 125 ...\n##  $ pressure: num  72 66 64 66 40 ...\n##  $ triceps : num  35 29 22.9 23 35 ...\n##  $ insulin : num  202.2 64.6 217.1 94 168 ...\n##  $ mass    : num  33.6 26.6 23.3 28.1 43.1 ...\n##  $ pedigree: num  0.627 0.351 0.672 0.167 2.288 ...\n##  $ age     : num  50 31 32 21 33 30 26 29 53 54 ...\n##  $ diabetes: Factor w/ 2 levels \"pos\",\"neg\": 2 1 2 1 2 1 2 1 2 2 ...\n\n各个变量的含义：\n\npregnant：怀孕次数\nglucose：血浆葡萄糖浓度（葡萄糖耐量试验）\npressure：舒张压（毫米汞柱）\ntriceps：三头肌皮褶厚度（mm）\ninsulin：2小时血清胰岛素（mu U/ml）\nmass：BMI\npedigree：糖尿病谱系功能，是一种用于预测糖尿病发病风险的指标，该指标是基于家族史的糖尿病遗传风险因素的计算得出的。它计算了患者的家族成员是否患有糖尿病以及他们与患者的亲缘关系，从而得出一个综合评分，用于预测患糖尿病的概率。\nage：年龄\ndiabetes：是否有糖尿病\n\n先对数据进行标准化：\n\n# 对数值型变量进行标准化\npimadiabetes[,-9] &lt;- scale(pimadiabetes[,-9])\nstr(pimadiabetes)\n## 'data.frame':    768 obs. of  9 variables:\n##  $ pregnant: num  0.64 -0.844 1.233 -0.844 -1.141 ...\n##  $ glucose : num  0.863 -1.203 2.011 -1.072 0.503 ...\n##  $ pressure: num  -0.0314 -0.5244 -0.6887 -0.5244 -2.6607 ...\n##  $ triceps : num  0.63124 -0.00231 -0.64853 -0.63586 0.63124 ...\n##  $ insulin : num  0.478 -0.933 0.63 -0.631 0.127 ...\n##  $ mass    : num  0.172 -0.844 -1.323 -0.626 1.551 ...\n##  $ pedigree: num  0.468 -0.365 0.604 -0.92 5.481 ...\n##  $ age     : num  1.4251 -0.1905 -0.1055 -1.0409 -0.0205 ...\n##  $ diabetes: Factor w/ 2 levels \"pos\",\"neg\": 2 1 2 1 2 1 2 1 2 2 ...",
    "crumbs": [
      "代码实战",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>K最近邻</span>"
    ]
  },
  {
    "objectID": "KNN.html#class包",
    "href": "KNN.html#class包",
    "title": "14  K最近邻",
    "section": "14.3 class包",
    "text": "14.3 class包\n数据划分为训练集和测试集，划分比例为7：3。\n但是R语言里class包在使用时需要把真实结果去掉，所以我们把真实结果去掉，只保留预测变量。\n\n# 划分是随机的，设置种子数可以让结果复现\nset.seed(123)\nind &lt;- sample(1:nrow(pimadiabetes), size = 0.7*nrow(pimadiabetes))\n\n# 去掉真实结果列\ntrain &lt;- pimadiabetes[ind,-9]\ntest &lt;- pimadiabetes[-ind, -9]\n\ndim(train)\n## [1] 537   8\ndim(test)\n## [1] 231   8\n\nstr(train)\n## 'data.frame':    537 obs. of  8 variables:\n##  $ pregnant: num  -1.141 1.233 0.343 -0.251 1.233 ...\n##  $ glucose : num  0.535 -1.564 0.699 -1.137 -1.203 ...\n##  $ pressure: num  -1.017 -0.196 0.462 -1.017 -1.428 ...\n##  $ triceps : num  0.631 1.159 0.86 -1.164 -0.953 ...\n##  $ insulin : num  0.117 -1.092 1.093 -0.881 -0.767 ...\n##  $ mass    : num  0.317 0.419 1.827 -1.541 -1.164 ...\n##  $ pedigree: num  0.1875 0.7036 -0.8507 -0.0841 -1.0137 ...\n##  $ age     : num  -1.041 0.49 1.17 -1.041 0.745 ...\n\n# 把真实结果列单独拿出来，后面用\ntruth_train &lt;- pimadiabetes[ind,9]\ntruth_test &lt;- pimadiabetes[-ind,9]\n\n\n14.3.1 建立模型\n在训练集建立模型，1行代码搞定，1行代码即可实现：在训练集拟合模型，并对测试集做出预测：\n\nlibrary(class)\n\nf &lt;- knn(train = train, # 训练集，只有预测变量，没有结果变量\n         test = test, # 测试集，没有结果变量\n         cl = truth_train, # 训练集的真实结果\n         k = 8, # 使用的近邻个数\n         prob = TRUE # 需要计算概率\n         )\n\n# 查看测试集的预测结果，只看前6个\nhead(f)\n## [1] neg neg pos neg neg pos\n## Levels: pos neg\n\n# 查看测试集的预测概率，只看前6个\nprob &lt;- attr(f,\"prob\")\nhead(prob)\n## [1] 0.500 0.750 1.000 0.750 0.625 0.625\n\n此时得到的f这个结果是一个因子型的向量，而且是有名字和属性的，大多数模型拟合结果的格式都是不一样的，使用时需要注意！\n还要注意这里的概率，并不是阳性结果的概率，而是预测结果的概率！比如第一个概率0.500是neg的概率，第二个概率0.750是neg的概率，第三个概率1.000是pos的概率！\n所以你如果想要阳性结果(pos)的概率，需要自己计算一下：\n\nprob &lt;- ifelse(f == \"pos\", prob, 1-prob)\nhead(prob)\n## [1] 0.500 0.250 1.000 0.250 0.375 0.625\n\n\n\n14.3.2 模型评价\n模型拟合好之后，下一步就是查看模型的各种指标，来看看这个模型在训练集中的表现如何，比如混淆矩阵、AUC值、准确率等。\n不管是什么类型的模型，如果我们想要评价它的模型表现，都是需要用到模型的预测结果和真实结果的。\n对于回归任务来说，预测结果也是数值型的，对于分类任务来说，模型的预测结果可以是某一种类别的概率，也可以是预测出的具体类别。大多数模型都是既支持计算类别概率又支持计算具体类别的，但是有些模型可能只支持一种类型。\n首先查看混淆矩阵，我们借助caret包展示，这个包目前仍然是查看混线矩阵最全面的，，没有之一，非常好用！\n\n# 借助caret包，f是预测的类别，truth_test是真实的结果\ncaret::confusionMatrix(f,truth_test)\n## Confusion Matrix and Statistics\n## \n##           Reference\n## Prediction pos neg\n##        pos 131  32\n##        neg  19  49\n##                                          \n##                Accuracy : 0.7792         \n##                  95% CI : (0.7201, 0.831)\n##     No Information Rate : 0.6494         \n##     P-Value [Acc &gt; NIR] : 1.269e-05      \n##                                          \n##                   Kappa : 0.4966         \n##                                          \n##  Mcnemar's Test P-Value : 0.09289        \n##                                          \n##             Sensitivity : 0.8733         \n##             Specificity : 0.6049         \n##          Pos Pred Value : 0.8037         \n##          Neg Pred Value : 0.7206         \n##              Prevalence : 0.6494         \n##          Detection Rate : 0.5671         \n##    Detection Prevalence : 0.7056         \n##       Balanced Accuracy : 0.7391         \n##                                          \n##        'Positive' Class : pos            \n## \n\n结果非常全面，最上面是混淆矩阵，然后给出了：\n\n准确率（Accuracy）和准确率的可信区间（95% CI）\n无信息率（No Information Rate）和P值（P-Value Acc&gt;NIR]）、\nKappa值（Kappa一致性指数）\nMcnemar检验的P值（Mcnemar’s Test P-Value）\n敏感性、特异性\n阳性预测值、阴性预测值\n流行率（Prevalence）\n检出率（Detection Rate）\n（Detection Prevalence）\n均衡准确率（Balanced Accuracy）\n\n最后告诉你参考类别是pos。\n当然这些值你也可以单独计算：\n\ncaret::precision(f,truth_test) # 精准率\n## [1] 0.803681\ncaret::recall(f,truth_test) # 召回率，灵敏度\n## [1] 0.8733333\ncaret::F_meas(f,truth_test) # F1分数\n## [1] 0.8370607\n\n然后再画个ROC曲线。首先使用ROCR进行演示，不管是什么包，都遵循前面说过的规律，画ROC曲线是需要真实结果和预测概率的！\n\nlibrary(ROCR)\n\n# ROCR画ROC曲线就是2步，先prediction，再performance\npred &lt;- prediction(prob,truth_test) # 预测概率，真实类别\nperf &lt;- performance(pred, \"tpr\",\"fpr\") # ROC曲线的横纵坐标，不要写错了\nauc &lt;- round(performance(pred, \"auc\")@y.values[[1]],digits = 4) # 提取AUC值\nauc\n## [1] 0.8477\n\n# 画图\nplot(perf,lwd=2,col=\"tomato\")\nabline(0,1,lty=2) # 添加对角线\n# 添加图例\nlegend(\"bottomright\", legend=paste(\"AUC: \",auc), col=\"tomato\", lwd=2,bty = \"n\")\n\n\n\n\n\n\n\n\n另一种方法，使用pROC进行演示，还是那句话，不管是哪种方法，画ROC曲线都是需要提供真实结果和预测概率！\n\nlibrary(pROC)\n\nrocc &lt;- roc(truth_test, prob) # 预测概率，真实结果\nrocc # 看下结果\n## \n## Call:\n## roc.default(response = truth_test, predictor = prob)\n## \n## Data: prob in 150 controls (truth_test pos) &gt; 81 cases (truth_test neg).\n## Area under the curve: 0.8477\n\n# 画图\nplot(rocc, \n     print.auc=TRUE, \n     auc.polygon=TRUE, \n     max.auc.polygon=TRUE, \n     auc.polygon.col=\"skyblue\", \n     grid=c(0.1, 0.2), \n     grid.col=c(\"green\", \"red\"), \n     print.thres=TRUE)\n\n\n\n\n\n\n\n\n关于ROC曲线绘制的合集，共13篇文章，链接：ROC曲线绘制合集\n顺手再展示下PR曲线，也是用ROCR实现：\n\nlibrary(ROCR)\n\n# ROCR画ROC曲线就是2步，先prediction，再performance\npred &lt;- prediction(prob,truth_test) # 预测概率，真实类别\nperf &lt;- performance(pred, \"rec\",\"prec\") # ROC曲线的横纵坐标，不要写错了\nauc &lt;- round(performance(pred, \"auc\")@y.values[[1]],digits = 4) # 提取AUC值\nauc\n## [1] 0.8477\n\n# 画图\nplot(perf,lwd=2,col=\"tomato\")\n# 添加图例\nlegend(\"bottomright\", legend=paste(\"AUC: \",auc), col=\"tomato\", lwd=2,bty = \"n\")\n\n\n\n\n\n\n\n\n是不是非常easy？\n顺手再画个校准曲线，公众号后台回复校准曲线即可获取合集，也是非常简单：\n我这里给大家介绍最新的方法（其实之前也介绍过了），用probably这个包绘制：\n\nlibrary(probably)\n\ncali_data &lt;- data.frame(.pred_pos = prob, diabetes=truth_test)\n\ncal_plot_breaks(cali_data,diabetes, .pred_pos,conf_level = 0.95)\n\n\n\n\n\n\n\n\n但是目前这个版本（1.0.3）有个bug，第3个参数estimate，必须是.pred_xxx，其中的xxx必须是真实结果中的某一个类别，比如我这个数据diabetes中的类别就是pos和neg，那么这个名字就必须是.pred_pos或者.pred_neg，其他都会报错（下标出界）！！\n顺手再画个决策曲线，这个决策曲线是临床预测模型中才有的内容，其他内容基本上都是机器学习的基础知识。后台回复决策曲线即可获取合集：\n\nsource(\"datasets/dca.r\")\n\n# 把概率加到测试集中\ndca_data &lt;- pimadiabetes[-ind,]\ndca_data$prob &lt;- prob\n\n# 结果变量变成0，1\ndca_data$diabetes &lt;- ifelse(dca_data$diabetes==\"pos\",1,0)\n\ndc &lt;- dca(data = dca_data, # 测试集\n          outcome = \"diabetes\",\n          predictors = \"prob\",\n          probability = T\n          )\n\n\n\n\n\n\n\n\n太简单！\n\n前几年stdca.r和dca.r这两个脚本是可以在网络中免费下载的，但是从2022年底左右这个网站就不提供这两段代码的下载了。因为我很早就下载好了，所以我把这两段代码放在粉丝qq群文件里，大家有需要的加群下载即可。。当然我还介绍了很多其他方法，公众号后台回复决策曲线即可获取合集。\n\n\n\n14.3.3 超参数调优\nKNN算法只有一个超参数，就是近邻的数量，所以KNN的超参数调优其实就是如何确定最佳的K值，到底用几个近邻是能得到最好的结果呢？\n现在有很多好用的工具可以实现调优过程了，比如caret、tidymodels、mlr3等，但是这里我给大家演示下for循环的做法，因为它只有1个超参数，很适合这种方法，还可以绘制学习曲线。\n模型评价指标选择AUC。\n\naucs &lt;- list()\nfor (i in 1:50) { # K的值选择1~50\n  f &lt;- knn(train = train, # 训练集\n         test = test, # 测试集\n         cl = truth_train, # 训练集的真实类别\n         k = i, # 使用的近邻个数\n         prob = TRUE # 需要计算概率\n         )\n  prob &lt;- attr(f,\"prob\")\n  prob &lt;- ifelse(f == \"pos\", prob, 1-prob)\n  pred &lt;- prediction(prob,truth_test)\n  perf &lt;- performance(pred, \"tpr\",\"fpr\")\n  auc &lt;- round(performance(pred, \"auc\")@y.values[[1]],digits = 4)\n  aucs[[i]] &lt;- auc\n}\naucs &lt;- do.call(rbind,aucs)[,1]\naucs # 50个AUC值，分别对应50个K值\n##  [1] 0.7068 0.7870 0.8180 0.8264 0.8505 0.8510 0.8481 0.8477 0.8586 0.8540\n## [11] 0.8543 0.8580 0.8537 0.8513 0.8500 0.8451 0.8449 0.8453 0.8488 0.8467\n## [21] 0.8450 0.8473 0.8464 0.8462 0.8474 0.8492 0.8483 0.8508 0.8519 0.8500\n## [31] 0.8499 0.8504 0.8497 0.8491 0.8460 0.8455 0.8460 0.8427 0.8439 0.8417\n## [41] 0.8422 0.8402 0.8393 0.8384 0.8393 0.8377 0.8377 0.8374 0.8356 0.8342\n\n画个图看下不同的K值对应的AUC变化的情况，看图更加直观：\n\nplot_df &lt;- data.frame(k=1:50,auc=aucs)\nlibrary(ggplot2)\n\nggplot(plot_df, aes(k,auc))+\n  geom_line(linewidth=1)+\n  geom_point(size=2)+\n  geom_hline(yintercept = 0.85,linetype = 2)+\n  geom_vline(xintercept = 9,linetype = 2,color=\"red\")\n\n\n\n\n\n\n\n\n结果显示当K=9的时候，AUC值是最大的，此时是0.8586。\n这个图其实是一个学习曲线图，是一种经典的进行超参数调优时使用的图，我在介绍决策树的超参数调优时介绍过了，不知道大家有没有印象？\n所以此时你可以用K=9再重新跑一遍模型，作为你最终的结果。\n\nfinal_f &lt;- knn(train = train, # 训练集，只有预测变量，没有结果变量\n         test = test, # 测试集，没有结果变量\n         cl = truth_train, # 训练集的真实结果\n         k = 9,            # 这里的K值选择9哦！！！\n         prob = TRUE # 需要计算概率\n         )\n\n后续模型评价、画ROC曲线就是一样的代码了，就不再重复了。",
    "crumbs": [
      "代码实战",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>K最近邻</span>"
    ]
  },
  {
    "objectID": "KNN.html#kknn包",
    "href": "KNN.html#kknn包",
    "title": "14  K最近邻",
    "section": "14.4 kknn包",
    "text": "14.4 kknn包\n数据划分为训练集和测试集，划分比例为7：3。\nkknn包不需要把结果变量去掉。\n\n# 划分是随机的，设置种子数可以让结果复现\nset.seed(123)\nind &lt;- sample(1:nrow(pimadiabetes), size = 0.7*nrow(pimadiabetes))\n\n# 训练集、测试集\ntrain &lt;- pimadiabetes[ind,]\ntest &lt;- pimadiabetes[-ind, ]\n\n# 把真实结果列单独拿出来，后面用\ntruth_train &lt;- pimadiabetes[ind,9]\ntruth_test &lt;- pimadiabetes[-ind,9]\n\n\n14.4.1 建立模型\n在训练集拟合模型，支持R语言经典的formula形式：\n\nlibrary(kknn)\nfit &lt;- kknn::kknn(diabetes ~ ., train, test,\n                  scale = F, # w我们已经对数据进行过标准化了，这里就不用了\n            )\n# 直接summary可以查看预测类别和预测概率，太长不展示\n#summary(fit)\n\n我们最关心的东西其实只有预测类别和预测概率而已，所以可以单独查看它们：\n\n# 预测类别\npred_class &lt;- fit[[\"fitted.values\"]]\nhead(pred_class)\n## [1] neg neg pos neg pos pos\n## Levels: pos neg\n\n# 预测概率\npred_prob &lt;- fit[[\"prob\"]]\nhead(pred_prob)\n##            pos       neg\n## [1,] 0.3860350 0.6139650\n## [2,] 0.3440688 0.6559312\n## [3,] 1.0000000 0.0000000\n## [4,] 0.2768308 0.7231692\n## [5,] 0.6679614 0.3320386\n## [6,] 0.5939196 0.4060804\n\n而且这个包的结果给出了两种类别的概率，不用再自己计算了。\n\n\n14.4.2 模型评价\n首先还是借助caret包查看混淆矩阵等各种信息：\n\ncaret::confusionMatrix(pred_class,truth_test)\n## Confusion Matrix and Statistics\n## \n##           Reference\n## Prediction pos neg\n##        pos 131  34\n##        neg  19  47\n##                                           \n##                Accuracy : 0.7706          \n##                  95% CI : (0.7109, 0.8232)\n##     No Information Rate : 0.6494          \n##     P-Value [Acc &gt; NIR] : 4.558e-05       \n##                                           \n##                   Kappa : 0.4738          \n##                                           \n##  Mcnemar's Test P-Value : 0.05447         \n##                                           \n##             Sensitivity : 0.8733          \n##             Specificity : 0.5802          \n##          Pos Pred Value : 0.7939          \n##          Neg Pred Value : 0.7121          \n##              Prevalence : 0.6494          \n##          Detection Rate : 0.5671          \n##    Detection Prevalence : 0.7143          \n##       Balanced Accuracy : 0.7268          \n##                                           \n##        'Positive' Class : pos             \n## \n\n然后是绘制ROC曲线，完全一样的代码：\n\nlibrary(ROCR)\n\npred &lt;- prediction(pred_prob[,1],truth_test) # 预测概率，真实类别\nperf &lt;- performance(pred, \"tpr\",\"fpr\")\nauc &lt;- round(performance(pred, \"auc\")@y.values[[1]],digits = 4)\nauc\n## [1] 0.8491\n\nplot(perf,lwd=2,col=\"tomato\")\nabline(0,1,lty=2)\nlegend(\"bottomright\", legend=paste(\"AUC: \",auc), col=\"tomato\", lwd=2,bty = \"n\")\n\n\n\n\n\n\n\n\neasy！一样的用法，基本没啥变化，所以pROC的画法就不再重复了，大家想要学习的就自己写一下即可。\nPR曲线、校准曲线、决策曲线也是一样的简单，就不重复了。\n\n\n14.4.3 超参数调优\n借助for循环也可以，这里再给大家演示下如何使用e1071包实现轻量化的超参数调优。\n\nlibrary(e1071)\n\nset.seed(123)\ntune.knn(x=train[,-9], # 预测变量\n         y=truth_train,# 结果变量\n         k=1:50        # k的值\n         )\n## \n## Parameter tuning of 'knn.wrapper':\n## \n## - sampling method: 10-fold cross validation \n## \n## - best parameters:\n##   k\n##  25\n## \n## - best performance: 0.2198113\n\n1行代码出结果，默认是使用10折交叉验证，比我们的手动for循环更加稳健，结果最佳的k值是25。使用的评价指标不同，具体计算的步骤也不一样，得出的结果不一样是很正常的。\n如果你想用caret、tidymodels、mlr3实现，那就需要学习它们的语法了，后台回复关键词即可获取合集教程。",
    "crumbs": [
      "代码实战",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>K最近邻</span>"
    ]
  },
  {
    "objectID": "支持向量机.html",
    "href": "支持向量机.html",
    "title": "15  支持向量机",
    "section": "",
    "text": "15.1 算法简介\n支持向量机的原理非常复杂，我这里尽量用简单的话说明。还是用前一章中的二分类问题为例进行说明。\n下图有两个特征可以用来预测肿瘤是”良性”还是”恶性”，SVM的关键就是找到一条完美的线（或者平面）把它分成两类，下图中展示的实线和虚线都可以做到这一点，但这只是无数条线其中的两条而已。\n如果数据的维度多于2，那就不是一条线能解决的了，那就需要一个平面，这个平面就被称为超平面，\n如果一个超平面能够把数据分为两类，每类只包含一种类别，那么这个超平面就是数据的决策边界。比如上图中的实线和虚线就是两个决策边界。\n如上图所示，我们可以找到多个决策边界把数据分为两类，但是哪一个才是最好的呢？\n如下左图所示，我们把其中一条决策边界（比如实线）往左右两边移动，当碰到第一个数据点时停下来，此时两条实线之间的距离就被称为（中间）实线这条决策边界的边际（margin）。\n支持向量机就是要找到使得边际最大的决策边界来实现对数据的分类。如上右图所示的边际就明显小于作图所示的边际。\n如果这条决策边界非常完美，能够把所有的样本完美分成两类，那么此时模型的误差就是0，因为完全分类正确，但是此时模型对新数据的预测可能会出现较大的误差，也就是说出现了过拟合。\n上图中在左右边界上的样本被称为支持向量（support vector）。\n实际生活中很多数据并不是完全线性可分的（上面这个例子是线性可分的），为了能够适用于非线性的数据，支持向量机可以将原始数据进行各种转换，这就是核技巧（kernel trick），或者叫核函数（kernel function）。通过使用的不同的核技巧，支持向量机可以适用于多种不同类型的数据。",
    "crumbs": [
      "代码实战",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>支持向量机</span>"
    ]
  },
  {
    "objectID": "支持向量机.html#加载r包",
    "href": "支持向量机.html#加载r包",
    "title": "15  支持向量机",
    "section": "15.2 加载R包",
    "text": "15.2 加载R包\nSVM有多种实现方法，最著名的就是台湾林智仁教授开发的LIBSVM了，python中的sklearn中的支持向量机就是使用的这种方法，在R语言中是通过e1071这个包实现的。\n该包还可以实现缺失值插补、异常值检测、计算偏度/峰度、多种算法的超参数调优（决策树、随机森林、knn、神经网络等）等，功能非常强大。\n\nlibrary(e1071)\n#example(\"svm\")",
    "crumbs": [
      "代码实战",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>支持向量机</span>"
    ]
  },
  {
    "objectID": "支持向量机.html#参数解释",
    "href": "支持向量机.html#参数解释",
    "title": "15  支持向量机",
    "section": "15.3 参数解释",
    "text": "15.3 参数解释\n支持向量机在e1071中主要是通过svm()这个函数实现的。既可以使用公式语法，也可以使用x/y的形式，此时如果因变量y是因子型，则函数默认这是一个分类任务，如果y不是因子型则默认为回归任务，如果不提供y，则默认是异常值检测。\nsvm(x, y = NULL, scale = TRUE, type = NULL, kernel = \"radial\", degree = 3, \n    gamma = if (is.vector(x)) 1 else 1 / ncol(x),\n    coef0 = 0, cost = 1, nu = 0.5, class.weights = NULL, cachesize = 40, \n    tolerance = 0.001, epsilon = 0.1, shrinking = TRUE, cross = 0, \n    probability = FALSE, fitted = TRUE, ..., subset, na.action = na.omit)\n部分参数介绍：\n\nscale：是否进行标准化，默认是TRUE\ntype：任务类型，比如回归还是分类，具体可选以下几种：C-classification，nu-classification，one-classification (for novelty detection)，eps-regression，nu-regression\nkernel：e1071中的核函数有4种，每个核函数都有最适合的情况，但通常都会选择径向基核函数。除了线性核之外，其余3种都有属于自己的超参数：\n\nlinear：线性核函数\npolynomial：多项式核函数，(3个超参数:gamma, degree, coef0)\nradial basis：RBF，径向基核函数，(1个超参数: gamma)\nsigmoid： (2个超参数:gamma, coef0)\n\ndegree：多项式核的阶数，默认是3，如果是1就类似于线性核\ngamma：默认是1/特征数量，取值范围是0到正无穷，一般在优化这个参数时会进行log转换，gamma越大，通常导致支持向量越多\ncoef0：多项式核的参数，默认是0，需要整数\ncost：正则化程度，或者表示犯错的成本，类似于sklearn中的C，正整数。一个较大的成本意味着模型对误差的惩罚更大，从而将生成一个更复杂的分类边界，对应的训练集中的误差也会更小（也就是边际大），但也意味着可能存在过拟合问题，即对新样本单元的预测误差可能很大。较小的成本意味着分类边界更平滑，但可能会导致欠拟合\nclass.weight：因变量的权重，比如svm(x, y, class.weights = c(A = 0.3, B = 0.7))，指定inverse会直接使用和类别比例相反的类别权重\ncachesize：使用的内存大小，默认40MB\ncross：交叉验证的折数，默认是0，不进行交叉验证。如果使用了交叉验证，结果中会多出一些额外信息，回归任务会多出：\n\naccuracies：每一折的准确率；\ntot.accuracy:平均准确率。 分类任务会多出：\nMSE:每一折的MSE；\ntot.MSE：平均MSE；\nscorrcoef:相关系数的平方\n\nprobability:是否生成各个类别的概率\nfitted：是否生成分类结果\n\n这个核函数的概念非常重要，所以我做了一个表格：\n\n\n\n\n\n\n\n\n\n核函数\n中文\n适用范围\n参数\n\n\n\n\nlinear\n线性核\n线性数据\n无\n\n\npolynomial\n多项式核\n偏向于线性数据\ngamma/degree/coef0\n\n\nradial basis\nRBF,高斯径向基核\n偏向于非线性数据\ngamma\n\n\nsigmoid\nsigmoid核\n非线性数据\ngamma/coef0\n\n\n\ngamma/degree/coef0这3个参数对SVM的影响非常复杂，因为并不是完全的正相关或者负相关关系，尤其是对多项式核、sigmoid核来说，多个参数会共同影响，关系就更加复杂了，所以通常此时我们会使用网格搜索等方法确定这些参数的值。对于RBF核来说，它只有一个参数gamma，此时我们可以用学习曲线来确定最佳的gamma值。",
    "crumbs": [
      "代码实战",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>支持向量机</span>"
    ]
  },
  {
    "objectID": "支持向量机.html#建立模型",
    "href": "支持向量机.html#建立模型",
    "title": "15  支持向量机",
    "section": "15.4 建立模型",
    "text": "15.4 建立模型\n演示数据就用R语言自带的鸢尾花数据集，这个数据集是一个3分类的数据，其中Species是结果变量，为3分类，其余变量是预测变量。\n\nrm(list = ls())\ndata(iris)\n\n# 公式形式\nmodel &lt;- svm(Species ~ ., data = iris, probability = T)\n\n# 或者x/y形式\nx &lt;- subset(iris, select = -Species)\ny &lt;- iris$Species\n#model &lt;- svm(x, y, probability = T) \n\nprint(model)\n## \n## Call:\n## svm(formula = Species ~ ., data = iris, probability = T)\n## \n## \n## Parameters:\n##    SVM-Type:  C-classification \n##  SVM-Kernel:  radial \n##        cost:  1 \n## \n## Number of Support Vectors:  51\nsummary(model)\n## \n## Call:\n## svm(formula = Species ~ ., data = iris, probability = T)\n## \n## \n## Parameters:\n##    SVM-Type:  C-classification \n##  SVM-Kernel:  radial \n##        cost:  1 \n## \n## Number of Support Vectors:  51\n## \n##  ( 8 22 21 )\n## \n## \n## Number of Classes:  3 \n## \n## Levels: \n##  setosa versicolor virginica",
    "crumbs": [
      "代码实战",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>支持向量机</span>"
    ]
  },
  {
    "objectID": "支持向量机.html#查看结果",
    "href": "支持向量机.html#查看结果",
    "title": "15  支持向量机",
    "section": "15.5 查看结果",
    "text": "15.5 查看结果\n查看支持向量：\n\n# 51个支持向量，这是预处理(比如na.omit)后的序号\nmodel$index\n##  [1]   9  14  16  21  23  24  26  42  51  53  54  55  57  58  60  61  64  67  69\n## [20]  71  73  77  78  79  84  85  86  87  88  99 107 109 111 117 119 120 122 124\n## [39] 126 127 128 130 132 134 135 138 139 143 147 149 150\n\n查看预测类别和预测概率，只有在建模时使用了probability=T，才能在predict()时使用此参数：\n\n# 获取训练集预测结果\n# newdata写测试集就是测试集结果\npred &lt;- predict(model, newdata = x, probability = T)\n\n# 查看结果\nhead(pred)\n##      1      2      3      4      5      6 \n## setosa setosa setosa setosa setosa setosa \n## Levels: setosa versicolor virginica\n\n# 查看预测概率\nhead(attr(pred,\"probabilities\"))\n##      setosa versicolor   virginica\n## 1 0.9816955 0.01075790 0.007546567\n## 2 0.9746218 0.01725092 0.008127315\n## 3 0.9804885 0.01135611 0.008155367\n## 4 0.9766379 0.01459413 0.008767961\n## 5 0.9809263 0.01108326 0.007990473\n## 6 0.9756922 0.01600990 0.008297935\n\n查看混淆矩阵：\n\n# 混淆矩阵\ntable(pred, y)\n##             y\n## pred         setosa versicolor virginica\n##   setosa         50          0         0\n##   versicolor      0         48         2\n##   virginica       0          2        48\n\n# 或者caret版混淆矩阵\n#caret::confusionMatrix(table(pred, y))\ncaret::confusionMatrix(pred, y)\n## Confusion Matrix and Statistics\n## \n##             Reference\n## Prediction   setosa versicolor virginica\n##   setosa         50          0         0\n##   versicolor      0         48         2\n##   virginica       0          2        48\n## \n## Overall Statistics\n##                                           \n##                Accuracy : 0.9733          \n##                  95% CI : (0.9331, 0.9927)\n##     No Information Rate : 0.3333          \n##     P-Value [Acc &gt; NIR] : &lt; 2.2e-16       \n##                                           \n##                   Kappa : 0.96            \n##                                           \n##  Mcnemar's Test P-Value : NA              \n## \n## Statistics by Class:\n## \n##                      Class: setosa Class: versicolor Class: virginica\n## Sensitivity                 1.0000            0.9600           0.9600\n## Specificity                 1.0000            0.9800           0.9800\n## Pos Pred Value              1.0000            0.9600           0.9600\n## Neg Pred Value              1.0000            0.9800           0.9800\n## Prevalence                  0.3333            0.3333           0.3333\n## Detection Rate              0.3333            0.3200           0.3200\n## Detection Prevalence        0.3333            0.3333           0.3333\n## Balanced Accuracy           1.0000            0.9700           0.9700\n\n三分类也是可以绘制ROC曲线的，只不过稍微复杂一点，大家有需要的可以参考ROC曲线绘制合集",
    "crumbs": [
      "代码实战",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>支持向量机</span>"
    ]
  },
  {
    "objectID": "支持向量机.html#可视化决策边界",
    "href": "支持向量机.html#可视化决策边界",
    "title": "15  支持向量机",
    "section": "15.6 可视化决策边界",
    "text": "15.6 可视化决策边界\n如果预测变量超过1个就要使用formula，因为只能在两个维度上画出这个图，slice=list(Sepal.Width=3,Sepal.Length=4)表示要把Sepal.Width这一列都当成3，把Sepal.Length这一列都变成4，这个数值具体怎么影响这幅图也没找到合适的解释，希望有大佬能指点迷津。\n\nplot(model, data = iris, formula = Petal.Width ~ Petal.Length,\n     slice = list(Sepal.Width = 3, Sepal.Length = 4),\n     svSymbol = \"x\" # 支持向量的形状\n     )\n\n\n\n\n\n\n\n\n图中的x表示支持向量，o表示数据点。",
    "crumbs": [
      "代码实战",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>支持向量机</span>"
    ]
  },
  {
    "objectID": "支持向量机.html#不同核函数比较",
    "href": "支持向量机.html#不同核函数比较",
    "title": "15  支持向量机",
    "section": "15.7 不同核函数比较",
    "text": "15.7 不同核函数比较\n我们把4种核函数的结果都画出来，放在一起比较下决策边界。\n\nmodel_rbf &lt;- svm(Species ~ ., data = iris, kernel = \"radial\")#径向基核\nmodel_linear &lt;- svm(Species ~ ., data = iris, kernel = \"linear\")#线性核\nmodel_ploy &lt;- svm(Species ~ ., data = iris, kernel = \"polynomial\")#多项式核\nmodel_sig &lt;- svm(Species ~ ., data = iris, kernel = \"sigmoid\")#sigmoid核\n\npar(mfrow=c(2,2))\nplot(model_rbf, data = iris, formula = Petal.Width ~ Petal.Length,\n     slice = list(Sepal.Width = 3, Sepal.Length = 4))\n\n\n\n\n\n\n\nplot(model_linear, data = iris, formula = Petal.Width ~ Petal.Length,\n     slice = list(Sepal.Width = 3, Sepal.Length = 4))\n\n\n\n\n\n\n\nplot(model_ploy, data = iris, formula = Petal.Width ~ Petal.Length,\n     slice = list(Sepal.Width = 3, Sepal.Length = 4))\n\n\n\n\n\n\n\nplot(model_sig, data = iris, formula = Petal.Width ~ Petal.Length,\n     slice = list(Sepal.Width = 3, Sepal.Length = 4))",
    "crumbs": [
      "代码实战",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>支持向量机</span>"
    ]
  },
  {
    "objectID": "支持向量机.html#超参数调优",
    "href": "支持向量机.html#超参数调优",
    "title": "15  支持向量机",
    "section": "15.8 超参数调优",
    "text": "15.8 超参数调优\n使用默认的tune.svm()调整超参数，我们就用常见的径向基核为例进行演示，我们同时调整2个超参数：gamma和cost。\n\nset.seed(123)\ntune_model &lt;- tune.svm(Species ~ ., data = iris,\n                       cost = 10^(-1:3), # 设置cost的值\n                       gamma = 10^(-3:1), # 设置gamma的值\n                       \n                       # 重抽样方法选择自助法，次数选择100次\n                       tunecontrol=tune.control(sampling = \"bootstrap\",\n                                                nboot = 100\n                                                )#自助法\n                       )\n\ntune_model\n## \n## Parameter tuning of 'svm':\n## \n## - sampling method: bootstrapping \n## \n## - best parameters:\n##  gamma cost\n##   0.01  100\n## \n## - best performance: 0.03924088\n\n错误率最低是0.03924088，此时gamma=0.01，cost=100。\n使用这个超参数重新拟合模型：\n\nmodel_final &lt;- svm(Species ~ ., data = iris, cost=100, gamma=0.01)\nprint(model_final)\n## \n## Call:\n## svm(formula = Species ~ ., data = iris, cost = 100, gamma = 0.01)\n## \n## \n## Parameters:\n##    SVM-Type:  C-classification \n##  SVM-Kernel:  radial \n##        cost:  100 \n## \n## Number of Support Vectors:  24\n\n# 预测新数据\ntest_df &lt;- head(iris)\npred &lt;- predict(model, newdata = test_df)\ncaret::confusionMatrix(pred, test_df$Species)\n## Confusion Matrix and Statistics\n## \n##             Reference\n## Prediction   setosa versicolor virginica\n##   setosa          6          0         0\n##   versicolor      0          0         0\n##   virginica       0          0         0\n## \n## Overall Statistics\n##                                      \n##                Accuracy : 1          \n##                  95% CI : (0.5407, 1)\n##     No Information Rate : 1          \n##     P-Value [Acc &gt; NIR] : 1          \n##                                      \n##                   Kappa : NaN        \n##                                      \n##  Mcnemar's Test P-Value : NA         \n## \n## Statistics by Class:\n## \n##                      Class: setosa Class: versicolor Class: virginica\n## Sensitivity                      1                NA               NA\n## Specificity                     NA                 1                1\n## Pos Pred Value                  NA                NA               NA\n## Neg Pred Value                  NA                NA               NA\n## Prevalence                       1                 0                0\n## Detection Rate                   1                 0                0\n## Detection Prevalence             1                 0                0\n## Balanced Accuracy               NA                NA               NA\n\n可视化决策边界：\n\nplot(model_final, data = iris, formula = Petal.Width ~ Petal.Length,\n     slice = list(Sepal.Width = 3, Sepal.Length = 4))",
    "crumbs": [
      "代码实战",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>支持向量机</span>"
    ]
  },
  {
    "objectID": "支持向量机.html#使用建议",
    "href": "支持向量机.html#使用建议",
    "title": "15  支持向量机",
    "section": "15.9 使用建议",
    "text": "15.9 使用建议\n\nSVM对超参数值很敏感，建议多试几次；\n对于分类任务，首选C-classification和RBF(default)，表现好且超参数比较少，只有cost和gamma。libsvm的作者建议先对cost用一个很大的范围，比如1~1000，然后用交叉验证选择比较好的，最后用这几个选中的cost值再尝试不同的gamma；\n可以使用网格搜索实现超参数调优，tune.svm()；\n对数据进行标准化会提高模型表现，强烈建议，svm()默认会对数据进行标准化。",
    "crumbs": [
      "代码实战",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>支持向量机</span>"
    ]
  },
  {
    "objectID": "支持向量机.html#其他",
    "href": "支持向量机.html#其他",
    "title": "15  支持向量机",
    "section": "15.10 其他",
    "text": "15.10 其他\n除了支持向量机外，e1071包还支持对其他模型进行超参数调优，包括：\n\ntune.gknn()：generalized k-nearest neighbors\ntune.knn()：knn\ntune.nnet()：nnet，神经网络\ntune.randomForest()：随机森林\ntune.rpart()：决策树\n\n这几个函数都是tune()的变体。每种算法可以调优的超参数都在帮助文档中有详细的说明，作为一个轻量化的调优R包来用是非常不错的选择，虽然我们有更好的方法。\n支持向量机的递归特征消除，可参考推文：递归特征消除",
    "crumbs": [
      "代码实战",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>支持向量机</span>"
    ]
  },
  {
    "objectID": "支持向量机.html#参考资料",
    "href": "支持向量机.html#参考资料",
    "title": "15  支持向量机",
    "section": "15.11 参考资料",
    "text": "15.11 参考资料\n\nexample(“svm”)\nhttps://stackoverflow.com/questions/49877752/slice-option-in-plot-function-of-package-e1071\nhttps://c3h3notes.wordpress.com/2010/10/20/r%E4%B8%8A%E7%9A%84libsvm-package-e1071\nhttps://www.datacamp.com/tutorial/support-vector-machines-r\nhttps://rpubs.com/cliex159/865583\n菜菜的sklearn课堂",
    "crumbs": [
      "代码实战",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>支持向量机</span>"
    ]
  },
  {
    "objectID": "支持向量机核函数比较.html",
    "href": "支持向量机核函数比较.html",
    "title": "16  SVM核函数比较",
    "section": "",
    "text": "16.1 加载数据和R包\n使用e1071包做演示。数据使用印第安人糖尿病数据集。\nlibrary(e1071)\nlibrary(pROC)\nlibrary(dplyr)\n\nrm(list = ls())\n\nload(file = \"datasets/pimadiabetes.rdata\")",
    "crumbs": [
      "代码实战",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>SVM核函数比较</span>"
    ]
  },
  {
    "objectID": "支持向量机核函数比较.html#数据划分",
    "href": "支持向量机核函数比较.html#数据划分",
    "title": "16  SVM核函数比较",
    "section": "16.2 数据划分",
    "text": "16.2 数据划分\n划分训练集和测试集，经典7：3分：\n\n# 划分是随机的，设置种子数可以让结果复现\nset.seed(123)\nind &lt;- sample(1:nrow(pimadiabetes), size = 0.7*nrow(pimadiabetes))\n\n# 训练集、测试集\ntrain &lt;- pimadiabetes[ind,]\ntest &lt;- pimadiabetes[-ind, ]",
    "crumbs": [
      "代码实战",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>SVM核函数比较</span>"
    ]
  },
  {
    "objectID": "支持向量机核函数比较.html#训练集建模",
    "href": "支持向量机核函数比较.html#训练集建模",
    "title": "16  SVM核函数比较",
    "section": "16.3 训练集建模",
    "text": "16.3 训练集建模\ne1071使用起来非常简单，直接一个函数搞定，也是使用R语言经典的formula写法，二分类数据我们通常希望获得预测概率，所以加上probability = TRUE\n然后kernel参数就是分别用4种核函数。\n\nset.seed(123)\nsvmLinear &lt;- svm(diabetes ~ ., data = train,\n                 probability = TRUE,\n                 kernel=\"linear\"\n                 )\n\nsvmPoly &lt;- svm(diabetes ~ ., data = train,\n               probability = TRUE,\n               kernel=\"polynomial\"\n               )\n\nsvmRadial &lt;- svm(diabetes ~ ., data = train,\n                 probability = TRUE,\n                 kernel=\"radial\"\n                 )\n\nsvmSigmoid &lt;- svm(diabetes ~., data = train,\n                  probability = TRUE,\n                  kernel=\"sigmoid\"\n                  )\n\n接下来就是查看模型在训练集中的表现，我们为了少写几行代码，先定义一个函数，可以自定帮我们提取训练结果，并组成1个数据框，内含原始数据的结果变量，预测结果，预测概率。\n\n# 定义函数\ngetres &lt;- function(svmfunc, dataset){\n  data_pred &lt;- predict(svmfunc, newdata=dataset, probability = T)\n  data_pred_df &lt;- dataset %&gt;% dplyr::select(diabetes) %&gt;% \n  dplyr::bind_cols(status_pred = data_pred) %&gt;% \n  dplyr::bind_cols(attr(data_pred, \"probabilities\"))\n}\n\n接下来提取数据即可，我们先提取1个看看：\n\nLinear_train_pred_df &lt;- getres(svmLinear, train)\nhead(Linear_train_pred_df)\n##     diabetes status_pred        neg       pos\n## 415      neg         pos 0.36393796 0.6360620\n## 463      pos         pos 0.16223183 0.8377682\n## 179      pos         neg 0.73073668 0.2692633\n## 526      pos         pos 0.04261936 0.9573806\n## 195      pos         pos 0.08214236 0.9178576\n## 118      pos         pos 0.12508113 0.8749189\n\n上面这个是：线性核函数，训练集，的结果，看起来没什么问题，第一列是真实结果变量，第2列是预测结果类别，第3和4列是预测的类别概率。\n如果你想看看混淆矩阵，可以借助caret包实现：\n\ncaret::confusionMatrix(Linear_train_pred_df$diabetes,\n                       Linear_train_pred_df$status_pred,\n                       mode = \"everything\")\n## Confusion Matrix and Statistics\n## \n##           Reference\n## Prediction pos neg\n##        pos 312  38\n##        neg  82 105\n##                                           \n##                Accuracy : 0.7765          \n##                  95% CI : (0.7389, 0.8111)\n##     No Information Rate : 0.7337          \n##     P-Value [Acc &gt; NIR] : 0.01292         \n##                                           \n##                   Kappa : 0.4792          \n##                                           \n##  Mcnemar's Test P-Value : 8.661e-05       \n##                                           \n##             Sensitivity : 0.7919          \n##             Specificity : 0.7343          \n##          Pos Pred Value : 0.8914          \n##          Neg Pred Value : 0.5615          \n##               Precision : 0.8914          \n##                  Recall : 0.7919          \n##                      F1 : 0.8387          \n##              Prevalence : 0.7337          \n##          Detection Rate : 0.5810          \n##    Detection Prevalence : 0.6518          \n##       Balanced Accuracy : 0.7631          \n##                                           \n##        'Positive' Class : pos             \n## \n\n内容非常全面，我们就不解读了。\n我们直接把剩下的核函数在训练集、测试集中的结果都提取出来，方便接下来使用。\n\n# 提取4种核函数分别在训练集、测试集的结果\nLinear_test_pred_df &lt;- getres(svmLinear, test)\nPoly_train_pred_df &lt;- getres(svmPoly, train)\nPoly_test_pred_df &lt;- getres(svmPoly, test)\n\nRadial_train_pred_df &lt;- getres(svmRadial, train)\nRadial_test_pred_df &lt;- getres(svmRadial, test)\n\nSigmoid_train_pred_df &lt;- getres(svmSigmoid, train)\nSigmoid_test_pred_df &lt;- getres(svmSigmoid, test)\n\n接下来又是大家喜闻乐见的画图环节，就选大家最喜欢的ROC曲线吧。\n关于这个ROC曲线，我一共写了十几篇推文，应该是全面覆盖了，大家还不会的去翻历史推文吧。\n其实这里你也可以写个函数哈，大神们都说只要重复超过3遍的都建议写函数实现…\n\n# 首先构建训练集中4个ROC对象\nroc_train_linear &lt;- roc(Linear_train_pred_df$diabetes, \n                        Linear_train_pred_df$pos,\n                        auc=T\n                        )\n## Setting levels: control = pos, case = neg\n## Setting direction: controls &gt; cases\n\nroc_train_Poly &lt;- roc(Poly_train_pred_df$diabetes, \n                      Poly_train_pred_df$pos,\n                      auc=T\n                        )\n## Setting levels: control = pos, case = neg\n## Setting direction: controls &gt; cases\n\nroc_train_Radial &lt;- roc(Radial_train_pred_df$diabetes, \n                        Radial_train_pred_df$pos,\n                        auc=T\n                        )\n## Setting levels: control = pos, case = neg\n## Setting direction: controls &gt; cases\n\nroc_train_Sigmoid &lt;- roc(Sigmoid_train_pred_df$diabetes, \n                        Sigmoid_train_pred_df$pos,\n                        auc=T\n                        )\n## Setting levels: control = pos, case = neg\n## Setting direction: controls &gt; cases\n\n然后我们准备4种颜色，这种小代码，建议大家记住，因为使用很高频，它可以直接给你十六进制颜色代码，复制粘贴就可以使用了！\n\nRColorBrewer::brewer.pal(4,\"Set1\")\n## [1] \"#E41A1C\" \"#377EB8\" \"#4DAF4A\" \"#984EA3\"\n\n然后就是把训练集中，4种核函数的ROC曲线画在1张图上：\n\nplot.roc(Linear_train_pred_df$diabetes, \n         Linear_train_pred_df$pos,\n         col=\"#1c61b6\",legacy=T,lwd=2)\n## Setting levels: control = pos, case = neg\n## Setting direction: controls &gt; cases\nlines.roc(Poly_train_pred_df$diabetes, \n          Poly_train_pred_df$pos, col=\"#008600\")\n## Setting levels: control = pos, case = neg\n## Setting direction: controls &gt; cases\nlines.roc(Radial_train_pred_df$diabetes, \n          Radial_train_pred_df$pos, col=\"#E41A1C\")\n## Setting levels: control = pos, case = neg\n## Setting direction: controls &gt; cases\nlines.roc(Sigmoid_train_pred_df$diabetes, \n          Sigmoid_train_pred_df$pos, col=\"#984EA3\")\n## Setting levels: control = pos, case = neg\n## Setting direction: controls &gt; cases\n\nlegend(\"bottomright\", \n       legend=c(paste0(\"train_svmLinear AUC: \",round(roc_train_linear[[\"auc\"]],3)),\n                paste0(\"train_svmPoly AUC: \",round(roc_train_Poly[[\"auc\"]],3)),\n                paste0(\"train_svmRadial AUC: \",round(roc_train_Radial[[\"auc\"]],3)),\n                paste0(\"train_svmSigmoid AUC: \",round(roc_train_Sigmoid[[\"auc\"]],3))\n                ),\n       col=c(\"#1c61b6\", \"#008600\",\"#E41A1C\",\"#984EA3\"), \n       lwd=2)\n\n\n\n\n\n\n\n\neasy！看着还行。果然是高斯径向基核函数最牛逼，堪称万金油！",
    "crumbs": [
      "代码实战",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>SVM核函数比较</span>"
    ]
  },
  {
    "objectID": "支持向量机核函数比较.html#测试集",
    "href": "支持向量机核函数比较.html#测试集",
    "title": "16  SVM核函数比较",
    "section": "16.4 测试集",
    "text": "16.4 测试集\n测试集的数据已经提取好了，直接用即可。还是写个函数吧….\n\n# 构建测试集中4个ROC对象\nroc_test &lt;- lapply(list(Linear_test_pred_df,Poly_test_pred_df,\n                        Radial_test_pred_df,Sigmoid_test_pred_df), function(x){\n                          roc_res &lt;- roc(x$diabetes, x$pos,auc=T)\n                          }\n                   )\n## Setting levels: control = pos, case = neg\n## Setting direction: controls &gt; cases\n## Setting levels: control = pos, case = neg\n## Setting direction: controls &gt; cases\n## Setting levels: control = pos, case = neg\n## Setting direction: controls &gt; cases\n## Setting levels: control = pos, case = neg\n## Setting direction: controls &gt; cases\nroc_test[[1]]\n## \n## Call:\n## roc.default(response = x$diabetes, predictor = x$pos, auc = T)\n## \n## Data: x$pos in 150 controls (x$diabetes pos) &gt; 81 cases (x$diabetes neg).\n## Area under the curve: 0.8414\n\n然后把测试集中，4种核函数的ROC曲线画在一起：\n\nplot.roc(Linear_test_pred_df$diabetes, \n         Linear_test_pred_df$pos,\n         col=\"#1c61b6\",legacy=T)\n## Setting levels: control = pos, case = neg\n## Setting direction: controls &gt; cases\nlines.roc(Poly_test_pred_df$diabetes, \n          Poly_test_pred_df$pos, col=\"#008600\")\n## Setting levels: control = pos, case = neg\n## Setting direction: controls &gt; cases\nlines.roc(Radial_test_pred_df$diabetes, \n          Radial_test_pred_df$pos, col=\"#E41A1C\")\n## Setting levels: control = pos, case = neg\n## Setting direction: controls &gt; cases\nlines.roc(Sigmoid_test_pred_df$diabetes, \n          Sigmoid_test_pred_df$pos, col=\"#984EA3\")\n## Setting levels: control = pos, case = neg\n## Setting direction: controls &gt; cases\n\nlegend(\"bottomright\", \n       legend=c(paste0(\"test_svmLinear AUC: \",round(roc_test[[1]][[\"auc\"]],3)),\n                paste0(\"test_svmPoly AUC: \",round(roc_test[[2]][[\"auc\"]],3)),\n                paste0(\"test_svmRadial AUC: \",round(roc_test[[3]][[\"auc\"]],3)),\n                paste0(\"test_svmSigmoid AUC: \",round(roc_test[[4]][[\"auc\"]],3))\n                ),\n       col=c(\"#1c61b6\", \"#008600\",\"#E41A1C\",\"#984EA3\"), \n       lwd=2)\n\n\n\n\n\n\n\n\n结果看起来还不错哦。",
    "crumbs": [
      "代码实战",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>SVM核函数比较</span>"
    ]
  },
  {
    "objectID": "决策树.html",
    "href": "决策树.html",
    "title": "17  决策树",
    "section": "",
    "text": "17.1 算法原理\n决策树（decision tree）是一种有监督的学习方法，它通过不断的根据条件进行分支来解决分类和回归问题。\n下面是一个决策树工作原理的简单示意图，通过多个条件，一层一层的判断某个西瓜是否是“好瓜”还是“坏瓜”：\n决策树的结构就像一棵树，通过不断的询问是或者否来对样本进行预测。\n最开始进行分支的问题被称为根节点（root node），上图中的脐部=？就是根节点。最终的结论，也就是“好瓜”、“坏瓜”是叶子节点（leaf node）或者叫终末节点（terminal node），中间的每个问题都是中间节点。\n在上下两个相邻的节点中，更靠近根节点的是父节点（parent node），另一个是子节点（child node）。\n对于决策树模型来说，数据中的任何一个预测变量都可以当做是其中一个中间节点，可以不断的进行分裂，这样的好处是最终每一个样本都可以得到完美的结果，也就是误差会非常小，但是这样做一个很明显的问题就是容易使模型过拟合。\n根据具体的实现方式，决策树包括多种方法，比如：ID3、C4.5、CART。\n本文会介绍决策树的R语言实现和超参数调优。我会介绍决策树常见的超参数以及它们的意义，超参数调优的常见方法和调优思路，比如学习曲线、网格搜索等。",
    "crumbs": [
      "代码实战",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>决策树</span>"
    ]
  },
  {
    "objectID": "决策树.html#准备数据和r包",
    "href": "决策树.html#准备数据和r包",
    "title": "17  决策树",
    "section": "17.2 准备数据和R包",
    "text": "17.2 准备数据和R包\n演示数据为印第安人糖尿病数据集，这个数据一共有768行，9列，其中diabetes是结果变量，为二分类，其余列是预测变量。\n\nrm(list = ls())\nlibrary(rpart)\nlibrary(rpart.plot)\n\nload(file = \"datasets/pimadiabetes.rdata\")\n\ndim(pimadiabetes)\n## [1] 768   9\nstr(pimadiabetes)\n## 'data.frame':    768 obs. of  9 variables:\n##  $ pregnant: num  6 1 8 1 0 5 3 10 2 8 ...\n##  $ glucose : num  148 85 183 89 137 116 78 115 197 125 ...\n##  $ pressure: num  72 66 64 66 40 ...\n##  $ triceps : num  35 29 22.9 23 35 ...\n##  $ insulin : num  202.2 64.6 217.1 94 168 ...\n##  $ mass    : num  33.6 26.6 23.3 28.1 43.1 ...\n##  $ pedigree: num  0.627 0.351 0.672 0.167 2.288 ...\n##  $ age     : num  50 31 32 21 33 30 26 29 53 54 ...\n##  $ diabetes: Factor w/ 2 levels \"pos\",\"neg\": 2 1 2 1 2 1 2 1 2 2 ...\n\n划分训练集、测试集。训练集用于建立模型，测试集用于测试模型表现，划分比例为7:3。\n\n# 划分是随机的，设置种子数可以让结果复现\nset.seed(123)\nind &lt;- sample(1:nrow(pimadiabetes), size = 0.7*nrow(pimadiabetes))\n\n# 去掉真实结果列\ntrain &lt;- pimadiabetes[ind,]\ntest &lt;- pimadiabetes[-ind,]\n\ndim(train)\n## [1] 537   9\ndim(test)\n## [1] 231   9",
    "crumbs": [
      "代码实战",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>决策树</span>"
    ]
  },
  {
    "objectID": "决策树.html#建立模型",
    "href": "决策树.html#建立模型",
    "title": "17  决策树",
    "section": "17.3 建立模型",
    "text": "17.3 建立模型\n首先在训练集上建立模型，先不进行任何设置，就用默认的，看看效果如何：\n\n# 决策树计算过程中有随机性，需设置种子数保证结果能复现\nset.seed(123)\ntreefit &lt;- rpart(diabetes ~ ., data = train)\ntreefit\n## n= 537 \n## \n## node), split, n, loss, yval, (yprob)\n##       * denotes terminal node\n## \n##   1) root 537 187 pos (0.65176909 0.34823091)  \n##     2) insulin&lt; 124.52 233  22 pos (0.90557940 0.09442060)  \n##       4) glucose&lt; 127 210  10 pos (0.95238095 0.04761905) *\n##       5) glucose&gt;=127 23  11 neg (0.47826087 0.52173913)  \n##        10) mass&lt; 34.5 15   5 pos (0.66666667 0.33333333) *\n##        11) mass&gt;=34.5 8   1 neg (0.12500000 0.87500000) *\n##     3) insulin&gt;=124.52 304 139 neg (0.45723684 0.54276316)  \n##       6) mass&lt; 28.9 67  16 pos (0.76119403 0.23880597)  \n##        12) age&lt; 27.5 21   1 pos (0.95238095 0.04761905) *\n##        13) age&gt;=27.5 46  15 pos (0.67391304 0.32608696)  \n##          26) age&gt;=50.5 23   3 pos (0.86956522 0.13043478) *\n##          27) age&lt; 50.5 23  11 neg (0.47826087 0.52173913)  \n##            54) mass&lt; 25.75 7   1 pos (0.85714286 0.14285714) *\n##            55) mass&gt;=25.75 16   5 neg (0.31250000 0.68750000) *\n##       7) mass&gt;=28.9 237  88 neg (0.37130802 0.62869198)  \n##        14) glucose&lt; 158.5 182  83 neg (0.45604396 0.54395604)  \n##          28) age&lt; 30.5 87  35 pos (0.59770115 0.40229885)  \n##            56) pressure&gt;=73.85 45  13 pos (0.71111111 0.28888889) *\n##            57) pressure&lt; 73.85 42  20 neg (0.47619048 0.52380952)  \n##             114) age&lt; 22.5 11   3 pos (0.72727273 0.27272727) *\n##             115) age&gt;=22.5 31  12 neg (0.38709677 0.61290323)  \n##               230) insulin&lt; 179 20  10 pos (0.50000000 0.50000000)  \n##                 460) insulin&gt;=155.66 12   4 pos (0.66666667 0.33333333) *\n##                 461) insulin&lt; 155.66 8   2 neg (0.25000000 0.75000000) *\n##               231) insulin&gt;=179 11   2 neg (0.18181818 0.81818182) *\n##          29) age&gt;=30.5 95  31 neg (0.32631579 0.67368421)  \n##            58) pedigree&lt; 0.3245 33  16 pos (0.51515152 0.48484848)  \n##             116) insulin&lt; 230.35 26  10 pos (0.61538462 0.38461538) *\n##             117) insulin&gt;=230.35 7   1 neg (0.14285714 0.85714286) *\n##            59) pedigree&gt;=0.3245 62  14 neg (0.22580645 0.77419355) *\n##        15) glucose&gt;=158.5 55   5 neg (0.09090909 0.90909091) *\n\n上面就是一颗建立好的决策树，它给出了每次分支的标准。\n我们可以把结果通过图形的方式画出来，默认的画图：\n\npar(xpd = TRUE)\nplot(treefit,compress = TRUE)\ntext(treefit, use.n = TRUE)\n\n\n\n\n\n\n\n\n并不是很美观，建立好的决策树的结果还可以通过rpart.plot函数画出来：\n\nrpart.plot(treefit)\n\n\n\n\n\n\n\n\n这样的结果一目了然，它告诉我们，第一次分支使用了insulin这个变量，分支的标准是124.5,分支后每个子节点的样本比例也很清楚的显示了。\n这棵树一共分支了8次，树的深度为9层。\n也可以使用partykit进行可视化：\n\nlibrary(partykit)\n\nplot(as.party(treefit))\n\n\n\n\n\n\n\n\n如果你还想要更加花里胡哨的可视化效果，可以试一下我们之前介绍过的treeheatR，效果绝对炫酷：超级炫酷的决策树可视化R包",
    "crumbs": [
      "代码实战",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>决策树</span>"
    ]
  },
  {
    "objectID": "决策树.html#超参数介绍",
    "href": "决策树.html#超参数介绍",
    "title": "17  决策树",
    "section": "17.4 超参数介绍",
    "text": "17.4 超参数介绍\n这是默认情况下建立好的决策树，没有进行任何的超参数调优，这样的一棵树，会自由地“生长”，直到不能再生长为止。\n决策树的超参数有很多，大部分都是和剪枝有关的，通过剪枝参数，可以有效限制树的生长（防止过拟合），这几个参数也是所有和树有关的模型的通用参数，比如随机森林，常见的有：\n\n树的深度：设置树分支的最大深度；\n使用的特征个数：设置建立模型时最多使用几个特征；\n最小分支节点：如果当前节点的样本数少于某个值就不再往下分支了。\n\n常见的是这几个，其他还有很多，需要深入了解可以自行学习。\n这些超参数都可以通过rpart.control函数设置。\n通过预先设定好这些参数的值，达到防止树过度生长的目的，这种叫做预剪枝。\n除此之外，rpart.control函数中提供了cp参数，可以用来实现后剪枝。\n当决策树复杂度超过一定程度后，随着复杂度的提高，模型的分类准确度反而会降低。因此，建立的决策树不宜太复杂，需进行剪枝。该剪枝算法依赖于复杂度参数cp,cp随树复杂度的增加而减小，当增加一个节点引起的分类准确度变化量小于树复杂度变化的cp倍时，则须剪去该节点。故建立一棵既能精确分类，又不过度拟合的决策树的关键是求解一个合适的cp值。一般选择错判率最小值(Xerror)对应的cp值来修剪。\nrpart函数默认是使用十折交叉验证进行计算，也可以通过rpart.control函数进行设置\n\n# 这里都选择了默认值\n\nset.seed(123)\ntreefit &lt;- rpart(diabetes ~ ., data = train,\n                 control = rpart.control(minsplit = 20, # 最小分支节点\n                                         #minbucket = 2, # 分支后最小节点\n                                         maxdepth = 30, # 树的深度\n                                         cp = 0.01, # 复杂度\n                                         xval = 10 # 交叉验证，默认10折\n                                         )\n                 )\ntreefit\n## n= 537 \n## \n## node), split, n, loss, yval, (yprob)\n##       * denotes terminal node\n## \n##   1) root 537 187 pos (0.65176909 0.34823091)  \n##     2) insulin&lt; 124.52 233  22 pos (0.90557940 0.09442060)  \n##       4) glucose&lt; 127 210  10 pos (0.95238095 0.04761905) *\n##       5) glucose&gt;=127 23  11 neg (0.47826087 0.52173913)  \n##        10) mass&lt; 34.5 15   5 pos (0.66666667 0.33333333) *\n##        11) mass&gt;=34.5 8   1 neg (0.12500000 0.87500000) *\n##     3) insulin&gt;=124.52 304 139 neg (0.45723684 0.54276316)  \n##       6) mass&lt; 28.9 67  16 pos (0.76119403 0.23880597)  \n##        12) age&lt; 27.5 21   1 pos (0.95238095 0.04761905) *\n##        13) age&gt;=27.5 46  15 pos (0.67391304 0.32608696)  \n##          26) age&gt;=50.5 23   3 pos (0.86956522 0.13043478) *\n##          27) age&lt; 50.5 23  11 neg (0.47826087 0.52173913)  \n##            54) mass&lt; 25.75 7   1 pos (0.85714286 0.14285714) *\n##            55) mass&gt;=25.75 16   5 neg (0.31250000 0.68750000) *\n##       7) mass&gt;=28.9 237  88 neg (0.37130802 0.62869198)  \n##        14) glucose&lt; 158.5 182  83 neg (0.45604396 0.54395604)  \n##          28) age&lt; 30.5 87  35 pos (0.59770115 0.40229885)  \n##            56) pressure&gt;=73.85 45  13 pos (0.71111111 0.28888889) *\n##            57) pressure&lt; 73.85 42  20 neg (0.47619048 0.52380952)  \n##             114) age&lt; 22.5 11   3 pos (0.72727273 0.27272727) *\n##             115) age&gt;=22.5 31  12 neg (0.38709677 0.61290323)  \n##               230) insulin&lt; 179 20  10 pos (0.50000000 0.50000000)  \n##                 460) insulin&gt;=155.66 12   4 pos (0.66666667 0.33333333) *\n##                 461) insulin&lt; 155.66 8   2 neg (0.25000000 0.75000000) *\n##               231) insulin&gt;=179 11   2 neg (0.18181818 0.81818182) *\n##          29) age&gt;=30.5 95  31 neg (0.32631579 0.67368421)  \n##            58) pedigree&lt; 0.3245 33  16 pos (0.51515152 0.48484848)  \n##             116) insulin&lt; 230.35 26  10 pos (0.61538462 0.38461538) *\n##             117) insulin&gt;=230.35 7   1 neg (0.14285714 0.85714286) *\n##            59) pedigree&gt;=0.3245 62  14 neg (0.22580645 0.77419355) *\n##        15) glucose&gt;=158.5 55   5 neg (0.09090909 0.90909091) *\n\n可以看到这棵树和我们最开始建立的模型一模一样，因为我们用了默认的设置，并没有更改。\n接下来我们在测试集上看看模型表现：\n\npred &lt;- predict(treefit, newdata = test,type = \"class\")\n\n# 计算混淆矩阵\ncaret::confusionMatrix(test$diabetes, pred)\n## Confusion Matrix and Statistics\n## \n##           Reference\n## Prediction pos neg\n##        pos 130  20\n##        neg  36  45\n##                                          \n##                Accuracy : 0.7576         \n##                  95% CI : (0.697, 0.8114)\n##     No Information Rate : 0.7186         \n##     P-Value [Acc &gt; NIR] : 0.10561        \n##                                          \n##                   Kappa : 0.4423         \n##                                          \n##  Mcnemar's Test P-Value : 0.04502        \n##                                          \n##             Sensitivity : 0.7831         \n##             Specificity : 0.6923         \n##          Pos Pred Value : 0.8667         \n##          Neg Pred Value : 0.5556         \n##              Prevalence : 0.7186         \n##          Detection Rate : 0.5628         \n##    Detection Prevalence : 0.6494         \n##       Balanced Accuracy : 0.7377         \n##                                          \n##        'Positive' Class : pos            \n## \n\n混淆矩阵的结果显示准确度Accuracy : 0.6987，效果不是很好！",
    "crumbs": [
      "代码实战",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>决策树</span>"
    ]
  },
  {
    "objectID": "决策树.html#后剪枝",
    "href": "决策树.html#后剪枝",
    "title": "17  决策树",
    "section": "17.5 后剪枝",
    "text": "17.5 后剪枝\n下面我们通过cp参数进行后剪枝。\n首先可以通过printcp函数查看复杂度和误差之间的数值关系：\n\nprintcp(treefit)\n## \n## Classification tree:\n## rpart(formula = diabetes ~ ., data = train, control = rpart.control(minsplit = 20, \n##     maxdepth = 30, cp = 0.01, xval = 10))\n## \n## Variables actually used in tree construction:\n## [1] age      glucose  insulin  mass     pedigree pressure\n## \n## Root node error: 187/537 = 0.34823\n## \n## n= 537 \n## \n##         CP nsplit rel error  xerror     xstd\n## 1 0.163102      0   1.00000 1.00000 0.059037\n## 2 0.045455      2   0.67380 0.82353 0.056044\n## 3 0.018717      4   0.58289 0.71658 0.053626\n## 4 0.016043      6   0.54545 0.66310 0.052222\n## 5 0.010695     10   0.48128 0.67380 0.052514\n## 6 0.010000     15   0.42781 0.66845 0.052369\n\nCP是复杂度参数，nsplit是分裂次数，rel error是相对误差，即某次分裂的RSS(残差平方和)除以不分裂的RSS，xerror是平均误差，xstd是交叉验证的标准差\n从上面的结果可以看出，CP=0.016043时，分裂次数为6，平均误差和标准差最小，分别是0.66310和0.052222。\n下面用图形化的方式展示结果：\n\nplotcp(treefit)\n\n\n\n\n\n\n\n\n纵坐标是相对误差，底下的横坐标是复杂度，上面的横坐标是树的规模，上面可以看出树的规模为7(也就是分裂6次)时，相对误差最小，和上面printcp()结果相同。\n接下来就可以就可以根据CP进行剪枝了，通常选择xerror最小时的CP值(还有别的选择方法，比如1倍标准差法等)：\n\ncp &lt;- treefit$cptable[which.min(treefit$cptable[,\"xerror\"]), \"CP\"]\ncp\n## [1] 0.01604278\ntreepruned &lt;- prune(treefit, cp = cp)\ntreepruned\n## n= 537 \n## \n## node), split, n, loss, yval, (yprob)\n##       * denotes terminal node\n## \n##   1) root 537 187 pos (0.65176909 0.34823091)  \n##     2) insulin&lt; 124.52 233  22 pos (0.90557940 0.09442060) *\n##     3) insulin&gt;=124.52 304 139 neg (0.45723684 0.54276316)  \n##       6) mass&lt; 28.9 67  16 pos (0.76119403 0.23880597) *\n##       7) mass&gt;=28.9 237  88 neg (0.37130802 0.62869198)  \n##        14) glucose&lt; 158.5 182  83 neg (0.45604396 0.54395604)  \n##          28) age&lt; 30.5 87  35 pos (0.59770115 0.40229885)  \n##            56) pressure&gt;=73.85 45  13 pos (0.71111111 0.28888889) *\n##            57) pressure&lt; 73.85 42  20 neg (0.47619048 0.52380952)  \n##             114) age&lt; 22.5 11   3 pos (0.72727273 0.27272727) *\n##             115) age&gt;=22.5 31  12 neg (0.38709677 0.61290323) *\n##          29) age&gt;=30.5 95  31 neg (0.32631579 0.67368421) *\n##        15) glucose&gt;=158.5 55   5 neg (0.09090909 0.90909091) *\n\n剪枝后的树简单多了，画图展示剪枝后的树：\n\nrpart.plot(treepruned)\n\n\n\n\n\n\n\n\n接下来使用修剪后的树再次作用于测试集：\n\npred2 &lt;- predict(treepruned, newdata = test, type = \"class\")\ncaret::confusionMatrix(test$diabetes, pred2)\n## Confusion Matrix and Statistics\n## \n##           Reference\n## Prediction pos neg\n##        pos 120  30\n##        neg  26  55\n##                                          \n##                Accuracy : 0.7576         \n##                  95% CI : (0.697, 0.8114)\n##     No Information Rate : 0.632          \n##     P-Value [Acc &gt; NIR] : 3.133e-05      \n##                                          \n##                   Kappa : 0.4736         \n##                                          \n##  Mcnemar's Test P-Value : 0.6885         \n##                                          \n##             Sensitivity : 0.8219         \n##             Specificity : 0.6471         \n##          Pos Pred Value : 0.8000         \n##          Neg Pred Value : 0.6790         \n##              Prevalence : 0.6320         \n##          Detection Rate : 0.5195         \n##    Detection Prevalence : 0.6494         \n##       Balanced Accuracy : 0.7345         \n##                                          \n##        'Positive' Class : pos            \n## \n\n通过后剪枝的准确度和不剪枝的结果差不多，但是由于分裂次数明显减少，模型复杂度明显减小了。\n这是非常常见的情况，有时候你调优后的模型可能还不如默认的好，有时候却效果超群，原因是多方面的，首先是数据问题，其次是选择的方法。",
    "crumbs": [
      "代码实战",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>决策树</span>"
    ]
  },
  {
    "objectID": "决策树.html#学习曲线",
    "href": "决策树.html#学习曲线",
    "title": "17  决策树",
    "section": "17.6 学习曲线",
    "text": "17.6 学习曲线\n接下来我们看看通过预剪枝能不能让模型的表现更好一点。\n如果要对单个超参数进行调优，我们可以用学习曲线的方式，学习曲线的横坐标是超参数的各个取值，纵坐标是不同超参数下的模型表现，通过学习曲线，可以找到模型表现较好的超参数值。\n我们就以调整minsplit为例。\nminsplit是每个节点的最少样本量，我们训练集一共有537个样本，就设置为2:300先看看情况。\n模型的评价指标选择准确率。\n\nres &lt;- list()\nfor(i in 2:300){\n  \n  f &lt;- rpart(diabetes ~ ., data = train,\n                 control = rpart.control(minsplit = i )\n             )\n  pred &lt;- predict(f, newdata = test,type = \"class\")\n  acc &lt;- caret::confusionMatrix(test$diabetes, pred)[[\"overall\"]][[\"Accuracy\"]]\n  df &lt;- data.frame(minsplit = i, accuracy = acc)\n  res[[i-1]] &lt;- df\n}\n\nacc.res &lt;- do.call(rbind, res)\n\n查看准确率的范围：\n\nrange(acc.res$accuracy)\n## [1] 0.6969697 0.7878788\n\n准确率最高是0.7878788，接下来我们把结果画出来：\n\nlibrary(ggplot2)\n\nggplot(acc.res, aes(minsplit, accuracy))+\n  geom_point()+\n  geom_line()+\n  geom_vline(xintercept = 13,linetype = 2, color = \"red\")+\n  theme_bw()\n\n\n\n\n\n\n\n\n这个图就是minsplit的学习曲线，从这个学习曲线可以看出，minsplit=13的时候，准确度accuracy达到了最大。\n我们可以轻松找出准确度最高的minsplit值（可能有多个minsplit都对应着最高的准确率，但是我们肯定是优先选择最简单的一个）：\n\nacc.res[which.max(acc.res$accuracy),]\n##    minsplit  accuracy\n## 12       13 0.7878788\n\nminsplit为13时，准确度就已经到最高了。这个结果比后剪枝的结果好一点。\n所以我们就选择minsplit为13，重新建立决策树模型。\n\n# 建立模型\nset.seed(123)\ntreef &lt;- rpart(diabetes ~ ., data = train,\n               control = rpart.control(minsplit = 13) # 这里选择13\n               )\n\n# 测试集查看效果\npred &lt;- predict(treef, newdata = test,type = \"class\")\n\n# 测试集的混淆矩阵\ncaret::confusionMatrix(test$diabetes, pred)\n## Confusion Matrix and Statistics\n## \n##           Reference\n## Prediction pos neg\n##        pos 126  24\n##        neg  25  56\n##                                           \n##                Accuracy : 0.7879          \n##                  95% CI : (0.7295, 0.8388)\n##     No Information Rate : 0.6537          \n##     P-Value [Acc &gt; NIR] : 5.953e-06       \n##                                           \n##                   Kappa : 0.5329          \n##                                           \n##  Mcnemar's Test P-Value : 1               \n##                                           \n##             Sensitivity : 0.8344          \n##             Specificity : 0.7000          \n##          Pos Pred Value : 0.8400          \n##          Neg Pred Value : 0.6914          \n##              Prevalence : 0.6537          \n##          Detection Rate : 0.5455          \n##    Detection Prevalence : 0.6494          \n##       Balanced Accuracy : 0.7672          \n##                                           \n##        'Positive' Class : pos             \n## \n\n决策树的可视化：\n\nrpart.plot(treef)",
    "crumbs": [
      "代码实战",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>决策树</span>"
    ]
  },
  {
    "objectID": "决策树.html#网格搜索",
    "href": "决策树.html#网格搜索",
    "title": "17  决策树",
    "section": "17.7 网格搜索",
    "text": "17.7 网格搜索\n学习曲线适合于某个单独的超参数调优，如果是多个超参数一起调优还是要使用其他方法，最常见的一种就是网格搜索，网格搜索现在我们有非常成熟的工具包，比如tidymodels、mlr3、caret。\n但是今天我想介绍下使用e1071包实现网格搜索，这个包功能强大，不仅可以实现支持向量机，而且包含多种调优功能，详情请参考：R语言支持向量机e1071\n\nlibrary(e1071)\n\n# 默认10折交叉验证\nset.seed(456)\ntune_obj &lt;- tune.rpart(diabetes ~ ., data = train,\n                       minsplit = seq(10,120,5),\n                       cp = c(0.0001,0.001,0.01,0.1,1)\n                       ) \n\ntune_obj\n## \n## Parameter tuning of 'rpart.wrapper':\n## \n## - sampling method: 10-fold cross validation \n## \n## - best parameters:\n##  minsplit    cp\n##       100 1e-04\n## \n## - best performance: 0.2269392\n\n速度很快，使用也很简单。直接给出最优结果是minsplit=100，cp=0.0001。\n重新拟合模型：\n\nrpart_fit &lt;- rpart(diabetes ~ ., data = train,\n                   control = rpart.control(minsplit = 100, \n                                         cp = 0.0001, \n                                         xval = 10 \n                                         )\n                   )\nrpart_fit\n## n= 537 \n## \n## node), split, n, loss, yval, (yprob)\n##       * denotes terminal node\n## \n##  1) root 537 187 pos (0.65176909 0.34823091)  \n##    2) insulin&lt; 124.52 233  22 pos (0.90557940 0.09442060) *\n##    3) insulin&gt;=124.52 304 139 neg (0.45723684 0.54276316)  \n##      6) mass&lt; 28.9 67  16 pos (0.76119403 0.23880597) *\n##      7) mass&gt;=28.9 237  88 neg (0.37130802 0.62869198)  \n##       14) glucose&lt; 158.5 182  83 neg (0.45604396 0.54395604)  \n##         28) age&lt; 30.5 87  35 pos (0.59770115 0.40229885) *\n##         29) age&gt;=30.5 95  31 neg (0.32631579 0.67368421) *\n##       15) glucose&gt;=158.5 55   5 neg (0.09090909 0.90909091) *\n\n这棵树就简单多了，效果也不错。\n接下来我们在测试集上看看模型表现：\n\npred &lt;- predict(rpart_fit, newdata = test,type = \"class\")\n\n# 计算混淆矩阵\ncaret::confusionMatrix(test$diabetes, pred)\n## Confusion Matrix and Statistics\n## \n##           Reference\n## Prediction pos neg\n##        pos 128  22\n##        neg  32  49\n##                                           \n##                Accuracy : 0.7662          \n##                  95% CI : (0.7063, 0.8192)\n##     No Information Rate : 0.6926          \n##     P-Value [Acc &gt; NIR] : 0.00816         \n##                                           \n##                   Kappa : 0.4717          \n##                                           \n##  Mcnemar's Test P-Value : 0.22067         \n##                                           \n##             Sensitivity : 0.8000          \n##             Specificity : 0.6901          \n##          Pos Pred Value : 0.8533          \n##          Neg Pred Value : 0.6049          \n##              Prevalence : 0.6926          \n##          Detection Rate : 0.5541          \n##    Detection Prevalence : 0.6494          \n##       Balanced Accuracy : 0.7451          \n##                                           \n##        'Positive' Class : pos             \n## \n\n混淆矩阵的结果显示准确度Accuracy : 0.7662 ，效果还可以哦！",
    "crumbs": [
      "代码实战",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>决策树</span>"
    ]
  },
  {
    "objectID": "决策树.html#其他",
    "href": "决策树.html#其他",
    "title": "17  决策树",
    "section": "17.8 其他",
    "text": "17.8 其他\n\n以决策树为例演示超参数调优的基本方法(下)\nR语言机器学习caret-09：决策树的小例子",
    "crumbs": [
      "代码实战",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>决策树</span>"
    ]
  },
  {
    "objectID": "决策树可视化.html",
    "href": "决策树可视化.html",
    "title": "18  决策树可视化",
    "section": "",
    "text": "18.1 安装\ninstall.packages('treeheatr')\n\n# install.packages('remotes') # uncomment to install devtools\nremotes::install_github('trangdata/treeheatr')",
    "crumbs": [
      "代码实战",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>决策树可视化</span>"
    ]
  },
  {
    "objectID": "决策树可视化.html#使用",
    "href": "决策树可视化.html#使用",
    "title": "18  决策树可视化",
    "section": "18.2 使用",
    "text": "18.2 使用\n可以直接提供数据框，它会自动帮你进行条件推断树，并画出结果：\n\nlibrary(treeheatr)\n\nheat_tree(penguins, target_lab = 'species')\n\n\n\n\n\n\n\n\n非常多的参数可以自定义，真的是太花里胡哨了！非常强！我喜欢！\n\nheat_tree(\n  penguins, target_lab = 'species',\n  par_node_vars = list(\n    label.size = 0.2,\n    label.padding = ggplot2::unit(0.1, 'lines'),\n    line_list = list(\n      ggplot2::aes(label = paste('Node', id)),\n      ggplot2::aes(label = splitvar),\n      ggplot2::aes(label = paste('p =', formatC(p.value, format = 'e', digits = 2)))),\n    line_gpar = list(\n      list(size = 8),\n      list(size = 8),\n      list(size = 6)),\n    id = 'inner'),\n  # terminal_vars = list(size = 0),\n  #cont_legend = TRUE, cate_legend = TRUE,\n  edge_vars = list(size = 1, color = 'grey'))\n\n\n\n\n\n\n\n\n当然也是支持你自己先把树做好，然后提供给它，不过此时需要是a 'party' or 'partynode' object\n\n# build tree using rpart:\nx &lt;- partykit::as.party(rpart::rpart(Outcome ~ ., data = train_covid))\n\nheat_tree(x = x, label_map = c(`1` = 'Deceased', `0` = 'Survived'))\n\n\n\n\n\n\n\n\n而且，支持支持在测试集中使用，顺滑又流畅，做演示用非常炫酷！\n\nheat_tree(\n  x = x,\n  data_test = test_covid, # 测试集\n  target_lab = 'Outcome',\n  label_map = c(`1` = 'Death', `0` = 'Survival'),\n  lev_fac = 3)\n\n\n\n\n\n\n\n\n更多细节大家去官网学习：https://trang1618.github.io/treeheatr/index.html",
    "crumbs": [
      "代码实战",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>决策树可视化</span>"
    ]
  },
  {
    "objectID": "集成算法类型.html",
    "href": "集成算法类型.html",
    "title": "19  集成算法简介",
    "section": "",
    "text": "19.1 袋装法\n袋装法又称为自助聚合法(boosting-aggregating)。袋装的过程和自助法重抽样的过程非常像，比如，一个数据集有100个样本，每次随机抽取1个，然后放回，再随机抽取1个，再放回，再随机抽取1个，再放回，这样的过程重复100次，就得到了一个和原数据集样本量相等的抽样数据集，这个抽样数据集就叫做自助集。\n由于每次都是有放回然后再随机抽取，所以一个自助集中可能有多个同一样本！所以就有可能在100次随机抽取中，有一些没被抽中过的样本，这些样本就被称为袋外样本(out-of-bag)，其中被抽中的样本(也就是自助集)用于训练模型，袋外样本用来评估模型表现。\n袋装法的典型代表就是随机森林算法。\n随机森林只有一点和上面的过程不同，那就是在建立模型时使用的预测变量（特征）也是随机选择的，并不是每次都使用全部的预测变量（特征）。",
    "crumbs": [
      "代码实战",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>集成算法简介</span>"
    ]
  },
  {
    "objectID": "集成算法类型.html#提升法",
    "href": "集成算法类型.html#提升法",
    "title": "19  集成算法简介",
    "section": "19.2 提升法",
    "text": "19.2 提升法\n提升法也是通过集合多个基模型的结果来给出更准确的预测，但是提升法可以对每次学习出错的样本给予更多的权重，也就是对于这些没预测对的样本进行加强学习，从而获得更好的结果。所以袋装法是并行训练的，但是提升法是顺序学习的，后一个学习器需要以前一个学习器的结果为基础。\n提升法的典型代表是Gradient boosting（梯度提升法）和Adaboost（自适应提升法）。不过目前来看还是梯度提升法发展的更好。\nAdaBoost 是通过提升错分数据点的权重来定位模型的不足，而Gradient Boosting 是通过算梯度（gradient）来定位模型的不足。因此相比 AdaBoost，Gradient Boosting可以使用更多种类的目标函数。\n如果Gradient Boosting使用的基模型是CART（classification and regression tree），就被称为Gradient Boosting Decision Tree（GBDT）。\n你可能还见过GBRT，可以简单理解为当GBDT用于回归模型时，是梯度提升回归（regression）树GBRT，二者的区别主要是损失函数不同。\n梯度提升法目前有以下几个不同的算法：\n\nGBM\nxgboost\nlightgbm\ncatboost\n\nCatBoost和xgboost、LightGBM并称为GBDT的三大主流神器，都是在GBDT算法框架下的一种改进实现。\nXGBoost是陈天奇等人开源的一个机器学习项目，LightGBM是由微软开发的算法，CatBoost是俄罗斯的搜索巨头Yandex开源的机器学习库。它们在R语言中的实现我们都介绍过，可以在公众号后台回复提升算法获取相关推文。",
    "crumbs": [
      "代码实战",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>集成算法简介</span>"
    ]
  },
  {
    "objectID": "集成算法类型.html#堆叠法",
    "href": "集成算法类型.html#堆叠法",
    "title": "19  集成算法简介",
    "section": "19.3 堆叠法",
    "text": "19.3 堆叠法\n堆叠法现在经常和融合法（模型融合）混在一起。它的实现方法简单来说是这样的：先在训练数据中拟合多个模型（可以是多个同一种模型，比如多个支持向量机模型（同质）；也可以是多个不同种模型，比如SVM和决策树（异质）），并拿到这些模型的预测结果，然后使用这些预测结果作为训练数据再拟合一个新的模型，这个新的模型被叫做超模型或者元模型（meta model）。\n当然具体实现起来还是挺复杂的，没有我说的这么简单。目前在R中堆叠法可以通过mlr3或者tidymodels实现，后续我会写一些教程。",
    "crumbs": [
      "代码实战",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>集成算法简介</span>"
    ]
  },
  {
    "objectID": "集成算法类型.html#参考资料",
    "href": "集成算法类型.html#参考资料",
    "title": "19  集成算法简介",
    "section": "19.4 参考资料：",
    "text": "19.4 参考资料：\n\ntidymodeling with R\n典型Stacking方法图解：https://zhuanlan.zhihu.com/p/48262572",
    "crumbs": [
      "代码实战",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>集成算法简介</span>"
    ]
  },
  {
    "objectID": "随机森林.html",
    "href": "随机森林.html",
    "title": "20  随机森林",
    "section": "",
    "text": "20.1 加载R包和数据\n演示数据为印第安人糖尿病数据集，这个数据一共有768行，9列，其中diabetes是结果变量，为二分类，其余列是预测变量。\nrm(list = ls())\nlibrary(randomForest)\n\nload(file = \"datasets/pimadiabetes.rdata\")\n\ndim(pimadiabetes)\n## [1] 768   9\nstr(pimadiabetes)\n## 'data.frame':    768 obs. of  9 variables:\n##  $ pregnant: num  6 1 8 1 0 5 3 10 2 8 ...\n##  $ glucose : num  148 85 183 89 137 116 78 115 197 125 ...\n##  $ pressure: num  72 66 64 66 40 ...\n##  $ triceps : num  35 29 22.9 23 35 ...\n##  $ insulin : num  202.2 64.6 217.1 94 168 ...\n##  $ mass    : num  33.6 26.6 23.3 28.1 43.1 ...\n##  $ pedigree: num  0.627 0.351 0.672 0.167 2.288 ...\n##  $ age     : num  50 31 32 21 33 30 26 29 53 54 ...\n##  $ diabetes: Factor w/ 2 levels \"pos\",\"neg\": 2 1 2 1 2 1 2 1 2 2 ...",
    "crumbs": [
      "代码实战",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>随机森林</span>"
    ]
  },
  {
    "objectID": "随机森林.html#数据划分",
    "href": "随机森林.html#数据划分",
    "title": "20  随机森林",
    "section": "20.2 数据划分",
    "text": "20.2 数据划分\n划分训练集、测试集。训练集用于建立模型，测试集用于测试模型表现，划分比例为7:3。\n\n# 划分是随机的，设置种子数可以让结果复现\nset.seed(123)\nind &lt;- sample(1:nrow(pimadiabetes), size = 0.7*nrow(pimadiabetes))\n\n# 去掉真实结果列\ntrain &lt;- pimadiabetes[ind,]\ntest &lt;- pimadiabetes[-ind,]\n\ndim(train)\n## [1] 537   9\ndim(test)\n## [1] 231   9",
    "crumbs": [
      "代码实战",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>随机森林</span>"
    ]
  },
  {
    "objectID": "随机森林.html#建立模型",
    "href": "随机森林.html#建立模型",
    "title": "20  随机森林",
    "section": "20.3 建立模型",
    "text": "20.3 建立模型\n在训练集拟合模型，参数importance=T表示需要计算变量的重要性：\n\nset.seed(123)\nfit &lt;- randomForest(diabetes ~ ., data = train, importance = T)\nfit\n## \n## Call:\n##  randomForest(formula = diabetes ~ ., data = train, importance = T) \n##                Type of random forest: classification\n##                      Number of trees: 500\n## No. of variables tried at each split: 2\n## \n##         OOB estimate of  error rate: 22.91%\n## Confusion matrix:\n##     pos neg class.error\n## pos 299  51   0.1457143\n## neg  72 115   0.3850267\n\n结果给出了树的数量：500颗；OOB错误率；还给出了混淆矩阵。",
    "crumbs": [
      "代码实战",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>随机森林</span>"
    ]
  },
  {
    "objectID": "随机森林.html#结果探索",
    "href": "随机森林.html#结果探索",
    "title": "20  随机森林",
    "section": "20.4 结果探索",
    "text": "20.4 结果探索\n下面是可视化整体错误率和树的数量的关系，可以看到随着树的数量增加，错误率逐渐降低并渐趋平稳，中间的黑色线条是整体的错误率，上下两条是结果变量中两个类别的错误率。\n\nplot(fit)\n\n\n\n\n\n\n\n\n查看整体错误率最小时有几棵树：\n\nwhich.min(fit$err.rate[,1])\n## [1] 55\n\n查看各个变量的重要性，这里还给出了mean decrease gini，数值越大说明变量越重要：\n\nrandomForest::importance(fit)\n##                pos       neg MeanDecreaseAccuracy MeanDecreaseGini\n## pregnant  3.076717  2.318634            4.0433361         15.45065\n## glucose  26.966011 22.856482           35.5073316         52.00314\n## pressure  1.148162 -0.574716            0.4489352         17.98889\n## triceps   7.448920  3.465856            8.4222497         24.27110\n## insulin  20.385928 20.507513           29.5489019         50.91964\n## mass      8.257001 18.574513           18.9842081         34.35775\n## pedigree  5.186099  4.690982            6.9584762         24.19246\n## age       6.456647  7.064110            9.6989105         23.94584\n\n可视化变量重要性：\n\nvarImpPlot(fit)\n\n\n\n\n\n\n\n\n通过变量重要性，大家就可以选择比较重要的变量了。你可以选择前5个，前10个，或者大于所有变量重要性平均值(中位数，百分位数等)的变量等等。\n使用随机森林筛选变量，我专门写过一篇文章：R语言随机森林筛选变量\n这个图还可以使用ggRandomForests来画，更加好看：\n\nlibrary(ggRandomForests)\ngg_dta &lt;- gg_vimp(fit)\nplot(gg_dta) #MeanDecreaseGini\n\n\n\n\n\n\n\n\n或者通过vip来画：\n\nlibrary(vip)\n\nvip(fit) #MeanDecreaseAccuracy\n\n\n\n\n\n\n\n\n\n20.4.1 提取某一棵树\n会给出这颗树在分支时的各种细节，结果太长了，没放出来：\n\ngetTree(fit, k=2)\n\n\n\n20.4.2 重新建立模型\n选择树的数量为55，并重新建立模型：\n\nfit1 &lt;- randomForest(diabetes ~ ., data = train, ntree = 55)\nfit1\n## \n## Call:\n##  randomForest(formula = diabetes ~ ., data = train, ntree = 55) \n##                Type of random forest: classification\n##                      Number of trees: 55\n## No. of variables tried at each split: 2\n## \n##         OOB estimate of  error rate: 24.02%\n## Confusion matrix:\n##     pos neg class.error\n## pos 299  51   0.1457143\n## neg  78 109   0.4171123\n\n查看测试集效果：\n\npred &lt;- predict(fit1, newdata = test)\nhead(pred)\n##   1   3   4   9  15  17 \n## neg neg pos neg pos neg \n## Levels: pos neg\n\n混淆矩阵：\n\ncaret::confusionMatrix(test$diabetes, pred)\n## Confusion Matrix and Statistics\n## \n##           Reference\n## Prediction pos neg\n##        pos 127  23\n##        neg  33  48\n##                                          \n##                Accuracy : 0.7576         \n##                  95% CI : (0.697, 0.8114)\n##     No Information Rate : 0.6926         \n##     P-Value [Acc &gt; NIR] : 0.01776        \n##                                          \n##                   Kappa : 0.4521         \n##                                          \n##  Mcnemar's Test P-Value : 0.22910        \n##                                          \n##             Sensitivity : 0.7937         \n##             Specificity : 0.6761         \n##          Pos Pred Value : 0.8467         \n##          Neg Pred Value : 0.5926         \n##              Prevalence : 0.6926         \n##          Detection Rate : 0.5498         \n##    Detection Prevalence : 0.6494         \n##       Balanced Accuracy : 0.7349         \n##                                          \n##        'Positive' Class : pos            \n## \n\n准确率0.7576，效果一般。\n\n# 计算预测概率，准备绘制ROC曲线\npred &lt;- predict(fit1, newdata = test, type = \"prob\")\nhead(pred)\n##          pos       neg\n## 1  0.3454545 0.6545455\n## 3  0.4181818 0.5818182\n## 4  1.0000000 0.0000000\n## 9  0.1454545 0.8545455\n## 15 0.6181818 0.3818182\n## 17 0.3272727 0.6727273\n\nROC曲线：\n\nlibrary(pROC)\n\n# 提供真实结果，预测概率\nrocc &lt;- roc(test$diabetes, as.matrix(pred)[,1])\nrocc\n## \n## Call:\n## roc.default(response = test$diabetes, predictor = as.matrix(pred)[,     1])\n## \n## Data: as.matrix(pred)[, 1] in 150 controls (test$diabetes pos) &gt; 81 cases (test$diabetes neg).\n## Area under the curve: 0.8438\n\nplot(rocc, \n     print.auc=TRUE, \n     auc.polygon=TRUE, \n     max.auc.polygon=TRUE, \n     auc.polygon.col=\"skyblue\", \n     grid=c(0.1, 0.2), \n     grid.col=c(\"green\", \"red\"), \n     print.thres=TRUE)\n\n\n\n\n\n\n\n\n训练集的ROC曲线怎么画呢？\n\n# 也是先计算预测概率\npred_train &lt;- predict(fit1, newdata = train, type = \"prob\")\n\nlibrary(pROC)\n# 提供真实结果，预测概率\nrocc &lt;- roc(train$diabetes, as.matrix(pred_train)[,1])\nrocc\n## \n## Call:\n## roc.default(response = train$diabetes, predictor = as.matrix(pred_train)[,     1])\n## \n## Data: as.matrix(pred_train)[, 1] in 350 controls (train$diabetes pos) &gt; 187 cases (train$diabetes neg).\n## Area under the curve: 1\n\nplot(rocc, \n     print.auc=TRUE, \n     auc.polygon=TRUE, \n     max.auc.polygon=TRUE, \n     auc.polygon.col=\"skyblue\", \n     grid=c(0.1, 0.2), \n     grid.col=c(\"green\", \"red\"), \n     print.thres=TRUE)\n\n\n\n\n\n\n\n\n训练集的AUC是1，但测试集的AUC只有0.84，严重的过拟合，随机森林本身就是容易过拟合的模型，它的各种超参数也都是降低模型复杂度，防止过拟合的。\n随机森林的网格搜索超参数调优可以通过caret、mlr3、tidymodels实现，caret法之前已介绍过，可参考：R语言机器学习caret-10：随机森林的小例子",
    "crumbs": [
      "代码实战",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>随机森林</span>"
    ]
  },
  {
    "objectID": "随机森林.html#超参数调优",
    "href": "随机森林.html#超参数调优",
    "title": "20  随机森林",
    "section": "20.5 超参数调优",
    "text": "20.5 超参数调优\nrandomForest自带一个调优函数：tuneRF()，可以实现对mtry的调优：\n\n#?tuneRF\ntuneRF(x = train[,-9], y = train$diabetes)\n## mtry = 2  OOB error = 26.82% \n## Searching left ...\n## mtry = 1     OOB error = 24.02% \n## 0.1041667 0.05 \n## Searching right ...\n## mtry = 4     OOB error = 24.21% \n## -0.007751938 0.05\n\n\n\n\n\n\n\n##       mtry  OOBError\n## 1.OOB    1 0.2402235\n## 2.OOB    2 0.2681564\n## 4.OOB    4 0.2420857\n\n结果如上图。\n轻量化的调参我还是推荐使用e1071包实现。\n其中的tune.randomForest()可实现对随机森林中mtry、nodesize、ntree3个超参数的调节，默认使用10折交叉验证法。\n\nmtry：每次分支时使用的预测变量数量，肯定是介于 1个~预测变量个数，之间\nnodesize：终末节点的样本量，如果这个数量特别大，那肯定树的深度就不会很深，可以防止过拟合\nntree：使用的树的数量，对于随机森林来说这个数量肯定是越多越好，通常选500或1000\n\n\nlibrary(e1071)\n\ntune_res &lt;- tune.randomForest(x = train[,-9], y = train$diabetes,\n                  nodesize = c(1:3),\n                  mtry = c(2,4,6,8),\n                  ntree = 500\n                  )\n#save(tune_res,file = \"datasets/tune_res.rdata\")\n\n\nload(file = \"datasets/tune_res.rdata\")\ntune_res\n## \n## Parameter tuning of 'randomForest':\n## \n## - sampling method: 10-fold cross validation \n## \n## - best parameters:\n##  nodesize mtry ntree\n##         3    5   500\n## \n## - best performance: 0.06731802\n\n查看错误率：\n\ntune_res$performances\n##   nodesize mtry ntree      error  dispersion\n## 1        1    2   500 0.07100520 0.003384379\n## 2        2    2   500 0.07087212 0.003004106\n## 3        3    2   500 0.07089114 0.003181621\n## 4        1    5   500 0.06845834 0.003377654\n## 5        2    5   500 0.06785020 0.003619697\n## 6        3    5   500 0.06731802 0.003680636\n## 7        1    9   500 0.07035890 0.003786503\n## 8        2    9   500 0.06956064 0.003933042\n## 9        3    9   500 0.06838229 0.003646747\n\n最小错误率：\n\ntune_res$best.performance\n## [1] 0.06731802\n\n最好的模型：\n\ntune_res$best.model\n## \n## Call:\n##  best.randomForest(x = train_df[, -1], y = train_df$children,      nodesize = c(1:3), mtry = c(2, 5, 9), ntree = 500) \n##                Type of random forest: classification\n##                      Number of trees: 500\n## No. of variables tried at each split: 5\n## \n##         OOB estimate of  error rate: 6.82%\n## Confusion matrix:\n##          children  none class.error\n## children     1357  2854  0.67774875\n## none          733 47672  0.01514306\n\n有了这个结果你就可以像上文一样，使用最好的超参数重新拟合模型了。\n\n\n\n\n\n\n注释\n\n\n\n随机森林由于是集合了多棵树的集成模型，所以它天生就是一个容易过拟合的模型，基本上它的调优思路就是限制模型复杂度，防止模型过拟合。它的很多参数都可以显示模型复杂度，比如树的数量、节点样本数、分支样本数等，调优时多个限制复杂度的参数不需要同时调整，只需要选择其中1~2个即可。",
    "crumbs": [
      "代码实战",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>随机森林</span>"
    ]
  },
  {
    "objectID": "随机森林.html#其他",
    "href": "随机森林.html#其他",
    "title": "20  随机森林",
    "section": "20.6 其他",
    "text": "20.6 其他\n除此之外，randomForest还可以进行：\n\n缺失值插补：rfImpute()和na.roughfix\n异常值检测：outlier()\n\n大家自己探索下即可。\n后台回复随机森林可获取相关推文合集，回复caret、mlr3、tidymodels也可获取相关推文合集。",
    "crumbs": [
      "代码实战",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>随机森林</span>"
    ]
  },
  {
    "objectID": "gbm.html",
    "href": "gbm.html",
    "title": "21  梯度提升机GBM",
    "section": "",
    "text": "21.1 安装\ninstall.packages(\"gbm\")",
    "crumbs": [
      "代码实战",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>梯度提升机GBM</span>"
    ]
  },
  {
    "objectID": "gbm.html#准备数据",
    "href": "gbm.html#准备数据",
    "title": "21  梯度提升机GBM",
    "section": "21.2 准备数据",
    "text": "21.2 准备数据\n使用皮玛印第安人糖尿病数据集。这个一个分类数据,其中diabetes是结果变量,pos表示有糖尿病,neg表示没有糖尿病,gbm要求结果变量必须用数字1和数字0表示,不能是字符型或者因子型,所以我们改一下,并按照7:3的比例划分训练集和测试集:\n\nrm(list = ls())\n\nload(file = \"datasets/pimadiabetes.rdata\")\npimadiabetes$diabetes &lt;- ifelse(pimadiabetes$diabetes==\"pos\",1,0)\n\n# 划分是随机的，设置种子数可以让结果复现\nset.seed(123)\nind &lt;- sample(1:nrow(pimadiabetes), size = 0.7*nrow(pimadiabetes))\n\n# 去掉真实结果列\ntrain &lt;- pimadiabetes[ind,]\ntest &lt;- pimadiabetes[-ind,]\n\ndim(train)\n## [1] 537   9\ndim(test)\n## [1] 231   9\n\nhead(train)\n##     pregnant glucose pressure triceps insulin mass pedigree age diabetes\n## 415        0     138       60   35.00  167.00 34.6    0.534  21        0\n## 463        8      74       70   40.00   49.00 35.3    0.705  39        1\n## 179        5     143       78   37.17  262.21 45.0    0.190  47        1\n## 526        3      87       60   18.00   69.59 21.8    0.444  21        1\n## 195        8      85       55   20.00   80.76 24.4    0.136  42        1\n## 118        5      78       48   30.96   76.57 33.7    0.654  25        1",
    "crumbs": [
      "代码实战",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>梯度提升机GBM</span>"
    ]
  },
  {
    "objectID": "gbm.html#拟合模型",
    "href": "gbm.html#拟合模型",
    "title": "21  梯度提升机GBM",
    "section": "21.3 拟合模型",
    "text": "21.3 拟合模型\n首先加载R包:\n\nlibrary(gbm)\n## Loaded gbm 2.1.8.1\n\n该包的默认学习率（shrinkage）是0.001，学习率越小，需要的树的数量就越多，但是默认的树的数量（n.trees）是100，这个数量是偏小的。默认的树的深度（interaction.depth）是1。\n下面我们建立一个gbm模型，注意因变量需要是0/1这种数值型：\n\nset.seed(123)  \ngbm1 &lt;- gbm(\n  formula = diabetes ~ .,\n  data = train,\n  distribution = \"bernoulli\", # 回归数据选\"gaussian\"\n  n.trees = 100,\n  shrinkage = 0.1,\n  interaction.depth = 3,\n  n.minobsinnode = 10,\n  cv.folds = 10,\n  train.fraction = 0.7\n)\n\ngbm1\n## gbm(formula = diabetes ~ ., distribution = \"bernoulli\", data = train, \n##     n.trees = 100, interaction.depth = 3, n.minobsinnode = 10, \n##     shrinkage = 0.1, train.fraction = 0.7, cv.folds = 10)\n## A gradient boosted model with bernoulli loss function.\n## 100 iterations were performed.\n## The best cross-validation iteration was 29.\n## The best test-set iteration was 42.\n## There were 8 predictors of which 8 had non-zero influence.\n\n结果表明,交叉验证法的最佳的树的数量是29,测试集法的最佳的树的数量是42.",
    "crumbs": [
      "代码实战",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>梯度提升机GBM</span>"
    ]
  },
  {
    "objectID": "gbm.html#结果探索",
    "href": "gbm.html#结果探索",
    "title": "21  梯度提升机GBM",
    "section": "21.4 结果探索",
    "text": "21.4 结果探索\ngbm1$fit是训练集的结果,但是并不是直接的类别或者预测概率,对于不同的distribution有不同的含义,可查看帮助文档?gbm.object\n\nhead(gbm1$fit)\n## [1] -0.5306145  1.4316161  0.3399815  4.7168734  3.5942364  3.1978575\nlength(gbm1$fit)\n## [1] 537\n\ngbm.perf可用于展示模型性能随树的数量的变化趋势,首先看看使用测试集的结果:\n\n# 使用0.3的测试集,train.fraction = 0.7\nbest.iter &lt;- gbm.perf(gbm1, method = \"test\")\n\n\n\n\n\n\n\n\nprint(best.iter)\n## [1] 42\n\n此时最佳的树的数量是42.\n再看看使用交叉验证法的情况:\n\n# plot error curve\nbest.iter &lt;- gbm.perf(gbm1, method = \"cv\")\n\n\n\n\n\n\n\n\nprint(best.iter)\n## [1] 29\n\n此时最佳的树的数量是29.\n再看看使用袋外数据的情况,结果中还贴心的给出了提示:\n\nbest.iter &lt;- gbm.perf(gbm1, method = \"OOB\")\n## OOB generally underestimates the optimal number of iterations although predictive performance is reasonably competitive. Using cv_folds&gt;1 when calling gbm usually results in improved predictive performance.\n\n\n\n\n\n\n\n\nprint(best.iter)\n## [1] 18\n## attr(,\"smoother\")\n## Call:\n## loess(formula = object$oobag.improve ~ x, enp.target = min(max(4, \n##     length(x)/10), 50))\n## \n## Number of Observations: 100 \n## Equivalent Number of Parameters: 8.32 \n## Residual Standard Error: 0.001453\n\nsummary.gbm可用于计算每个变量的相对影响,还可以画图:\n\nsummary(gbm1, n.trees = 1)  # 使用1棵树\n\n\n\n\n\n\n\n##               var   rel.inf\n## insulin   insulin 60.089053\n## mass         mass 31.326700\n## glucose   glucose  8.584247\n## pregnant pregnant  0.000000\n## pressure pressure  0.000000\n## triceps   triceps  0.000000\n## pedigree pedigree  0.000000\n## age           age  0.000000\n\n结果显示insulin/mass/glucose这3个变量影响最大.\n再看看使用最佳树的数量:\n\nsummary(gbm1, n.trees = 29)  \n\n\n\n\n\n\n\n##               var   rel.inf\n## insulin   insulin 34.840225\n## glucose   glucose 19.775059\n## mass         mass 17.649108\n## age           age  9.711670\n## pedigree pedigree  8.577304\n## triceps   triceps  4.217389\n## pregnant pregnant  3.155696\n## pressure pressure  2.073551\n\n可以看到变量的影响发生了一些变化,但是最重要的前3个还是没变.\npretty.gbm.tree函数可以提取每棵树的具体信息,但其实作用不大,帮助文档也说这个函数主要是为了debug和满足某些用户的好奇心!\n\nprint(pretty.gbm.tree(gbm1, i.tree = 1)) # 选择第一棵树\n##   SplitVar SplitCodePred LeftNode RightNode MissingNode ErrorReduction Weight\n## 0        4 145.010000000        1         5           9      12.204767    187\n## 1        1 128.500000000        2         3           4       1.743558    100\n## 2       -1   0.125744043       -1        -1          -1       0.000000     83\n## 3       -1  -0.030029028       -1        -1          -1       0.000000     17\n## 4       -1   0.099262621       -1        -1          -1       0.000000    100\n## 5        5  29.100000000        6         7           8       6.362808     87\n## 6       -1   0.124742981       -1        -1          -1       0.000000     16\n## 7       -1  -0.184594521       -1        -1          -1       0.000000     71\n## 8       -1  -0.127704866       -1        -1          -1       0.000000     87\n## 9       -1  -0.006331878       -1        -1          -1       0.000000    187\n##     Prediction\n## 0 -0.006331878\n## 1  0.099262621\n## 2  0.125744043\n## 3 -0.030029028\n## 4  0.099262621\n## 5 -0.127704866\n## 6  0.124742981\n## 7 -0.184594521\n## 8 -0.127704866\n## 9 -0.006331878\n\n直接对gbm使用plot方法竟然是画出某个变量的部份依赖图(partial dependence plots,PDP)!\nPDP是一种模型解释方法，我专门写文章介绍过，可参考合集：R语言模型解释\n比如我们画一下第一个变量的部份依赖图,展示了pregnant这个变量对结果变量的贡献.\n\nplot(gbm1, i.var = 1, n.trees = 29)\n\n\n\n\n\n\n\n\n也可以直接使用变量的名字:\n\nplot(gbm1, i.var = \"insulin\", n.trees = 29)\n\n\n\n\n\n\n\n\n还可以同时展示两个变量的部分依赖图,比如选择前两个变量:\n\nplot(gbm1, i.var = 1:2, n.trees = best.iter)\n\n\n\n\n\n\n\n\n或者直接使用变量名称:\n\nplot(gbm1, i.var = c(\"insulin\", \"pregnant\"), n.trees = 29)\n\n\n\n\n\n\n\n\n还可以同时展示3个变量:\n\nplot(gbm1, i.var = c(1, 2, 6), n.trees = 29,\n     continuous.resolution = 20)",
    "crumbs": [
      "代码实战",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>梯度提升机GBM</span>"
    ]
  },
  {
    "objectID": "gbm.html#测试集预测",
    "href": "gbm.html#测试集预测",
    "title": "21  梯度提升机GBM",
    "section": "21.5 测试集预测",
    "text": "21.5 测试集预测\npredict.gbm用于对新数据进行预测,返回预测向量。默认情况下，返回的结果为f(x)。 例如，如果distribution选择伯努利，返回值是log-odds，而coxph则采用log-hazard,如果是泊松分布也是返回log尺度的值.\ntype=\"response\"只对伯努利分布和泊松分布有效,如果是伯努利则返回类别概率,如果是泊松分布则返回预期计数。\n我们这个数据是二分类，所以distribution选择伯努利，此时我们可以计算预测概率：\n\n# 返回概率\npred &lt;- predict.gbm(gbm1, newdata = test, \n                    n.trees = 29,\n                    type = \"response\")\n\nhead(pred)\n## [1] 0.25366710 0.59607364 0.93003304 0.09244194 0.57341071 0.44773319\n\n随手画个ROC曲线，就是需要真实结果和预测概率而已！\n\nlibrary(pROC)\n\n# 提供真实结果，预测概率\nrocc &lt;- roc(test$diabetes, pred)\nrocc\n## \n## Call:\n## roc.default(response = test$diabetes, predictor = pred)\n## \n## Data: pred in 81 controls (test$diabetes 0) &lt; 150 cases (test$diabetes 1).\n## Area under the curve: 0.8299\n\nplot(rocc, \n     print.auc=TRUE, \n     auc.polygon=TRUE, \n     max.auc.polygon=TRUE, \n     auc.polygon.col=\"skyblue\", \n     grid=c(0.1, 0.2), \n     grid.col=c(\"green\", \"red\"), \n     print.thres=TRUE)\n\n\n\n\n\n\n\n\n我们可以自己把概率转换为类别,比如规定概率大于0.5就是类别1,否则就是类别2:\n\npred_type &lt;- ifelse(pred &gt; 0.5,1,0)\nhead(pred_type)\n## [1] 0 1 1 0 1 0\n\n这样就可以得到混淆矩阵以及其他指标了:\n\ncaret::confusionMatrix(factor(test$diabetes), factor(pred_type))\n## Confusion Matrix and Statistics\n## \n##           Reference\n## Prediction   0   1\n##          0  46  35\n##          1  24 126\n##                                           \n##                Accuracy : 0.7446          \n##                  95% CI : (0.6833, 0.7995)\n##     No Information Rate : 0.697           \n##     P-Value [Acc &gt; NIR] : 0.0647          \n##                                           \n##                   Kappa : 0.4211          \n##                                           \n##  Mcnemar's Test P-Value : 0.1930          \n##                                           \n##             Sensitivity : 0.6571          \n##             Specificity : 0.7826          \n##          Pos Pred Value : 0.5679          \n##          Neg Pred Value : 0.8400          \n##              Prevalence : 0.3030          \n##          Detection Rate : 0.1991          \n##    Detection Prevalence : 0.3506          \n##       Balanced Accuracy : 0.7199          \n##                                           \n##        'Positive' Class : 0               \n##",
    "crumbs": [
      "代码实战",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>梯度提升机GBM</span>"
    ]
  },
  {
    "objectID": "gbm.html#超参数调优",
    "href": "gbm.html#超参数调优",
    "title": "21  梯度提升机GBM",
    "section": "21.6 超参数调优",
    "text": "21.6 超参数调优\nGBM对各种超参数很敏感，比随机森林的调参更加复杂，\n通常的调参策略：\n\n先选择一个相对较高的学习率。一般来说，默认值0.1就可以了，但是建议在0.05和0.2之间尝试。\n确定此学习率下树的最佳数量。\n在固定树的数量的情况下，再尝试微调学习率，便于在速度与性能之间取得平衡。\n调整树相关的参数以确定学习率。\n确定树相关的参数后，适当降低学习率以评估准确性有无改进。\n使用最终的超参数设置和增加交叉验证的折数来获得更稳健的估计。\n\n我们可以自己写for循环实现,也可以借助caret/mlr3实现,tidymodels暂不支持gbm引擎.\n由于梯度提升模型有更多更好的选择,比如xgboost/lightgbm等,所以我们就不详细演示gbm的调优了,感兴趣自己试一下即可.",
    "crumbs": [
      "代码实战",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>梯度提升机GBM</span>"
    ]
  },
  {
    "objectID": "xgboost.html",
    "href": "xgboost.html",
    "title": "22  XGBoost",
    "section": "",
    "text": "22.1 准备数据\n先用自带数据演示一下简单的使用方法。\n首先是加载数据，这是一个二分类数据，其中label是结果变量，数值型，使用0和1表示，其余是预测变量。\nlibrary(xgboost)\n\nrm(list = ls())\n# load data\ndata(agaricus.train, package='xgboost')\ndata(agaricus.test, package='xgboost')\ntrain &lt;- agaricus.train\ntest &lt;- agaricus.test\n\nstr(train)\n## List of 2\n##  $ data :Formal class 'dgCMatrix' [package \"Matrix\"] with 6 slots\n##   .. ..@ i       : int [1:143286] 2 6 8 11 18 20 21 24 28 32 ...\n##   .. ..@ p       : int [1:127] 0 369 372 3306 5845 6489 6513 8380 8384 10991 ...\n##   .. ..@ Dim     : int [1:2] 6513 126\n##   .. ..@ Dimnames:List of 2\n##   .. .. ..$ : NULL\n##   .. .. ..$ : chr [1:126] \"cap-shape=bell\" \"cap-shape=conical\" \"cap-shape=convex\" \"cap-shape=flat\" ...\n##   .. ..@ x       : num [1:143286] 1 1 1 1 1 1 1 1 1 1 ...\n##   .. ..@ factors : list()\n##  $ label: num [1:6513] 1 0 0 1 0 0 0 1 0 0 ...\nstr(test)\n## List of 2\n##  $ data :Formal class 'dgCMatrix' [package \"Matrix\"] with 6 slots\n##   .. ..@ i       : int [1:35442] 0 2 7 11 13 16 20 22 27 31 ...\n##   .. ..@ p       : int [1:127] 0 83 84 806 1419 1603 1611 2064 2064 2701 ...\n##   .. ..@ Dim     : int [1:2] 1611 126\n##   .. ..@ Dimnames:List of 2\n##   .. .. ..$ : NULL\n##   .. .. ..$ : chr [1:126] \"cap-shape=bell\" \"cap-shape=conical\" \"cap-shape=convex\" \"cap-shape=flat\" ...\n##   .. ..@ x       : num [1:35442] 1 1 1 1 1 1 1 1 1 1 ...\n##   .. ..@ factors : list()\n##  $ label: num [1:1611] 0 1 0 0 0 0 1 0 1 0 ...\n\nclass(train$data)\n## [1] \"dgCMatrix\"\n## attr(,\"package\")\n## [1] \"Matrix\"\nxgboost对数据格式是有要求的，可以看到train和test都是列表，其中包含了预测变量data和结果变量label，其中label是使用0和1表示的。data部分是稀疏矩阵的形式（dgCMatrix）。\n# 查看数据维度,6513行，126列\ndim(train$data)\n## [1] 6513  126\n\ntable(train$label)\n## \n##    0    1 \n## 3373 3140",
    "crumbs": [
      "代码实战",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>XGBoost</span>"
    ]
  },
  {
    "objectID": "xgboost.html#拟合模型",
    "href": "xgboost.html#拟合模型",
    "title": "22  XGBoost",
    "section": "22.2 拟合模型",
    "text": "22.2 拟合模型\n接下来就是使用训练数据拟合模型：\n\nmodel &lt;- xgboost(data = train$data, label = train$label, \n                 max.depth = 2, # 树的最大深度\n                 eta = 1, # 学习率\n                 nrounds = 2,\n                 nthread = 2, # 使用的CPU线程数\n                 objective = \"binary:logistic\")\n## [1]  train-logloss:0.233376 \n## [2]  train-logloss:0.136658\n\n其中的参数nround在这里表示最终模型中树的数量，objective是目标函数。\n除了使用列表传入数据，也支持R语言中的密集矩阵（matrix），比如：\n\nmodel &lt;- xgboost(data = as.matrix(train$data), label = train$label, \n                 max.depth = 2, \n                 eta = 1, \n                 nrounds = 2,\n                 nthread = 2, \n                 objective = \"binary:logistic\")\n## [1]  train-logloss:0.233376 \n## [2]  train-logloss:0.136658\n\n开发者最推荐的格式还是特别为xgboost设计的xgb.DMatrix格式。\n\n# 建立xgb.DMatrix\ndtrain &lt;- xgb.DMatrix(data = train$data, label = train$label)\n\nxx &lt;- xgboost(data = dtrain, # 这样就不用单独传入label了\n              max.depth = 2, \n              eta = 1, \n              nrounds = 2,\n              nthread = 2, \n              objective = \"binary:logistic\"\n              )\n## [1]  train-logloss:0.233376 \n## [2]  train-logloss:0.136658\n\n这个格式也是xgboost特别设计的，有助于更好更快的进行计算，还可以支持更多的信息传入。可以通过getinfo获取其中的元素：\n\nhead(getinfo(dtrain, \"label\"))\n## [1] 1 0 0 1 0 0",
    "crumbs": [
      "代码实战",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>XGBoost</span>"
    ]
  },
  {
    "objectID": "xgboost.html#新数据预测",
    "href": "xgboost.html#新数据预测",
    "title": "22  XGBoost",
    "section": "22.3 新数据预测",
    "text": "22.3 新数据预测\n拟合模型后，就可以对新数据进行预测了：\n\n# predict\npred &lt;- predict(model, test$data)\nhead(pred)\n## [1] 0.28583017 0.92392391 0.28583017 0.28583017 0.05169873 0.92392391\n\nrange(pred)\n## [1] 0.01072847 0.92392391\n\n我们的任务是一个二分类的，但是xgboost的预测结果是概率，并不是直接的类别。我们需要自己转换一下，比如规定概率大于0.5就是类别1，小于等于0.5就是类别0（这个阈值其实也可以当做超参数调整的）。\n\npred_label &lt;- ifelse(pred &gt; 0.5,1,0)\ntable(pred_label)\n## pred_label\n##   0   1 \n## 826 785\n\n# 混淆矩阵\ntable(test$label, pred_label)\n##    pred_label\n##       0   1\n##   0 813  22\n##   1  13 763\n\n全对，准确率100%。\n除此之外还提供一个xgb.cv()用于实现交叉验证的建模，使用方法与xgboost()一致：\n\ncv.res &lt;- xgb.cv(data = train$data, label = train$label, \n                 nrounds = 2,\n                 objective = \"binary:logistic\",\n                 nfold = 10 # 交叉验证的折数\n                 )\n## [1]  train-logloss:0.439673+0.000200 test-logloss:0.439961+0.000873 \n## [2]  train-logloss:0.299550+0.000206 test-logloss:0.299903+0.001440\n\n\nmin(cv.res$evaluation_log)\n## [1] 0.0001998477",
    "crumbs": [
      "代码实战",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>XGBoost</span>"
    ]
  },
  {
    "objectID": "xgboost.html#控制输出日志",
    "href": "xgboost.html#控制输出日志",
    "title": "22  XGBoost",
    "section": "22.4 控制输出日志",
    "text": "22.4 控制输出日志\nxgboost()和xgb.train()有参数verbose可以控制输出日志的多少，默认是verbose = 1，输出性能指标结果。xgboost()是xgb.train()的简单封装，xgb.train()是训练xgboost模型的高级接口。\n如果是0，则是没有任何输出：\n\nxx &lt;- xgboost(data = train$data, label = train$label, objective = \"binary:logistic\"\n        ,nrounds = 2\n        ,verbose = 0\n        )\n\n如果是2，会输出性能指标结果和其他信息（这里没显示）：\n\nxx &lt;- xgboost(data = train$data, label = train$label, objective = \"binary:logistic\"\n        ,nrounds = 2\n        ,verbose = 2\n        )\n## [1]  train-logloss:0.439409 \n## [2]  train-logloss:0.299260\n\nxgb.cv()的verbose参数只有TRUE和FALSE。",
    "crumbs": [
      "代码实战",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>XGBoost</span>"
    ]
  },
  {
    "objectID": "xgboost.html#变量重要性",
    "href": "xgboost.html#变量重要性",
    "title": "22  XGBoost",
    "section": "22.5 变量重要性",
    "text": "22.5 变量重要性\nxgboost中变量的重要性是这样计算的：\n\n我们如何在xgboost中定义特性的重要性?在xgboost中，每次分割都试图找到最佳特征和分割点(splitting point)来优化目标。我们可以计算每个节点上的增益，它是所选特征的贡献。最后，我们对所有的树进行研究，总结每个特征的贡献，并将其视为重要性。如果特征的数量很大，我们也可以在绘制图之前对特征进行聚类。\n\n查看变量重要性：\n\nimportance_matrix &lt;- xgb.importance(model = model)\nimportance_matrix\n##                    Feature       Gain     Cover Frequency\n##                     &lt;char&gt;      &lt;num&gt;     &lt;num&gt;     &lt;num&gt;\n## 1:               odor=none 0.67615470 0.4978746       0.4\n## 2:         stalk-root=club 0.17135376 0.1920543       0.2\n## 3:       stalk-root=rooted 0.12317236 0.1638750       0.2\n## 4: spore-print-color=green 0.02931918 0.1461960       0.2\n\n可视化变量重要性：\n\nxgb.plot.importance(importance_matrix)\n\n\n\n\n\n\n\n\n或者ggplot2版本：\n\nxgb.ggplot.importance(importance_matrix)",
    "crumbs": [
      "代码实战",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>XGBoost</span>"
    ]
  },
  {
    "objectID": "xgboost.html#查看树的信息",
    "href": "xgboost.html#查看树的信息",
    "title": "22  XGBoost",
    "section": "22.6 查看树的信息",
    "text": "22.6 查看树的信息\n可以把学习好的树打印出来，查看具体情况：\n\nxgb.dump(model, with_stats = T)\n##  [1] \"booster[0]\"                                                       \n##  [2] \"0:[f28&lt;0.5] yes=1,no=2,missing=1,gain=4000.53101,cover=1628.25\"   \n##  [3] \"1:[f55&lt;0.5] yes=3,no=4,missing=3,gain=1158.21204,cover=924.5\"     \n##  [4] \"3:leaf=1.71217716,cover=812\"                                      \n##  [5] \"4:leaf=-1.70044053,cover=112.5\"                                   \n##  [6] \"2:[f108&lt;0.5] yes=5,no=6,missing=5,gain=198.173828,cover=703.75\"   \n##  [7] \"5:leaf=-1.94070864,cover=690.5\"                                   \n##  [8] \"6:leaf=1.85964918,cover=13.25\"                                    \n##  [9] \"booster[1]\"                                                       \n## [10] \"0:[f59&lt;0.5] yes=1,no=2,missing=1,gain=832.544983,cover=788.852051\"\n## [11] \"1:[f28&lt;0.5] yes=3,no=4,missing=3,gain=569.725098,cover=768.389709\"\n## [12] \"3:leaf=0.78471756,cover=458.936859\"                               \n## [13] \"4:leaf=-0.968530357,cover=309.45282\"                              \n## [14] \"2:leaf=-6.23624468,cover=20.462389\"\n\n还可以可视化树：\n\nxgb.plot.tree(model = model)\n\n\n\n\n\n这个图展示了2棵树的分支过程，因为我们设置了nround=2，所以结果就是只有2棵树。\n如果树的数量非常多的时候，这样每棵树看过来并不是很直观，通常xgboost虽然不如随机森林需要的树多，但是几十棵总是要的，所以xgboost提供了一种能把所有的树结合在一起展示的方法。\n\n# 多棵树展示在一起\nxgb.plot.multi.trees(model = model,fill=TRUE)\n## Column 2 ['No'] of item 2 is missing in item 1. Use fill=TRUE to fill with NA (NULL for list columns), or use.names=FALSE to ignore column names. use.names='check' (default from v1.12.2) emits this message and proceeds as if use.names=FALSE for  backwards compatibility. See news item 5 in v1.12.2 for options to control this message.\n\n\n\n\n\n这幅图就是把上面那张图的信息整合到了一起，大家仔细对比下图中的数字就会发现信息是一样的哦。\n除了以上方法可以检查树的信息外，还可以通过查看树的深度来检查树的结构。\n\nbst &lt;- xgboost(data = train$data, label = train$label, max.depth = 15,\n                 eta = 1, nthread = 2, nround = 30, objective = \"binary:logistic\",\n                 min_child_weight = 50,verbose = 0)\n\nxgb.plot.deepness(model = bst)\n\n\n\n\n\n\n\n\n这两幅图的横坐标都是树的深度，上面的图纵坐标是叶子的数量，展示了每层深度中的叶子数量。下面的图纵坐标是每片叶子的归一化之后的加权覆盖。\n从图中可以看出树的深度在5之后，叶子的数量就很少了，这提示我们为了防止过拟合，可以把树的深度控制在5以内。",
    "crumbs": [
      "代码实战",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>XGBoost</span>"
    ]
  },
  {
    "objectID": "xgboost.html#保存加载模型",
    "href": "xgboost.html#保存加载模型",
    "title": "22  XGBoost",
    "section": "22.7 保存加载模型",
    "text": "22.7 保存加载模型\n保存加载训练好的模型：\n\n# 保存\nxgb.save(model, \"xgboost.model\")\n\n# 加载\nxgb.load(\"xgboost.model\")\naa &lt;- predict(model, test$data)\n\n在R语言中除了直接使用xgboost这个R包实现之外，还有许多综合性的R包都可以实现xgboost算法，并支持超参数调优等更多任务，比如caret、tidymodels、mlr3。",
    "crumbs": [
      "代码实战",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>XGBoost</span>"
    ]
  },
  {
    "objectID": "xgboost.html#参数解释",
    "href": "xgboost.html#参数解释",
    "title": "22  XGBoost",
    "section": "22.8 参数解释",
    "text": "22.8 参数解释\nxgboost本身是基于梯度提升树（GBDT）实现的集成算法，所以它的参数整体来说可以分为三个部分：集成算法本身，用于集成的弱评估器（决策树），以及应用中的其他过程。\n下面是一些参数的详细介绍：\n\nnrounds：最大迭代次数（最终模型中树的数量）。\nearly_stopping_rounds：一个正整数，表示在验证集中经过K次训练如果模型表现还是没有提高就停止训练。\nprint_every_n：如果verbose&gt;0，这个参数表示每多少次迭代（多少棵树）打印一次日志信息。\n\nparams是xgb.train()中最重要的参数了，params接受一个列表，列表内包含超多参数，这些参数主要分为3大类，也是我们调参需要重点关注的参数：\n\n通用参数\n\n\nbooster：提升器类型，gbtree(默认)或者gblinear。多数情况下都是gbtree的效果更好，但是如果你的预测变量和结果变量呈现明显的线性关系，可能gblinear更好，但也不是绝对的，开发者建议都试一下。\n\n\nbooster相关的参数 2.1 tree booster相关的参数\n\neta：学习率η，每棵树在最终解中的贡献，决定迭代速度，默认为0.3，范围是[0，1]。一般会选择比较小的值，比如0.01，0.001等。\ngamma：在进行分支时所需要的最小的目标函数减少量，如果大于这个值，则继续分支，非常重要的参数，控制树的规模，默认是0，范围是[0，inf]\nmax_depth：单个树的最大深度。非常重要，通常max_depth和gamma只调一个即可\nmin_child_weight：对树进行提升时使用的最小权重，默认为1。叶子节点的二阶导数之和\nsubsample：子样本数据占整个数据的比例，也就是每次重抽样的比例，默认值为1（100%）。\ncolsample_bytree：建立树时随机抽取的特征数量，用一个比率表示，默认值为1（使用100%的特征)。比较重要\nlambda：L2正则化的比例，默认是1，也就是lasso。\nalpha：L1正则化的比例，默认是0。\n… 2.2 linear booster相关的参数\n…\n\n任务相关的参数\n\n\nobjective：指定任务类型和目标函数，支持自定义函数，默认的有以下类型，主要是回归、分类、生存、排序等：\n\nreg:squarederror：均方根误差（默认值）。\nreg:squaredlogerror：均方根对数误差。\nreg:logistic：logistic函数。\nreg:pseudohubererror：Pseudo Huber损失函数。\nbinary:logistic：二分类逻辑回归，输出概率值。\nbinary:logitraw：二分类逻辑回归，输出logistic转换之前的值。\nbinary:hinge：二分类hinge loss,输出0或者1。支持向量机的损失函数\ncount:poisson：计数数据的泊松回归\nsurvival:cox：右删失生存数据的cox回归，返回风险比HR。\nsurvival:aft：加速失效模型。\n…\n\nbase_score：叶子权重\neval_metric：验证集的性能指标，回归任务默认是rmse，二分类默认是错分率error。\n\n以下是一些其他需要注意的点：\n\neta/subsample/nrounds通常并不是提高模型表现的，主要是控制调参时间的。\ngamma是通过降低训练集的表现来防止过拟合的（让训练集和测试集的模型表现更接近），如果太大，也会降低测试集的表现\n先通过网格搜索调整nrounds和eta，然后使用gamma或者max_depth`看是否过拟合，再剪枝\nxgboost可以自动处理缺失值，不需要预处理，参数missing\nscale_pos_weight用于处理类不平衡的权重",
    "crumbs": [
      "代码实战",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>XGBoost</span>"
    ]
  },
  {
    "objectID": "xgboost.html#超参数调优",
    "href": "xgboost.html#超参数调优",
    "title": "22  XGBoost",
    "section": "22.9 超参数调优",
    "text": "22.9 超参数调优\n下面我们使用印第安人糖尿病数据集，演示下如何对xgboost进行超参数调优，由于目前xgboost包里面并没有专门的调优参数，所以还是需要借助其他R包实现（或者自己写循环），我这里借助了caret。\n\nrm(list = ls())\nlibrary(MASS)\nlibrary(xgboost)\n\nload(file = \"datasets/pimadiabetes.rdata\")\n\n# 结果变量改成1和0表示\npimadiabetes$diabetes &lt;- ifelse(pimadiabetes$diabetes == \"pos\",1,0)\nstr(pimadiabetes)\n## 'data.frame':    768 obs. of  9 variables:\n##  $ pregnant: num  6 1 8 1 0 5 3 10 2 8 ...\n##  $ glucose : num  148 85 183 89 137 116 78 115 197 125 ...\n##  $ pressure: num  72 66 64 66 40 ...\n##  $ triceps : num  35 29 22.9 23 35 ...\n##  $ insulin : num  202.2 64.6 217.1 94 168 ...\n##  $ mass    : num  33.6 26.6 23.3 28.1 43.1 ...\n##  $ pedigree: num  0.627 0.351 0.672 0.167 2.288 ...\n##  $ age     : num  50 31 32 21 33 30 26 29 53 54 ...\n##  $ diabetes: num  0 1 0 1 0 1 0 1 0 0 ...\n\n# 按照7：3的比例划分训练集、测试集\nset.seed(502)\nind &lt;- sample(1:nrow(pimadiabetes), size = 0.7*nrow(pimadiabetes))\npima.train &lt;- pimadiabetes[ind,]\npima.test &lt;- pimadiabetes[-ind,]\n\ndim(pima.train)\n## [1] 537   9\ndim(pima.test)\n## [1] 231   9\n\n训练集有537行，9列，测试集有231行，9列，其中diabetes列是结果变量，1表示有糖尿病，0表示没有糖尿病。\n下面我们使用默认参数拟合模型，看看模型效果。顺便学习下如果准备这些参数。\n注意，所有的预测变量都需要是数值型（这和我们前面介绍过的xgboost输入数据的格式有关，矩阵需要都是数值型的），所以分类变量需要进行一些转换，比如哑变量、独热编码等。\n\n# 选择参数的值\nparam &lt;- list(objective = \"binary:logistic\", # 二分类\n              booster = \"gbtree\",\n              eval_metric = \"error\",\n              eta = 0.3,\n              max_depth = 3,\n              subsample = 1,\n              colsample_bytree = 1,\n              gamma = 0.5)\n\n# 准备预测变量和结果变量\nx &lt;- as.matrix(pima.train[, 1:8])\ny &lt;- pima.train$diabetes\n\n# 放进专用的格式中\ntrain.mat &lt;- xgb.DMatrix(data = x, label = y)\ntrain.mat\n## xgb.DMatrix  dim: 537 x 8  info: label  colnames: yes\n\n这样参数和数据就都准备好了，下面开始训练即可。xgboost()是xgb.train()的简单封装，xgb.train()是训练xgboost模型的高级接口。xgboost模型的参数非常多，详情参考上面的介绍。\n\nset.seed(1)\nxgb.fit &lt;- xgb.train(params = param, \n                     data = train.mat, \n                     nrounds = 100)\n\n有了这个结果后你可以查看变量重要性，查看每棵树的信息，得出预测类别的概率，画出ROC曲线等，详情请参考前面的部分，这里就不再重复演示了。\n下面就是对这些参数进行调整，我们就使用caret进行演示。\ncaret作为R语言中经典的机器学习综合性R包，使用起来非常简单，我们也写过非常详细的系列教程了，公众号后台回复caret即可获取caret系列推文合集。\n\nlibrary(caret)\n\n# 选择参数范围\ngrid &lt;- expand.grid(nrounds = c(75, 100),\n                    colsample_bytree = 1,\n                    min_child_weight = 1,\n                    eta = c(0.01, 0.1, 0.3),\n                    gamma = c(0.5, 0.25),\n                    subsample = 0.5,\n                    max_depth = c(2, 3))\n\n# 一些控制参数，重抽样方法选择5折交叉验证\ncntrl &lt;- trainControl(method = \"cv\",\n                      number = 5,\n                      verboseIter = F,\n                      returnData = F,\n                      returnResamp = \"final\")\n\n# 开始调优\nset.seed(1)\ntrain.xgb &lt;- train(x = pima.train[, 1:8],\n                   y = pima.train$diabetes,\n                   trControl = cntrl,\n                   tuneGrid = grid,\n                   method = \"xgbTree\")\n## [19:59:21] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n## [19:59:21] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n## [19:59:21] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n## [19:59:22] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n## [19:59:22] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n## [19:59:22] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n## [19:59:22] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n## [19:59:22] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n## [19:59:22] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n## [19:59:22] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n## [19:59:22] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n## [19:59:23] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n## [19:59:23] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n## [19:59:23] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n## [19:59:23] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n## [19:59:23] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n## [19:59:23] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n## [19:59:23] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n## [19:59:23] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n## [19:59:24] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n## [19:59:24] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n## [19:59:24] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n## [19:59:24] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n## [19:59:24] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n## [19:59:24] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n## [19:59:24] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n## [19:59:24] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n## [19:59:25] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n## [19:59:25] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n## [19:59:25] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n## [19:59:25] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n## [19:59:25] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n## [19:59:25] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n## [19:59:25] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n## [19:59:25] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n## [19:59:26] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n## [19:59:26] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n## [19:59:26] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n## [19:59:26] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n## [19:59:26] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n## [19:59:26] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n## [19:59:26] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n## [19:59:26] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n## [19:59:27] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n## [19:59:27] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n## [19:59:27] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n## [19:59:27] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n## [19:59:27] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n## [19:59:27] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n## [19:59:27] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n## [19:59:27] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n## [19:59:28] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n## [19:59:28] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n## [19:59:28] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n## [19:59:28] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n## [19:59:28] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n## [19:59:28] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n## [19:59:28] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n## [19:59:28] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n## [19:59:29] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n#train.xgb # 太长不展示了\n\n结果中给出了最优的超参数：nrounds = 75, max_depth = 2, eta = 0.1, gamma = 0.5, colsample_bytree = 1, min_child_weight = 1, subsample = 0.5。\n这个结果可以探索可视化的地方非常多，比如：查看不同超参数对模型性能的影响：\n\nplot(train.xgb)\n\n\n\n\n\n\n\n\n也是支持ggplot2的。\n\nggplot(train.xgb)\n\n\n\n\n\n\n\n\n更多方法大家可以探索我们的caret合集。",
    "crumbs": [
      "代码实战",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>XGBoost</span>"
    ]
  },
  {
    "objectID": "xgboost.html#重新拟合模型",
    "href": "xgboost.html#重新拟合模型",
    "title": "22  XGBoost",
    "section": "22.10 重新拟合模型",
    "text": "22.10 重新拟合模型\n接下来就是使用最优的超参数重新拟合模型。\n\n# 选择最优的参数值\nparam &lt;- list(objective = \"binary:logistic\",\n              booster = \"gbtree\",\n              eval_metric = \"error\",\n              eta = 0.1,\n              max_depth = 2,\n              subsample = 0.5,\n              colsample_bytree = 1,\n              gamma = 0.5)\n\n\n# 拟合模型\nset.seed(1)\nxgb.fit &lt;- xgb.train(params = param, \n                     data = train.mat, \n                     nrounds = 75)",
    "crumbs": [
      "代码实战",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>XGBoost</span>"
    ]
  },
  {
    "objectID": "xgboost.html#模型评价",
    "href": "xgboost.html#模型评价",
    "title": "22  XGBoost",
    "section": "22.11 模型评价",
    "text": "22.11 模型评价\n画个ROC曲线，先计算一下训练集的预测概率，再画ROC曲线即可，没有任何难度：\n\npred_train &lt;- predict(xgb.fit, newdata = train.mat)\nhead(pred_train)\n## [1] 0.8459527 0.5674704 0.2664627 0.6891936 0.8845208 0.9655565\n\nlibrary(ROCR)\npred &lt;- prediction(pred_train, pima.train$diabetes)\nperf &lt;- performance(pred, \"tpr\", \"fpr\")\nauc &lt;- round(performance(pred, \"auc\")@y.values[[1]],digits = 4)\n\nplot(perf, \n     main = paste(\"ROC curve (\", \"AUC = \",auc,\")\"), \n     col = 2, \n     lwd = 2)\nabline(0,1, lty = 2, lwd = 2)\n\n\n\n\n\n\n\n\nAUC值达到了0.9以上。\n公众号后台回复ROC即可获取ROC曲线合集，回复最佳截点即可获取ROC曲线的最佳截点合集。\n计算混淆矩阵等请参考前面的部分，无非就是把概率转换为硬类别而已。",
    "crumbs": [
      "代码实战",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>XGBoost</span>"
    ]
  },
  {
    "objectID": "xgboost.html#测试集",
    "href": "xgboost.html#测试集",
    "title": "22  XGBoost",
    "section": "22.12 测试集",
    "text": "22.12 测试集\n首先需要把测试集的格式转换一下。\n\n# 放在专用的格式中\ntest.mat &lt;- xgb.DMatrix(data = as.matrix(pima.test[, 1:8]), \n                        label = pima.test$diabetes)\n# 预测测试集的概率\npred_test &lt;- predict(xgb.fit, newdata = test.mat)\nhead(pred_test)\n## [1] 0.1269337 0.9513396 0.9778093 0.4253268 0.4961042 0.6491649\n\n# 绘制ROC曲线\nlibrary(ROCR)\npred &lt;- prediction(pred_test, pima.test$diabetes)\nperf &lt;- performance(pred, \"tpr\", \"fpr\")\nauc &lt;- round(performance(pred, \"auc\")@y.values[[1]],digits = 4)\n\nplot(perf, \n     main = paste(\"ROC curve (\", \"AUC = \",auc,\")\"), \n     col = 2, \n     lwd = 2)\nabline(0,1, lty = 2, lwd = 2)\n\n\n\n\n\n\n\n\neasy！但是有点过拟合了，可以尝试下不同的超参数再试下。\n有些指标是基于预测概率的，有些指标是基于预测列别的，xgboost只能给出预测概率，我们自己转换一下即可计算各种基于类别的指标了。",
    "crumbs": [
      "代码实战",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>XGBoost</span>"
    ]
  },
  {
    "objectID": "xgboost.html#参考资料",
    "href": "xgboost.html#参考资料",
    "title": "22  XGBoost",
    "section": "22.13 参考资料",
    "text": "22.13 参考资料\n\n帮助文档\nhttps://blog.csdn.net/weixin_43217641/article/details/126599474\n精通机器学习基于R\n官方文档：https://xgboost.readthedocs.io/en/latest/R-package/xgboostPresentation.html\nhttps://www.r-bloggers.com/2016/03/an-introduction-to-xgboost-r-package/",
    "crumbs": [
      "代码实战",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>XGBoost</span>"
    ]
  },
  {
    "objectID": "lightGBM.html",
    "href": "lightGBM.html",
    "title": "23  lightGBM",
    "section": "",
    "text": "23.1 安装\ninstall.packages(\"lightgbm\")\nlightgbm是支持GPU的，如果要使用GPU，可以按照官方教程进行配置：https://lightgbm.readthedocs.io/en/latest/R/index.html#installing-a-gpu-enabled-build\n我们这里就不演示了。",
    "crumbs": [
      "代码实战",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>lightGBM</span>"
    ]
  },
  {
    "objectID": "lightGBM.html#快速上手",
    "href": "lightGBM.html#快速上手",
    "title": "23  lightGBM",
    "section": "23.2 快速上手",
    "text": "23.2 快速上手\n加载R包和数据。\n\nlibrary(lightgbm)\n\ndata(bank, package = \"lightgbm\")\ndim(bank)\n## [1] 4521   17\n\nbank[1:5, c(\"y\", \"age\", \"balance\")]\n##         y   age balance\n##    &lt;char&gt; &lt;int&gt;   &lt;int&gt;\n## 1:     no    30    1787\n## 2:     no    33    4789\n## 3:     no    35    1350\n## 4:     no    30    1476\n## 5:     no    59       0\n\n# 结果变量\ntable(bank$y)\n## \n##   no  yes \n## 4000  521\n\n这是一个2分类数据，共有4521行，17列。\n然后就是训练模型了。由于参考了xgboost的设计思路，所以使用上真的和xgboost太像了，部分细节这里不再重复说了，可以参考之前的推文：R语言xgboost快速上手\nlightgbm提供了2个函数用于训练模型,其中lightgbm()是lgb.train()的轻量化实现，这个特性和xgboost是一模一样的，而且各种参数的名字也基本上一样的。\n在支持的格式上，也是和xgboost保持一致，只支持x/y接口，不支持R语言中经典的公式接口。\n预测变量要么是矩阵，要么是专门为lightgbm开发的lgb.Dataset。结果变量需要改为数值型，不能是因子型或者字符型。\n下面是使用lightgbm()训练模型的示例：\n\ny &lt;- as.numeric(bank$y == \"yes\")\nX &lt;- data.matrix(bank[, c(\"age\", \"balance\")]) # 只选了2个自变量\n\n# 训练\nfit &lt;- lightgbm(\n  data = X\n  , label = y\n  , params = list(\n    num_leaves = 4L\n    , learning_rate = 1.0\n    , objective = \"binary\"\n  )\n  , nrounds = 10L\n  , verbose = -1L\n)\n\n# 训练结果\nsummary(predict(fit, X))\n##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n## 0.01192 0.07370 0.09871 0.11593 0.14135 0.65796\n\n除此之外，lightgbm的专用格式可以通过lgb.Dataset()进行包装。包装后就可以使用lgb.train()进行训练了，它的params参数也是接受一个列表：\n\n# 专用格式\ndtrain &lt;- lgb.Dataset(X, label = y)\n\n# 参数设置\nparams &lt;- list(\n  objective = \"binary\"\n  , num_leaves = 4L\n  , learning_rate = 1.0\n)\n\n# 训练模型\nfit &lt;- lgb.train(\n  params\n  , data = dtrain\n  , nrounds = 10L\n  , verbose = -1L\n)\n\nlightgbm的超参数非常多，大家可以参考官方文档，大部分参数都和xgboost差不多，也可以参考之前的关于xgboost的推文。\n\nR语言xgboost快速上手\nR语言xgboost超参数调优\nR语言lightgbm超参数调优\n\n这个包和xgboost真的是一模一样，我就不多介绍了，大家结合之前的xgboost的推文，自己了解下吧。",
    "crumbs": [
      "代码实战",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>lightGBM</span>"
    ]
  },
  {
    "objectID": "lightGBM.html#推荐学习资源",
    "href": "lightGBM.html#推荐学习资源",
    "title": "23  lightGBM",
    "section": "23.3 推荐学习资源",
    "text": "23.3 推荐学习资源\n\nlightgbm官方文档：https://lightgbm.readthedocs.io/en/latest/index.html\nlightgbm中文文档：https://lightgbm.apachecn.org/#/docs/6",
    "crumbs": [
      "代码实战",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>lightGBM</span>"
    ]
  },
  {
    "objectID": "catboost.html",
    "href": "catboost.html",
    "title": "24  catboost",
    "section": "",
    "text": "24.1 特性介绍\n说实话官网介绍的这几个特性我觉得非常吸引我！\n总结来说就是：CatBoost比XGBoost和LightGBM更准、更快、更牛逼！选CatBoost就对了！\n看下官方给出的模型性能比较：\n再看下速度对比，快了不是一点点！",
    "crumbs": [
      "代码实战",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>catboost</span>"
    ]
  },
  {
    "objectID": "catboost.html#特性介绍",
    "href": "catboost.html#特性介绍",
    "title": "24  catboost",
    "section": "",
    "text": "无需超参数调优即可得到高质量结果：CatBoost默认参数得到的模型就已经足够优秀，省去大量调优时间\n不需要对分类变量重编码：多数模型都需要在运行前对分类变量进行一些预处理，比如虚拟变量转换等，但是CatBoost不需要这些操作！它会自动为你处理这些分类变量\n支持GPU：这个没啥说的，其他算法也有\n更高的准确率：过拟合的可能性更小，结果准确率更高\n更快的速度：比比XGBoost和LightGBM更快",
    "crumbs": [
      "代码实战",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>catboost</span>"
    ]
  },
  {
    "objectID": "catboost.html#安装",
    "href": "catboost.html#安装",
    "title": "24  catboost",
    "section": "24.2 安装",
    "text": "24.2 安装\n可以直接用以下代码在线安装，我写这篇推文时（2024.3.25）最新的版本是1.2.5，注意版本不要写错！这个算法的更新速度飞快，github一直在更新中，这也是我这么晚才介绍这个算法的原因之一。\n不同平台的安装方式略有不同，可参考官网教程：https://catboost.ai/en/docs/installation/r-installation-binary-installation\n下面是windows的安装方法，在线安装对网络有要求。\n\n#install.packages('remotes')\nremotes::install_url('https://github.com/catboost/catboost/releases/download/v1.2.5/catboost-R-windows-x86_64-1.2.5.tgz', INSTALL_opts = c(\"--no-multiarch\", \"--no-test-load\"))\n\n如果你网不行，那还是选择把安装包下载下来，本地安装，下载地址是：https://github.com/catboost/catboost/releases\n注意选择合适的版本，注意路径不要写错：\n\nremotes::install_local(\"E:/R/R包/catboost-R-windows-x86_64-1.2.5.tgz\")",
    "crumbs": [
      "代码实战",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>catboost</span>"
    ]
  },
  {
    "objectID": "catboost.html#快速上手",
    "href": "catboost.html#快速上手",
    "title": "24  catboost",
    "section": "24.3 快速上手",
    "text": "24.3 快速上手\n首先加载数据和R包。\n\nlibrary(catboost)\n\n数据就用著名的德国信用评分数据。这个数据一共有4454行，14列，其中Status是结果变量，二分类，因子型，good表示信用评分好，bad表示信用评分差，其余列是预测变量，预测变量既有数值型也有分类型，分类型的都是factor。并且这个数据有部分缺失值。\n\nlibrary(modeldata)\ndata(\"credit_data\")\ndim(credit_data)\n## [1] 4454   14\nstr(credit_data)\n## 'data.frame':    4454 obs. of  14 variables:\n##  $ Status   : Factor w/ 2 levels \"bad\",\"good\": 2 2 1 2 2 2 2 2 2 1 ...\n##  $ Seniority: int  9 17 10 0 0 1 29 9 0 0 ...\n##  $ Home     : Factor w/ 6 levels \"ignore\",\"other\",..: 6 6 3 6 6 3 3 4 3 4 ...\n##  $ Time     : int  60 60 36 60 36 60 60 12 60 48 ...\n##  $ Age      : int  30 58 46 24 26 36 44 27 32 41 ...\n##  $ Marital  : Factor w/ 5 levels \"divorced\",\"married\",..: 2 5 2 4 4 2 2 4 2 2 ...\n##  $ Records  : Factor w/ 2 levels \"no\",\"yes\": 1 1 2 1 1 1 1 1 1 1 ...\n##  $ Job      : Factor w/ 4 levels \"fixed\",\"freelance\",..: 2 1 2 1 1 1 1 1 2 4 ...\n##  $ Expenses : int  73 48 90 63 46 75 75 35 90 90 ...\n##  $ Income   : int  129 131 200 182 107 214 125 80 107 80 ...\n##  $ Assets   : int  0 0 3000 2500 0 3500 10000 0 15000 0 ...\n##  $ Debt     : int  0 0 0 0 0 0 0 0 0 0 ...\n##  $ Amount   : int  800 1000 2000 900 310 650 1600 200 1200 1200 ...\n##  $ Price    : int  846 1658 2985 1325 910 1645 1800 1093 1957 1468 ...\n\n先按照7：3划分个训练集和测试集：\n\nset.seed(6354)\nind &lt;- sample(1:nrow(credit_data),0.7*nrow(credit_data))\ntrain &lt;- credit_data[ind,]\ntest &lt;- credit_data[-ind,]\n\ndim(train)\n## [1] 3117   14\ndim(test)\n## [1] 1337   14\n\n\n24.3.1 数据准备\n与XGBoost和LightGBM一样，CatBoost在使用时也需要专用的格式，也使用专用的函数进行封装。\n封装前也是要把预测变量和结果变量单独拿出来，不能放一起，结果变量必须用数字1和0表示，分类变量建议使用因子型，计算时会自动进行预处理，然后使用catboost.load_pool()函数进行封装：\n\nfeatures &lt;- train[,-1]\nlabels &lt;- ifelse(train$Status == \"good\",1,0)\n\ntrain_pool &lt;- catboost.load_pool(data = features,label = labels)\ntrain_pool\n## catboost.Pool\n## 3117 rows, 13 columns\n\n\n\n24.3.2 训练模型\n封装好之后就可以提供给算法进行学习了，使用的函数是catboost.train()：\n\nmodel &lt;- catboost.train(train_pool,  NULL,\n                        params = list(loss_function = 'Logloss', # 损失函数\n                                      iterations = 100, # 100棵树\n                                      metric_period=10 # 每10棵树计算1次指标\n                                      #prediction_type=c(\"Class\",\"Probability\")\n                                      ) \n                        )\n## Learning rate set to 0.13828\n## 0:   learn: 0.6445127    total: 192ms    remaining: 19s\n## 10:  learn: 0.4703331    total: 350ms    remaining: 2.83s\n## 20:  learn: 0.4236971    total: 463ms    remaining: 1.74s\n## 30:  learn: 0.4003478    total: 585ms    remaining: 1.3s\n## 40:  learn: 0.3891287    total: 690ms    remaining: 992ms\n## 50:  learn: 0.3798819    total: 799ms    remaining: 768ms\n## 60:  learn: 0.3711200    total: 908ms    remaining: 581ms\n## 70:  learn: 0.3646432    total: 1.01s    remaining: 415ms\n## 80:  learn: 0.3577685    total: 1.12s    remaining: 262ms\n## 90:  learn: 0.3532076    total: 1.22s    remaining: 121ms\n## 99:  learn: 0.3474675    total: 1.32s    remaining: 0us\n\n\n\n24.3.3 查看结果\n默认的结果，平平无奇：\n\nmodel\n## CatBoost model (100 trees)\n## Loss function: Logloss\n## Fit to 13 feature(s)\n\n获取变量重要性：\n\ncatboost.get_feature_importance(model)\n##                [,1]\n## Seniority 13.704942\n## Home       4.409678\n## Time       4.220234\n## Age        2.819654\n## Marital    1.347322\n## Records   11.865015\n## Job       10.489621\n## Expenses   4.141522\n## Income    16.671911\n## Assets     7.982805\n## Debt       2.387191\n## Amount    13.280631\n## Price      6.679474\n\n查看模型的各种参数：\n\n# 太长不放出来了\ncatboost.get_model_params(model)",
    "crumbs": [
      "代码实战",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>catboost</span>"
    ]
  },
  {
    "objectID": "catboost.html#交叉验证",
    "href": "catboost.html#交叉验证",
    "title": "24  catboost",
    "section": "24.4 交叉验证",
    "text": "24.4 交叉验证\n正式使用的时候肯定是要用交叉验证或者bootstrap这种重抽样方法的，此时可以用catboost.cv()：\n\nmodel_cv &lt;- catboost.cv(train_pool, fold_count = 5, # 5折交叉验证\n                     params = list(loss_function = 'Logloss', # 损失函数\n                                   iterations = 100, # 100棵树，默认是1000\n                                   metric_period=10 # 每10棵树计算1次指标\n                                   ,verbose=0 # 减少日志输出\n                                   ,random_seed=1234\n                                   ) \n                     )\n## Training on fold [0/5]\n## \n## bestTest = 0.4371081209\n## bestIteration = 99\n## \n## Training on fold [1/5]\n## \n## bestTest = 0.4414686541\n## bestIteration = 99\n## \n## Training on fold [2/5]\n## \n## bestTest = 0.3944321369\n## bestIteration = 99\n## \n## Training on fold [3/5]\n## \n## bestTest = 0.4309943666\n## bestIteration = 99\n## \n## Training on fold [4/5]\n## \n## bestTest = 0.4337864039\n## bestIteration = 99\n\n结果有11行，分别是分析集和评估集（这个概念不懂的请参考tidymodels-rsample：数据划分）的模型指标，因为我们设置的是每10棵树计算1次指标，一共有100棵树，所以有11行，如果改成每20棵树计算一次，就是有6行：\n\nmodel_cv\n##    test.Logloss.mean test.Logloss.std train.Logloss.mean train.Logloss.std\n## 1          0.6776724      0.001077025          0.6769152       0.001407418\n## 2          0.5717982      0.005433284          0.5655896       0.002684553\n## 3          0.5189984      0.006360272          0.5075464       0.004846065\n## 4          0.4890277      0.009908884          0.4719268       0.005552004\n## 5          0.4681056      0.012011841          0.4474683       0.005473089\n## 6          0.4540802      0.012673318          0.4295447       0.005890671\n## 7          0.4454735      0.014272786          0.4175673       0.005970905\n## 8          0.4380360      0.015334895          0.4058692       0.006510348\n## 9          0.4329628      0.016470751          0.3966050       0.006976306\n## 10         0.4294426      0.017505843          0.3890955       0.007753856\n## 11         0.4275579      0.018925086          0.3832895       0.007536350\n\n给你顺手画个图，看看迭代次数（也就是树的数量）和模型性能的关系：\n\nlibrary(ggplot2)\nlibrary(tidyr)\nlibrary(dplyr)\n\nmodel_cv %&gt;% mutate(iteration = (row_number()-1)*10) %&gt;% \n  pivot_longer(cols = c(1,3),names_to = \"sets\",values_to = \"Logloss.mean\") %&gt;% \n  ggplot(., aes(iteration,Logloss.mean))+\n  geom_line(aes(group=sets,color=sets),linewidth=2)\n\n\n\n\n\n\n\n\n树越多越准确哈，但是太多了容易过拟合。",
    "crumbs": [
      "代码实战",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>catboost</span>"
    ]
  },
  {
    "objectID": "catboost.html#新数据预测",
    "href": "catboost.html#新数据预测",
    "title": "24  catboost",
    "section": "24.5 新数据预测",
    "text": "24.5 新数据预测\n这样这个模型就训练好了，下面就可以对新的数据进行预测了。\n预测前也是需要先用catboost.load_pool()把数据封装起来，然后使用catboost.predict()即可：\n\ntest_pool &lt;- catboost.load_pool(test[,-1])\n\n# 预测类别概率\npred_prob &lt;- catboost.predict(model, test_pool, prediction_type = \"Probability\")\nhead(pred_prob)\n## [1] 0.4514856 0.9225333 0.1778611 0.9073103 0.9636420 0.3190501\n\n二分类数据支持以下预测类型：\n\nProbability\nClass\nRawFormulaVal\nExponent\nLogProbability\nVirtEnsembles\nTotalUncertainty\n\n使用起来毫无难度，是不是很easy呢？",
    "crumbs": [
      "代码实战",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>catboost</span>"
    ]
  },
  {
    "objectID": "catboost.html#模型评价",
    "href": "catboost.html#模型评价",
    "title": "24  catboost",
    "section": "24.6 模型评价",
    "text": "24.6 模型评价\n模型评价还是那一套，混淆矩阵、准确率、敏感度、特异度、ROC曲线等等。我们需要的就是拿到模型的预测结果就好了，分类变量的结果有2种：预测类别或者预测的某种类别的概率。\n有些模型只能给出其中1种，有的模型都能给，比如catboost。\n获取测试集的预测概率和类别：\n\n# 预测类别\npred_status &lt;- catboost.predict(model, test_pool, prediction_type = \"Class\")\n\n# 得把1，0这种再变回去才好比较\npred_status &lt;- ifelse(pred_status==1,\"good\",\"bad\")\n\n# 再变成因子型，得和原始数据保持一致\nlevels(test$Status)\n## [1] \"bad\"  \"good\"\npred_status &lt;- factor(pred_status,levels = levels(test$Status))\nhead(pred_status)\n## [1] bad  good bad  good good bad \n## Levels: bad good\n\n\n24.6.1 混淆矩阵\n借助caret查看混淆矩阵，这个功能是目前R里面最强大的，没有对手：\n\ncaret::confusionMatrix(pred_status,test$Status, mode=\"everything\")\n## Confusion Matrix and Statistics\n## \n##           Reference\n## Prediction bad good\n##       bad  180   98\n##       good 198  861\n##                                           \n##                Accuracy : 0.7786          \n##                  95% CI : (0.7554, 0.8006)\n##     No Information Rate : 0.7173          \n##     P-Value [Acc &gt; NIR] : 2.014e-07       \n##                                           \n##                   Kappa : 0.4066          \n##                                           \n##  Mcnemar's Test P-Value : 8.702e-09       \n##                                           \n##             Sensitivity : 0.4762          \n##             Specificity : 0.8978          \n##          Pos Pred Value : 0.6475          \n##          Neg Pred Value : 0.8130          \n##               Precision : 0.6475          \n##                  Recall : 0.4762          \n##                      F1 : 0.5488          \n##              Prevalence : 0.2827          \n##          Detection Rate : 0.1346          \n##    Detection Prevalence : 0.2079          \n##       Balanced Accuracy : 0.6870          \n##                                           \n##        'Positive' Class : bad             \n## \n\n\n\n24.6.2 ROC曲线\n画个ROC曲线，就用ROCR包吧，3步走：\n\nlibrary(ROCR)\n\npred &lt;- prediction(pred_prob,test$Status) # 预测概率，真实类别\nperf &lt;- performance(pred, \"tpr\",\"fpr\")\nauc &lt;- round(performance(pred, \"auc\")@y.values[[1]],digits = 4)\n#auc\n\nplot(perf,lwd=2,col=\"tomato\")\nabline(0,1,lty=2)\nlegend(\"bottomright\", legend=paste(\"测试集AUC: \",auc), col=\"tomato\", lwd=2,bty = \"n\")\n\n\n\n\n\n\n\n\nAUC是0.82以上，结果还可以，但是要注意这个是good的AUC哦。\n\n\n24.6.3 PR曲线\n顺手再画个PR曲线吧，这个曲线很常见，但是在我之前的推文中画的不是很多，其实也很简单的，还是用ROCR就可以画，横坐标是查全率（recall，也叫召回率、灵敏度、真阳性率），纵坐标是查准率（precision，又叫精确率）。\n\nlibrary(ROCR)\n\npred &lt;- prediction(pred_prob,test$Status) # 预测概率，真实类别\nperf &lt;- performance(pred, \"prec\",\"rec\")\nauc &lt;- round(performance(pred, \"auc\")@y.values[[1]],digits = 4)\n\nplot(perf,lwd=2,col=\"tomato\")\nlegend(\"bottomright\", legend=paste(\"AUC: \",auc), col=\"tomato\", lwd=2,bty = \"n\")\n\n\n\n\n\n\n\n\n这个就是PR曲线，它的曲线下面积也是AUC（area under the curve），只不过这个是PR-AUC，上面的那个是ROC-AUC。\n\n\n24.6.4 校准曲线\n公众号后台回复校准曲线可获取合集，查看各种各样的校准曲线绘制，我这里给大家介绍最新的方法（其实之前也介绍过了），用probably这个包绘制：\n\nlibrary(probably)\n## Warning: package 'probably' was built under R version 4.3.3\nlibrary(dplyr)\n\ntest %&gt;% select(Status) %&gt;% \n  bind_cols(.pred_good = pred_prob) %&gt;% \n  cal_plot_breaks(Status, .pred_good,event_level = \"second\",conf_level = 0.95)\n\n\n\n\n\n\n\n\n但是目前这个版本（1.0.3）有个bug，第3个参数estimate，必须是.pred_xxx，其中的xxx必须是真实结果中的某一个类别，比如我这个数据Status中的类别就是good和bad，那么这个名字就必须是.pred_good或者.pred_bad，其他都会报错（下标出界）！！\n\n\n24.6.5 决策曲线\n顺手再画个决策曲线，这个决策曲线是临床预测模型中才有的内容，其他内容基本上都是机器学习的基础知识。公众号后台回复决策曲线即可获取合集：\n\nsource(\"datasets/dca.r\")\n\n# 需要真实结果和预测概率\ndca_data &lt;- data.frame(truth_status = test$Status, pred_prob = pred_prob)\n\n# 结果变量变成0，1\ndca_data$truth_status &lt;- ifelse(dca_data$truth_status == \"good\",1,0)\n\ndc &lt;- dca(data = dca_data, # 测试集\n          outcome = \"truth_status\",\n          predictors = \"pred_prob\",\n          probability = T\n          )\n\n\n\n\n\n\n\n\n太简单！\n\n前几年stdca.r和dca.r这两个脚本是可以在网络中免费下载的，但是从2022年底左右这个网站就不提供这两段代码的下载了。因为我很早就下载好了，所以我把这两段代码放在粉丝qq群文件里，大家有需要的加群下载即可。当然我还介绍了很多其他方法，公众号后台回复决策曲线即可获取合集。\n\n\n\n24.6.6 训练集指标\n有的时候写文章还需要同时写上训练集的各种指标，很简单，把数据换成训练集即可得到训练集的结果：\n\npred_status &lt;- catboost.predict(model, train_pool, # 这里写训练集\n                                prediction_type = \"Class\")\npred_prob &lt;- catboost.predict(model, train_pool,\n                              prediction_type = \"Probability\")\n\npred_status &lt;- ifelse(pred_status==1,\"good\",\"bad\")\npred_status &lt;- factor(pred_status,levels = levels(train$Status))\n\n# 混淆矩阵\ncaret::confusionMatrix(train$Status, pred_status, mode=\"everything\")\n## Confusion Matrix and Statistics\n## \n##           Reference\n## Prediction  bad good\n##       bad   546  330\n##       good  138 2103\n##                                           \n##                Accuracy : 0.8499          \n##                  95% CI : (0.8368, 0.8622)\n##     No Information Rate : 0.7806          \n##     P-Value [Acc &gt; NIR] : &lt; 2.2e-16       \n##                                           \n##                   Kappa : 0.6019          \n##                                           \n##  Mcnemar's Test P-Value : &lt; 2.2e-16       \n##                                           \n##             Sensitivity : 0.7982          \n##             Specificity : 0.8644          \n##          Pos Pred Value : 0.6233          \n##          Neg Pred Value : 0.9384          \n##               Precision : 0.6233          \n##                  Recall : 0.7982          \n##                      F1 : 0.7000          \n##              Prevalence : 0.2194          \n##          Detection Rate : 0.1752          \n##    Detection Prevalence : 0.2810          \n##       Balanced Accuracy : 0.8313          \n##                                           \n##        'Positive' Class : bad             \n## \n\nROC曲线和PR曲线也是一样的画，这里只演示下ROC：\n\nlibrary(ROCR)\n\npred &lt;- prediction(pred_prob,train$Status) # 预测概率，真实类别\nperf &lt;- performance(pred, \"tpr\",\"fpr\")\nauc &lt;- round(performance(pred, \"auc\")@y.values[[1]],digits = 4)\n#auc\n\nplot(perf,lwd=2,col=\"tomato\")\nabline(0,1,lty=2)\nlegend(\"bottomright\", legend=paste(\"训练集AUC: \",auc), col=\"tomato\", lwd=2,bty = \"n\")\n\n\n\n\n\n\n\n\n有点过拟合了，不过还可以，一般来说训练集肯定是要比测试集高一点的，因为这种方法并不正规，你把训练用的数据重新给模型预测，这样属于“泄题行为”~。",
    "crumbs": [
      "代码实战",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>catboost</span>"
    ]
  },
  {
    "objectID": "catboost.html#参数介绍",
    "href": "catboost.html#参数介绍",
    "title": "24  catboost",
    "section": "24.7 参数介绍",
    "text": "24.7 参数介绍\ncatboost.load_pool的参数没啥特殊的，主要的作用就是封装数据，大家可以看下帮助文档。与catboost.load_pool对应的还有个catboost.save_pool是用来保存数据的，保存的格式也是CatBoost支持的格式，看名字也知道有点类似于R中的save()和load()。\n重点说下catboost.train()的参数。catboost.train()说起来只有3个参数：\n\nlearn_pool：用来训练模型的数据\ntest_pool：测试数据，用来防止过拟合的，默认值是NULL，即不使用\nparams：这个才是最重要的参数，里面是一个列表，包括了非常多的超参数\n\n但是呢也不要害怕，因为CatBoost的优势之一就是使用默认的参数即可获得非常棒的结果，所以如果你不是非常懂这些参数，不要随便改，你改的可能还不如默认的好。\n另外就是这些参数虽然非常多（大概数了下竟然有78个！），但是多数参数都是和树模型有关的参数以及和提升算法有关的参数，比如树的数量、树的深度等等。剩下的参数就是一些控制输出的、控制日志的、和catboost算法本身有关的等等。\n大家使用时真正需要调整的可能还是损失函数、学习率、树的数量、输的深度等这种参数。这一点可以和XGBoost和LightGBM对照着学习。",
    "crumbs": [
      "代码实战",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>catboost</span>"
    ]
  },
  {
    "objectID": "catboost.html#超参数调优",
    "href": "catboost.html#超参数调优",
    "title": "24  catboost",
    "section": "24.8 超参数调优",
    "text": "24.8 超参数调优\n如果你要进行超参数调优，可以借助caret、mlr3进行，你需要学习下这几个包的使用语法，然后换上CatBoost这个“引擎”即可，使用语法没有任何特别之处，公众号后台回复关键词即可获取相关合集，这里就不再重复了。\ntidymodels目前并不支持CatBoost，目前看来还是遥遥无期，原因看这里：R语言lightGBM超参数调优",
    "crumbs": [
      "代码实战",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>catboost</span>"
    ]
  },
  {
    "objectID": "catboost.html#模型解释",
    "href": "catboost.html#模型解释",
    "title": "24  catboost",
    "section": "24.9 模型解释",
    "text": "24.9 模型解释\n所有的模型解释都可以使用DALEX及其扩展包实现，而且只需要3行代码即可，使用语法也都是统一的，公众号后台回复模型解释即可获取合集。",
    "crumbs": [
      "代码实战",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>catboost</span>"
    ]
  },
  {
    "objectID": "9999-appendix.html",
    "href": "9999-appendix.html",
    "title": "附录 A — 其他合集",
    "section": "",
    "text": "A.1 R语言、Rtools、Rstudio的安装",
    "crumbs": [
      "附录",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>其他合集</span>"
    ]
  },
  {
    "objectID": "9999-appendix.html#r语言rtoolsrstudio的安装",
    "href": "9999-appendix.html#r语言rtoolsrstudio的安装",
    "title": "附录 A — 其他合集",
    "section": "",
    "text": "公众号推文：可能是最适合小白的R语言和R包安装教程\nb站视频教程：适合小白的R语言和Rstudio安装教程",
    "crumbs": [
      "附录",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>其他合集</span>"
    ]
  },
  {
    "objectID": "9999-appendix.html#r包安装",
    "href": "9999-appendix.html#r包安装",
    "title": "附录 A — 其他合集",
    "section": "A.2 R包安装",
    "text": "A.2 R包安装\n\n公众号推文：可能是最好的R包安装教程\nb站视频教程：可能是最好用的R包安装教程",
    "crumbs": [
      "附录",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>其他合集</span>"
    ]
  },
  {
    "objectID": "9999-appendix.html#临床预测模型",
    "href": "9999-appendix.html#临床预测模型",
    "title": "附录 A — 其他合集",
    "section": "A.3 临床预测模型",
    "text": "A.3 临床预测模型\n临床预测模型合集：临床预测模型",
    "crumbs": [
      "附录",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>其他合集</span>"
    ]
  },
  {
    "objectID": "9999-appendix.html#机器学习",
    "href": "9999-appendix.html#机器学习",
    "title": "附录 A — 其他合集",
    "section": "A.4 机器学习",
    "text": "A.4 机器学习\n医学和生信笔记后台回复caret即可获取caret包的合集教程；回复tidymodels即可获取tidymodels的合集教程；回复mlr3即可获取mlr3合集教程，回复机器学习即可获取机器学习推文合集。\nR语言机器学习合集：R语言机器学习",
    "crumbs": [
      "附录",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>其他合集</span>"
    ]
  },
  {
    "objectID": "9999-appendix.html#生信数据挖掘",
    "href": "9999-appendix.html#生信数据挖掘",
    "title": "附录 A — 其他合集",
    "section": "A.5 生信数据挖掘",
    "text": "A.5 生信数据挖掘\n生信数据挖掘合集：生信数据挖掘\n医学和生信笔记公众号所有关于生信数据挖掘的推文都可以免费下载使用，请看：“灌水”生信类文章会用到哪些生信下游分析？（附下载地址）\ngithub地址：R语言生信数据挖掘",
    "crumbs": [
      "附录",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>其他合集</span>"
    ]
  },
  {
    "objectID": "9999-appendix.html#扫码关注",
    "href": "9999-appendix.html#扫码关注",
    "title": "附录 A — 其他合集",
    "section": "A.6 扫码关注",
    "text": "A.6 扫码关注\n欢迎扫码关注：医学和生信笔记",
    "crumbs": [
      "附录",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>其他合集</span>"
    ]
  }
]